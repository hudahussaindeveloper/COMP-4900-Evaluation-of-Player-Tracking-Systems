{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Player Tracking System Comparison\n",
        "\n",
        "This notebook compares 3 different player tracking systems:\n",
        "- Eagle\n",
        "- Darkmyter (using Ultralytics YOLO)\n",
        "- Ultralytics YOLO 11 + Botsort\n",
        "\n",
        "**Important**: Run cells in order from top to bottom!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Setup directories and utilities\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "BASE_DIR = Path(\"/content\")\n",
        "REPOS_DIR = BASE_DIR / \"repositories\"\n",
        "VIDEOS_DIR = BASE_DIR / \"videos\"\n",
        "CLIPS_DIR = BASE_DIR / \"clips\"\n",
        "OUTPUT_DIR = BASE_DIR / \"output\"\n",
        "\n",
        "for d in [REPOS_DIR, VIDEOS_DIR, CLIPS_DIR, OUTPUT_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def print_status(msg, status=\"INFO\"):\n",
        "    \"Print colored status messages\"\n",
        "    colors = {\n",
        "        \"INFO\": \"\\033[94m\",\n",
        "        \"SUCCESS\": \"\\033[92m\",\n",
        "        \"WARNING\": \"\\033[93m\",\n",
        "        \"ERROR\": \"\\033[91m\",\n",
        "        \"RESET\": \"\\033[0m\"\n",
        "    }\n",
        "    print(f\"{colors.get(status, '')}[{status}] {msg}{colors['RESET']}\")\n",
        "\n",
        "print_status(\"Directory structure created\", \"SUCCESS\")\n",
        "print(f\"Working directory: {BASE_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Clone all repositories\n",
        "\n",
        "import subprocess\n",
        "\n",
        "REPOSITORIES = {\n",
        "    \"eagle\": \"https://github.com/nreHieW/Eagle.git\",\n",
        "    \"darkmyter\": \"https://github.com/Darkmyter/Football-Players-Tracking.git\",\n",
        "}\n",
        "\n",
        "print_status(\"Cloning repositories...\", \"INFO\")\n",
        "\n",
        "for name, url in REPOSITORIES.items():\n",
        "    repo_path = REPOS_DIR / name\n",
        "\n",
        "    if repo_path.exists():\n",
        "        print_status(f\"{name}: Already exists, skipping\", \"WARNING\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        print_status(f\"{name}: Cloning...\", \"INFO\")\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"clone\", url, str(repo_path)],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=300\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print_status(f\"{name}: Cloned successfully\", \"SUCCESS\")\n",
        "        else:\n",
        "            print_status(f\"{name}: Clone failed - {result.stderr[:100]}\", \"ERROR\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"{name}: Clone failed - {str(e)}\", \"ERROR\")\n",
        "\n",
        "print_status(\"Repository cloning complete\", \"SUCCESS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Install dependencies\n",
        "\n",
        "print_status(\"Installing dependencies...\", \"INFO\")\n",
        "\n",
        "!pip install -q torch torchvision torchaudio tracklab\n",
        "!pip install -q opencv-python numpy scipy pandas scikit-learn matplotlib\n",
        "!pip install -q ultralytics supervision\n",
        "!pip install -q gdown Pillow tqdm requests\n",
        "!pip install -q \\\n",
        "    loguru cython cython_bbox lap onemetric scikit-image tabulate tqdm numpy torch torchvision opencv-python pyyaml yolox\n",
        "!pip install -q loguru\n",
        "!pip install onemetric #THIS CELL IS IMPORTANT\n",
        "!pip install psutil\n",
        "\n",
        "\n",
        "print_status(\"Dependencies installed\", \"SUCCESS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzy7qI3atgID"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO(\"yolo11m.pt\")\n",
        "print(\"Loaded weights from:\", getattr(model, \"ckpt_path\", \"unknown path\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_videos"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Download videos from Google Drive\n",
        "\n",
        "!pip install -q gdown\n",
        "\n",
        "import gdown\n",
        "from pathlib import Path\n",
        "\n",
        "# Shared folder ID\n",
        "FOLDER_ID = \"1Cs4kTX6GYwfcpKyDZdqRKBezz49wT7_N\"\n",
        "\n",
        "print_status(\"Downloading videos from shared folder...\", \"INFO\")\n",
        "\n",
        "try:\n",
        "    gdown.download_folder(\n",
        "        id=FOLDER_ID,\n",
        "        output=str(VIDEOS_DIR),\n",
        "        quiet=False,\n",
        "        use_cookies=False\n",
        "    )\n",
        "\n",
        "    # List downloaded videos\n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV', '.MKV']\n",
        "    available_videos = []\n",
        "\n",
        "    for ext in video_extensions:\n",
        "        available_videos.extend(list(VIDEOS_DIR.glob(f\"*{ext}\")))\n",
        "\n",
        "    if not available_videos:\n",
        "        print_status(\"No video files found\", \"ERROR\")\n",
        "    else:\n",
        "        print(f\"\\nDOWNLOADED {len(available_videos)} VIDEO(S)\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        for idx, video in enumerate(available_videos, 1):\n",
        "            size_mb = video.stat().st_size / (1024 * 1024)\n",
        "            print(f\"{idx}. {video.name} ({size_mb:.1f} MB)\")\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"VIDEO SELECTION\")\n",
        "\n",
        "        # Ask for number of videos\n",
        "        print(\"\\nHow many videos do you want to evaluate?\")\n",
        "        print(f\"  - Enter a number between 1 and {len(available_videos)}\")\n",
        "        print(f\"  - Enter 'all' or leave blank to process ALL {len(available_videos)} videos\")\n",
        "\n",
        "        num_selection = input(\"\\nNumber of videos: \").strip().lower()\n",
        "\n",
        "        VIDEO_PATHS = []\n",
        "\n",
        "        if not num_selection or num_selection == 'all':\n",
        "            VIDEO_PATHS = available_videos\n",
        "            print_status(f\"Selected ALL {len(VIDEO_PATHS)} videos\", \"SUCCESS\")\n",
        "        elif num_selection.isdigit():\n",
        "            num_videos = int(num_selection)\n",
        "            if 1 <= num_videos <= len(available_videos):\n",
        "                if num_videos == len(available_videos):\n",
        "                    VIDEO_PATHS = available_videos\n",
        "                else:\n",
        "                    print(f\"\\nSelect {num_videos} video(s) from the list above:\")\n",
        "                    print(\"  - Enter comma-separated numbers (e.g., '1,3,5')\")\n",
        "                    print(f\"  - Or enter 'first' to select the first {num_videos} videos\")\n",
        "\n",
        "                    video_selection = input(\"\\nYour selection: \").strip().lower()\n",
        "\n",
        "                    if video_selection == 'first':\n",
        "                        VIDEO_PATHS = available_videos[:num_videos]\n",
        "                    else:\n",
        "                        try:\n",
        "                            indices = [int(x.strip()) for x in video_selection.split(',')]\n",
        "                            if len(indices) != num_videos:\n",
        "                                print_status(f\"Warning: Selected {len(indices)} videos instead of {num_videos}\", \"WARNING\")\n",
        "                            for idx in indices[:num_videos]:\n",
        "                                if 1 <= idx <= len(available_videos):\n",
        "                                    VIDEO_PATHS.append(available_videos[idx - 1])\n",
        "                        except ValueError:\n",
        "                            print_status(\"Invalid input, selecting first videos\", \"WARNING\")\n",
        "                            VIDEO_PATHS = available_videos[:num_videos]\n",
        "\n",
        "                print_status(f\"Selected {len(VIDEO_PATHS)} video(s)\", \"SUCCESS\")\n",
        "                for video in VIDEO_PATHS:\n",
        "                    print(f\"  - {video.name}\")\n",
        "            else:\n",
        "                print_status(f\"Invalid number. Must be between 1 and {len(available_videos)}\", \"ERROR\")\n",
        "        else:\n",
        "            print_status(\"Invalid input\", \"ERROR\")\n",
        "\n",
        "        if not VIDEO_PATHS:\n",
        "            print_status(\"No videos selected\", \"ERROR\")\n",
        "\n",
        "except Exception as e:\n",
        "    print_status(f\"Download failed: {str(e)}\", \"ERROR\")\n",
        "    print(\"\\nNote: Make sure the folder is set to 'Anyone with the link can view'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract_clips"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Prepare videos and clips\n",
        "\n",
        "import cv2\n",
        "import subprocess\n",
        "\n",
        "CLIP_DURATION = 60\n",
        "\n",
        "# Prepare both full videos and clips\n",
        "FULL_VIDEOS = {}\n",
        "VIDEO_CLIPS = {}\n",
        "\n",
        "for VIDEO_PATH in VIDEO_PATHS:\n",
        "    VIDEO_NAME = VIDEO_PATH.stem\n",
        "\n",
        "    print(f\"\\nPREPARING: {VIDEO_NAME}\")\n",
        "\n",
        "    # Store full video path\n",
        "    FULL_VIDEOS[VIDEO_NAME] = {\"full\": VIDEO_PATH}\n",
        "\n",
        "    # Get video info for clip extraction\n",
        "    cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = total_frames / fps\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"Duration: {duration:.1f}s | FPS: {fps:.1f} | Frames: {total_frames}\")\n",
        "\n",
        "    # Determine clip positions\n",
        "    if duration < CLIP_DURATION * 3:\n",
        "        if duration < CLIP_DURATION:\n",
        "            CLIPS = [(0, duration, \"full\")]\n",
        "            print_status(f\"Video shorter than {CLIP_DURATION}s, will use full video\", \"INFO\")\n",
        "        else:\n",
        "            CLIPS = [\n",
        "                (0, CLIP_DURATION, \"start\"),\n",
        "                (max(duration - CLIP_DURATION, 0), CLIP_DURATION, \"end\")\n",
        "            ]\n",
        "            print_status(\"Will extract start and end clips\", \"INFO\")\n",
        "    else:\n",
        "        CLIPS = [\n",
        "            (0, CLIP_DURATION, \"start\"),\n",
        "            ((duration - CLIP_DURATION) / 2, CLIP_DURATION, \"middle\"),\n",
        "            (duration - CLIP_DURATION, CLIP_DURATION, \"end\")\n",
        "        ]\n",
        "        print_status(\"Will extract start, middle, and end clips\", \"INFO\")\n",
        "\n",
        "    # Extract clips\n",
        "    CLIP_PATHS = {}\n",
        "    for start_time, clip_dur, position in CLIPS:\n",
        "        clip_name = f\"{VIDEO_NAME}_{position}.mp4\"\n",
        "        clip_path = CLIPS_DIR / clip_name\n",
        "\n",
        "        cmd = [\n",
        "            \"ffmpeg\", \"-i\", str(VIDEO_PATH),\n",
        "            \"-ss\", str(start_time),\n",
        "            \"-t\", str(clip_dur),\n",
        "            \"-c\", \"copy\",\n",
        "            str(clip_path),\n",
        "            \"-y\",\n",
        "            \"-loglevel\", \"error\"\n",
        "        ]\n",
        "\n",
        "        result = subprocess.run(cmd, capture_output=True)\n",
        "\n",
        "        if result.returncode == 0 and clip_path.exists():\n",
        "            CLIP_PATHS[position] = clip_path\n",
        "            size_mb = clip_path.stat().st_size / (1024 * 1024)\n",
        "            print_status(f\"Clip '{position}' ready ({size_mb:.1f} MB)\", \"SUCCESS\")\n",
        "        else:\n",
        "            print_status(f\"Failed to extract '{position}' clip\", \"ERROR\")\n",
        "\n",
        "    VIDEO_CLIPS[VIDEO_NAME] = CLIP_PATHS\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"PREPARATION COMPLETE\")\n",
        "print(f\"Prepared {len(VIDEO_PATHS)} video(s) with both full and clip options\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "H_etsZN8K1QG"
      },
      "outputs": [],
      "source": [
        "# Cell: Setup Darkmyter (Original ByteTrack + YOLO - Authentic Implementation)\n",
        "\n",
        "print_status(\"Setting up Darkmyter tracking...\", \"INFO\")\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "darkmyter_dir = REPOS_DIR / \"darkmyter\"\n",
        "\n",
        "# Clone original ByteTrack repo (required for authentic Darkmyter)\n",
        "bytetrack_dir = darkmyter_dir / \"ByteTrack\"\n",
        "if not bytetrack_dir.exists():\n",
        "    print_status(\"Cloning original ByteTrack repository...\", \"INFO\")\n",
        "    subprocess.run([\n",
        "        \"git\", \"clone\", \"--depth\", \"1\",\n",
        "        \"https://github.com/ifzhang/ByteTrack.git\",\n",
        "        str(bytetrack_dir)\n",
        "    ], check=True)\n",
        "\n",
        "# Install ByteTrack dependencies\n",
        "!pip install -q cython lap cython_bbox\n",
        "\n",
        "# Download football-specific weights\n",
        "weights_dir = darkmyter_dir / \"yolov8-weights\"\n",
        "weights_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "custom_weights = weights_dir / \"yolov8l-football-players.pt\"\n",
        "gdrive_id = \"12dWRBsegmyGE3feTdy9LBf1eZ-hTZ9Sx\"\n",
        "\n",
        "def download_darkmyter_weights():\n",
        "    print_status(\"Downloading Darkmyter football weights...\", \"INFO\")\n",
        "    try:\n",
        "        import gdown\n",
        "        url = f\"https://drive.google.com/uc?id={gdrive_id}\"\n",
        "        gdown.download(url, str(custom_weights), quiet=False)\n",
        "        print_status(\"Darkmyter weights downloaded\", \"SUCCESS\")\n",
        "    except Exception as e:\n",
        "        print_status(f\"Failed to download weights: {e}\", \"ERROR\")\n",
        "\n",
        "if custom_weights.exists():\n",
        "    try:\n",
        "        with open(custom_weights, \"rb\") as f:\n",
        "            header = f.read(16)\n",
        "        if header.startswith(b\"<\"):\n",
        "            print_status(\"Weights file is HTML, re-downloading...\", \"ERROR\")\n",
        "            custom_weights.unlink(missing_ok=True)\n",
        "            download_darkmyter_weights()\n",
        "        else:\n",
        "            print_status(\"Darkmyter weights already present\", \"SUCCESS\")\n",
        "    except Exception:\n",
        "        custom_weights.unlink(missing_ok=True)\n",
        "        download_darkmyter_weights()\n",
        "else:\n",
        "    download_darkmyter_weights()\n",
        "\n",
        "# Create Darkmyter wrapper\n",
        "darkmyter_wrapper = darkmyter_dir / \"run_darkmyter.py\"\n",
        "darkmyter_wrapper.write_text('''\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Darkmyter: YOLOv8 + original ByteTrack (football, notebook-authentic).\n",
        "\n",
        "This script is a CLI version of the Roboflow \"track players with ByteTrack + YOLOv8\"\n",
        "notebook, adapted to output JSON instead of an annotated video.\n",
        "\n",
        "It:\n",
        "- Uses yolov8l-football-players.pt if present, else falls back to yolov8x.pt\n",
        "- Uses original ifzhang/ByteTrack\n",
        "- Uses football-specific BYTETrackerArgs\n",
        "- Uses the same format_predictions + match_detections_with_tracks pattern\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    import torch\n",
        "except ImportError:\n",
        "    print(\"Error: ultralytics or torch not installed\", file=sys.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "# Original ByteTrack imports\n",
        "BYTETRACK_PATH = Path(__file__).resolve().parent / \"ByteTrack\"\n",
        "sys.path.insert(0, str(BYTETRACK_PATH))\n",
        "\n",
        "try:\n",
        "    from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
        "except ImportError:\n",
        "    print(\"Error: ByteTrack repo not found; expected at ./ByteTrack\", file=sys.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "try:\n",
        "    from onemetric.cv.utils.iou import box_iou_batch\n",
        "except ImportError:\n",
        "    print(\"Error: onemetric not installed (needed for IoU). \"\n",
        "          \"Install with: pip install onemetric\", file=sys.stderr)\n",
        "    sys.exit(1)\n",
        "try:\n",
        "    from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
        "except ImportError as e:\n",
        "    import traceback\n",
        "    print(\"Error importing ByteTrack from ./ByteTrack:\", e, file=sys.stderr)\n",
        "    traceback.print_exc()\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "#BYTETrackerArgs: football-specific params (from notebook)\n",
        "@dataclass(frozen=True)\n",
        "class BYTETrackerArgs:\n",
        "    track_thresh: float = 0.25\n",
        "    track_buffer: int = 30\n",
        "    match_thresh: float = 0.8\n",
        "    aspect_ratio_thresh: float = 3.0\n",
        "    min_box_area: float = 1.0\n",
        "    mot20: bool = False\n",
        "\n",
        "\n",
        "# Same mapping as in the notebook\n",
        "IND_TO_CLS = {\n",
        "    0: \"ball\",\n",
        "    1: \"goalkeeper\",\n",
        "    2: \"player\",\n",
        "    3: \"referee\",\n",
        "}\n",
        "\n",
        "\n",
        "def format_predictions(predictions, with_conf: bool = True) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Format YOLO detections to ByteTrack format: (x1, y1, x2, y2, conf).\n",
        "\n",
        "    This mirrors the notebook's function exactly:\n",
        "        bbox = pred.boxes.xyxy.int().tolist()[0]\n",
        "        conf = pred.boxes.conf.item()\n",
        "    \"\"\"\n",
        "    frame_detections = []\n",
        "    for pred in predictions:\n",
        "        # pred is a ultralytics Results object with a single box\n",
        "        bbox = pred.boxes.xyxy.int().tolist()[0]  # [x1, y1, x2, y2]\n",
        "        conf = float(pred.boxes.conf.item())\n",
        "        if with_conf:\n",
        "            detection = bbox + [conf]\n",
        "        else:\n",
        "            detection = bbox\n",
        "        frame_detections.append(detection)\n",
        "\n",
        "    if not frame_detections:\n",
        "        # shape must be (0, 5) or (0, 4) depending on with_conf\n",
        "        return np.zeros((0, 5 if with_conf else 4), dtype=float)\n",
        "\n",
        "    return np.array(frame_detections, dtype=float)\n",
        "\n",
        "\n",
        "def match_detections_with_tracks(detections, tracks):\n",
        "    \"\"\"\n",
        "    Notebook-authentic matching:\n",
        "\n",
        "    - Build detections_bboxes using format_predictions(with_conf=False)\n",
        "    - Build tracks_bboxes from track.tlbr\n",
        "    - Compute IoU matrix with box_iou_batch\n",
        "    - For each track, assign its track_id to the best IoU detection if IoU != 0\n",
        "    \"\"\"\n",
        "    if not detections or not tracks:\n",
        "        return detections\n",
        "\n",
        "    detections_bboxes = format_predictions(detections, with_conf=False)\n",
        "    tracks_bboxes = np.array([track.tlbr for track in tracks], dtype=float)\n",
        "\n",
        "    iou = box_iou_batch(tracks_bboxes, detections_bboxes)  # [num_tracks, num_dets]\n",
        "    track2detection = np.argmax(iou, axis=1)\n",
        "\n",
        "    for tracker_index, detection_index in enumerate(track2detection):\n",
        "        if iou[tracker_index, detection_index] != 0:\n",
        "            detections[detection_index].tracker_id = tracks[tracker_index].track_id\n",
        "\n",
        "    return detections\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Darkmyter: YOLOv8 + original ByteTrack (football)\"\n",
        "    )\n",
        "    parser.add_argument(\"--video\", required=True, help=\"Path to input video\")\n",
        "    parser.add_argument(\"--output\", required=True, help=\"Path to output JSON file\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    video_path = Path(args.video)\n",
        "    output_path = Path(args.output)\n",
        "\n",
        "    if not video_path.exists():\n",
        "        print(f\"Error: video not found: {video_path}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load YOLO model (football weights if available)\n",
        "    repo_root = Path(__file__).resolve().parent\n",
        "    custom_weights = repo_root / \"yolov8-weights\" / \"yolov8l-football-players.pt\"\n",
        "\n",
        "    if custom_weights.exists():\n",
        "        print(f\"[Darkmyter] Using football-specific weights: {custom_weights}\", file=sys.stderr)\n",
        "        model = YOLO(str(custom_weights))\n",
        "        model_name = \"yolov8l-football\"\n",
        "        football_specific = True\n",
        "    else:\n",
        "        print(\"[Darkmyter] Football weights not found, using yolov8x.pt\", file=sys.stderr)\n",
        "        model = YOLO(\"yolov8x.pt\")\n",
        "        model_name = \"yolov8x\"\n",
        "        football_specific = False\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"[Darkmyter] Device: {device}\", file=sys.stderr)\n",
        "\n",
        "    # Open video\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: cannot open video: {video_path}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    if fps is None or fps <= 0:\n",
        "        fps = 30.0\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.get(cv2.CAP_PROP_FRAME_COUNT) > 0 else -1\n",
        "\n",
        "    print(f\"[Darkmyter] FPS={fps:.1f}, total_frames={total_frames}\", file=sys.stderr)\n",
        "\n",
        "    # Initialize ByteTrack (with proper frame_rate)\n",
        "    tracker = BYTETracker(BYTETrackerArgs(), frame_rate=int(round(fps)))\n",
        "\n",
        "    detections_json = []\n",
        "    total_tracks = set()\n",
        "    frame_idx = 0\n",
        "\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Notebook: detections = yolo_model(frame, verbose=0)[0]\n",
        "        results = model(frame, verbose=False)[0]\n",
        "\n",
        "        # \"detections\" in the notebook is iterable; each element is a single-box Results\n",
        "        detections_with_tracker = []\n",
        "        for detection in results:\n",
        "            detection.tracker_id = \"\"  # will be filled in after tracking\n",
        "            detections_with_tracker.append(detection)\n",
        "\n",
        "        if detections_with_tracker:\n",
        "            # get trackers with ByteTrack\n",
        "            bt_input = format_predictions(detections_with_tracker, with_conf=True)\n",
        "\n",
        "            tracks = tracker.update(\n",
        "                output_results=bt_input,\n",
        "                img_info=frame.shape,\n",
        "                img_size=frame.shape,\n",
        "            )\n",
        "\n",
        "            # set tracker_id in yolo detections\n",
        "            detections_with_tracker = match_detections_with_tracks(\n",
        "                detections_with_tracker,\n",
        "                tracks,\n",
        "            )\n",
        "\n",
        "            # Convert to JSON rows\n",
        "            for det in detections_with_tracker:\n",
        "                if det.tracker_id == \"\":\n",
        "                    continue\n",
        "\n",
        "                # Single box per det\n",
        "                bbox = det.boxes.xyxy.tolist()[0]\n",
        "                x1, y1, x2, y2 = map(float, bbox)\n",
        "                conf = float(det.boxes.conf.item())\n",
        "                cls_idx = int(det.boxes.cls.item()) if det.boxes.cls is not None else 0\n",
        "\n",
        "                detections_json.append(\n",
        "                    {\n",
        "                        \"frame_id\": int(frame_idx),\n",
        "                        \"track_id\": int(det.tracker_id),\n",
        "                        \"bbox\": [x1, y1, x2, y2],\n",
        "                        \"score\": conf,\n",
        "                        \"class_id\": cls_idx,\n",
        "                        \"class_name\": IND_TO_CLS.get(cls_idx, \"unknown\"),\n",
        "                    }\n",
        "                )\n",
        "                total_tracks.add(int(det.tracker_id))\n",
        "\n",
        "        if frame_idx % 100 == 0:\n",
        "            if total_frames > 0:\n",
        "                pct = 100.0 * frame_idx / total_frames\n",
        "                print(f\"[Darkmyter] Processed {frame_idx}/{total_frames} frames ({pct:.1f}%)\",\n",
        "                      file=sys.stderr)\n",
        "            else:\n",
        "                print(f\"[Darkmyter] Processed {frame_idx} frames...\", file=sys.stderr)\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    stats = {\n",
        "        \"total_tracks\": len(total_tracks),\n",
        "        \"frames_processed\": frame_idx,\n",
        "    }\n",
        "\n",
        "    full_output = {\n",
        "        \"framework\": \"Darkmyter\",\n",
        "        \"model\": model_name,\n",
        "        \"tracker\": \"ByteTrack (ifzhang/ByteTrack)\",\n",
        "        \"tracker_params\": {\n",
        "            \"track_thresh\": BYTETrackerArgs.track_thresh,\n",
        "            \"track_buffer\": BYTETrackerArgs.track_buffer,\n",
        "            \"match_thresh\": BYTETrackerArgs.match_thresh,\n",
        "            \"aspect_ratio_thresh\": BYTETrackerArgs.aspect_ratio_thresh,\n",
        "            \"min_box_area\": BYTETrackerArgs.min_box_area,\n",
        "        },\n",
        "        \"features\": {\n",
        "            \"football_specific\": football_specific,\n",
        "            \"original_bytetrack\": True,\n",
        "        },\n",
        "        \"detections\": detections_json,\n",
        "        \"statistics\": stats,\n",
        "    }\n",
        "\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with output_path.open(\"w\") as f:\n",
        "        json.dump(full_output, f, indent=2)\n",
        "\n",
        "    print(\n",
        "        f\"[Darkmyter] Complete: {len(detections_json)} detections, \"\n",
        "        f\"{stats['total_tracks']} tracks. Output saved to: {output_path}\",\n",
        "        file=sys.stderr,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "\n",
        "darkmyter_wrapper.chmod(0o755)\n",
        "print_status(\"Darkmyter wrapper created (original ByteTrack)\", \"SUCCESS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2ES5pWHUmqE"
      },
      "outputs": [],
      "source": [
        "# Patch ByteTrack for NumPy 1.24+ compatibility\n",
        "import os\n",
        "\n",
        "bytetrack_path = REPOS_DIR / \"darkmyter\" / \"ByteTrack\"\n",
        "\n",
        "# Files that typically have np.float issues\n",
        "for root, dirs, files in os.walk(bytetrack_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.py'):\n",
        "            filepath = os.path.join(root, file)\n",
        "            try:\n",
        "                with open(filepath, 'r') as f:\n",
        "                    content = f.read()\n",
        "\n",
        "                # Replace deprecated numpy aliases\n",
        "                new_content = content\n",
        "                new_content = new_content.replace('np.float,', 'float,')\n",
        "                new_content = new_content.replace('np.float)', 'float)')\n",
        "                new_content = new_content.replace('np.float]', 'float]')\n",
        "                new_content = new_content.replace('np.int,', 'int,')\n",
        "                new_content = new_content.replace('np.int)', 'int)')\n",
        "                new_content = new_content.replace('np.int]', 'int]')\n",
        "\n",
        "                if new_content != content:\n",
        "                    with open(filepath, 'w') as f:\n",
        "                        f.write(new_content)\n",
        "                    print(f\"Patched: {filepath}\")\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "print(\"ByteTrack patched for NumPy compatibility\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I5iE6qiWWTy"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import ultralytics\n",
        "\n",
        "REPOS_DIR = Path(\"/content/repositories\")\n",
        "ultra_dir = REPOS_DIR / \"ultra_trackers\"\n",
        "ultra_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# 1) Choose custom configs for bytetrack + botsort\n",
        "runner_script = ultra_dir / \"run_ultra_yolo_tracker.py\"\n",
        "runner_script.write_text(textwrap.dedent(\"\"\"\\\n",
        "    #!/usr/bin/env python\n",
        "    \\\"\\\"\\\"Run Ultralytics YOLO (v5 / v8 / v11 weights) with a chosen tracker and dump JSON tracks.\n",
        "\n",
        "    Usage:\n",
        "      python run_ultra_yolo_tracker.py \\\\\n",
        "          --video input.mp4 \\\\\n",
        "          --output output.json \\\\\n",
        "          --weights yolo11m.pt \\\\\n",
        "          --tracker botsort\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    import argparse\n",
        "    import json\n",
        "    from pathlib import Path\n",
        "    from ultralytics import YOLO\n",
        "    import yaml\n",
        "\n",
        "    def main():\n",
        "        parser = argparse.ArgumentParser(description=\"YOLO + tracker to JSON\")\n",
        "        parser.add_argument(\"--video\", required=True, help=\"Path to input video\")\n",
        "        parser.add_argument(\"--output\", required=True, help=\"Path to output JSON\")\n",
        "        parser.add_argument(\n",
        "            \"--weights\",\n",
        "            default=\"yolo11m.pt\",\n",
        "            help=\"YOLO weights (e.g., yolov5s.pt, yolov8n.pt, yolo11m.pt, ...)\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--tracker\",\n",
        "            default=\"botsort\",\n",
        "            choices=[\"botsort\", \"bytetrack\", \"deepsort\"],\n",
        "            help=\"Which tracker to use\",\n",
        "        )\n",
        "        parser.add_argument(\"--conf\", type=float, default=0.3,\n",
        "                    help=\"Confidence threshold (detector)\")\n",
        "        parser.add_argument(\"--iou\", type=float, default=0.4,\n",
        "                    help=\"IOU threshold for NMS (lower = keep more boxes)\")\n",
        "        parser.add_argument(\"--imgsz\", type=int, default=1280,\n",
        "                    help=\"Image size for inference\")\n",
        "        parser.add_argument(\"--max-det\", type=int, default=300,\n",
        "                    help=\"Maximum detections per image\")\n",
        "\n",
        "        args = parser.parse_args()\n",
        "\n",
        "        video_path = Path(args.video)\n",
        "        out_path = Path(args.output)\n",
        "\n",
        "        if not video_path.exists():\n",
        "            raise SystemExit(f\"Video not found: {video_path}\")\n",
        "\n",
        "        # Load YOLO model\n",
        "        model = YOLO(args.weights)\n",
        "\n",
        "        # Get the script's directory for saving custom configs\n",
        "        ultra_root = Path(__file__).resolve().parent\n",
        "\n",
        "        # Try to load Ultralytics default configs first\n",
        "        import ultralytics\n",
        "        ultra_path = Path(ultralytics.__file__).parent\n",
        "        tracker_base_path = ultra_path / \"cfg\" / \"trackers\"\n",
        "\n",
        "        # Select the default config path\n",
        "        if args.tracker == \"bytetrack\":\n",
        "            default_cfg_path = tracker_base_path / \"bytetrack.yaml\"\n",
        "        elif args.tracker == \"botsort\":\n",
        "            default_cfg_path = tracker_base_path / \"botsort.yaml\"\n",
        "        else:  # deepsort\n",
        "            default_cfg_path = tracker_base_path / \"deepsort.yaml\"\n",
        "\n",
        "        # Path for our custom config\n",
        "        custom_cfg_path = ultra_root / f\"{args.tracker}_football.yaml\"\n",
        "\n",
        "        # Load and modify the config\n",
        "        if default_cfg_path.exists():\n",
        "            # Load the default config\n",
        "            with open(default_cfg_path, 'r') as f:\n",
        "                tracker_cfg = yaml.safe_load(f)\n",
        "\n",
        "            # Modify with football-optimized values from tracklab\n",
        "\n",
        "            if args.tracker == \"botsort\":\n",
        "                tracker_cfg.update({\n",
        "                    \"track_high_thresh\": 0.33824964456239337,\n",
        "                    \"new_track_thresh\": 0.21144301345190655,\n",
        "                    \"track_buffer\": 60,\n",
        "                    \"match_thresh\": 0.22734550911325851,\n",
        "                    \"proximity_thresh\": 0.5945380911899254,\n",
        "                    \"appearance_thresh\": 0.4818211117541298,\n",
        "                    \"cmc_method\": \"sparseOptFlow\",\n",
        "                    \"frame_rate\": 30,\n",
        "                    \"lambda_\": 0.9896143462366406,\n",
        "                    \"conf_thres\": 0.3501265956918775,\n",
        "                    \"with_reid\": True\n",
        "                })\n",
        "\n",
        "            # Save the modified config to a file\n",
        "            with open(custom_cfg_path, 'w') as f:\n",
        "                yaml.dump(tracker_cfg, f)\n",
        "\n",
        "            # Use the custom config FILE PATH (not the dictionary!)\n",
        "            tracker_cfg_path = str(custom_cfg_path)\n",
        "        else:\n",
        "            # Fallback: just use the default tracker name\n",
        "            print(f\"Warning: Could not find default config at {default_cfg_path}\")\n",
        "            print(f\"Using default tracker: {args.tracker}.yaml\")\n",
        "            tracker_cfg_path = f\"{args.tracker}.yaml\"\n",
        "\n",
        "        # Run tracking with the config FILE PATH\n",
        "        results = model.track(\n",
        "            source=str(video_path),\n",
        "            tracker=tracker_cfg_path,  # Pass the FILE PATH, not dictionary!\n",
        "            conf=args.conf,\n",
        "            iou=args.iou,\n",
        "            imgsz=args.imgsz,\n",
        "            max_det=args.max_det,\n",
        "            stream=True,\n",
        "            device=0,\n",
        "            save=False,\n",
        "            verbose=False,\n",
        "            persist=True,\n",
        "            vid_stride=1,\n",
        "        )\n",
        "\n",
        "        print(f\"Tracking with {args.tracker} on device: {model.device}\")\n",
        "\n",
        "        all_detections = []\n",
        "        frame_idx = 0\n",
        "\n",
        "        for r in results:\n",
        "            boxes = r.boxes\n",
        "            if boxes is None:\n",
        "                frame_idx += 1\n",
        "                continue\n",
        "\n",
        "            ids = boxes.id\n",
        "            if ids is None:\n",
        "                frame_idx += 1\n",
        "                continue\n",
        "\n",
        "            xyxy = boxes.xyxy\n",
        "            confs = boxes.conf\n",
        "            clses = boxes.cls\n",
        "\n",
        "            ids = ids.cpu().tolist()\n",
        "            xyxy = xyxy.cpu().tolist()\n",
        "            confs = confs.cpu().tolist()\n",
        "            clses = clses.cpu().tolist()\n",
        "\n",
        "            for tid, (x1, y1, x2, y2), score, c in zip(ids, xyxy, confs, clses):\n",
        "                all_detections.append({\n",
        "                    \"frame_id\": frame_idx,\n",
        "                    \"track_id\": int(tid),\n",
        "                    \"bbox\": [float(x1), float(y1), float(x2), float(y2)],\n",
        "                    \"score\": float(score),\n",
        "                    \"class_id\": int(c),\n",
        "                })\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with out_path.open(\"w\") as f:\n",
        "            json.dump(all_detections, f)\n",
        "\n",
        "        print(f\"Wrote {len(all_detections)} tracked detections to {out_path}\")\n",
        "\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        main()\n",
        "    \"\"\"))\n",
        "\n",
        "runner_script.chmod(0o755)\n",
        "print_status(\"Created wrapper for Botsort and Bytetrack\", \"SUCCESS\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jIMyv7HMfsbb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cell: Setup Eagle with Python 3.13\n",
        "\n",
        "print_status(\"Setting up Eagle with Python 3.13...\", \"INFO\")\n",
        "\n",
        "eagle_dir = REPOS_DIR / \"eagle\"\n",
        "\n",
        "# Install Python 3.13 (Eagle's required version)\n",
        "print_status(\"Installing Python 3.13...\", \"INFO\")\n",
        "!apt-get update -qq\n",
        "!apt-get install -y software-properties-common\n",
        "!add-apt-repository -y ppa:deadsnakes/ppa\n",
        "!apt-get update -qq\n",
        "!apt-get install -y python3.13 python3.13-venv python3.13-dev python3.13-distutils\n",
        "\n",
        "# Install pip for Python 3.13\n",
        "!curl -sS https://bootstrap.pypa.io/get-pip.py | python3.13\n",
        "\n",
        "# Install uv if not already installed\n",
        "print_status(\"Installing uv...\", \"INFO\")\n",
        "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "\n",
        "# Add uv to PATH\n",
        "import os\n",
        "os.environ['PATH'] = f\"/root/.local/bin:{os.environ['PATH']}\"\n",
        "\n",
        "# Create Eagle environment with Python 3.13\n",
        "os.chdir(eagle_dir)\n",
        "print_status(\"Creating Eagle environment with Python 3.13...\", \"INFO\")\n",
        "!uv venv --python python3.13\n",
        "!uv sync\n",
        "\n",
        "# Download model weights\n",
        "print_status(\"Downloading Eagle model weights...\", \"INFO\")\n",
        "models_dir = eagle_dir / \"eagle\" / \"models\"\n",
        "if models_dir.exists():\n",
        "    os.chdir(models_dir)\n",
        "    !bash get_weights.sh\n",
        "    os.chdir(eagle_dir)\n",
        "    print_status(\"Eagle weights downloaded\", \"SUCCESS\")\n",
        "else:\n",
        "    print_status(\"Eagle models directory not found\", \"ERROR\")\n",
        "\n",
        "# Create Eagle wrapper that uses Python 3.13\n",
        "\n",
        "eagle_wrapper = eagle_dir / \"run_eagle.py\"\n",
        "eagle_wrapper.write_text('''\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Clean Eagle wrapper that produces a single output file\n",
        "Outputs Eagle's native raw format directly\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "\n",
        "def get_eagle_output(eagle_output_dir):\n",
        "    \"\"\"\n",
        "    Find and return Eagle's raw output data\n",
        "\n",
        "    Args:\n",
        "        eagle_output_dir: Path to Eagle's output directory\n",
        "\n",
        "    Returns:\n",
        "        Raw Eagle data dictionary\n",
        "    \"\"\"\n",
        "    coords_dir = eagle_output_dir / \"raw_coordinates\"\n",
        "    if not coords_dir.exists():\n",
        "        coords_dir = eagle_output_dir\n",
        "\n",
        "    raw_coords_file = coords_dir / \"raw_coordinates.json\"\n",
        "    raw_data_file = coords_dir / \"raw_data.json\"\n",
        "    processed_file = coords_dir / \"processed_data.json\"\n",
        "\n",
        "    data = None\n",
        "    source_file = None\n",
        "\n",
        "    for file_path in [raw_coords_file, processed_file, raw_data_file]:\n",
        "        if file_path.exists():\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            source_file = file_path\n",
        "            print(f\"[Eagle] Using {file_path.name} as source\", file=sys.stderr)\n",
        "            break\n",
        "\n",
        "    if data is None:\n",
        "        json_files = list(coords_dir.glob(\"*.json\"))\n",
        "        if json_files:\n",
        "            with open(json_files[0], 'r') as f:\n",
        "                data = json.load(f)\n",
        "            source_file = json_files[0]\n",
        "            print(f\"[Eagle] Using {json_files[0].name} as source\", file=sys.stderr)\n",
        "\n",
        "    if data is None:\n",
        "        print(f\"[Eagle] No output files found in {coords_dir}\", file=sys.stderr)\n",
        "        raise FileNotFoundError(f\"No Eagle JSON outputs found in {coords_dir}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Eagle wrapper for unified output')\n",
        "    parser.add_argument('--video', required=True, help='Path to input video')\n",
        "    parser.add_argument('--output', required=True, help='Path to output JSON file')\n",
        "    parser.add_argument('--fps', default=24, type=int, help='FPS to process (default: 24)')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    video_path = Path(args.video)\n",
        "    output_path = Path(args.output)\n",
        "\n",
        "    if not video_path.exists():\n",
        "        print(f\"Error: Video not found: {video_path}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "    cmd = [\n",
        "        \"uv\", \"run\", \"--python\", \"python3.13\",\n",
        "        \"main.py\",\n",
        "        \"--video_path\", str(video_path),\n",
        "        \"--fps\", str(args.fps),\n",
        "    ]\n",
        "\n",
        "    print(f\"[Eagle] Processing {video_path.name} at {args.fps} FPS...\", file=sys.stderr)\n",
        "    start = time.time()\n",
        "\n",
        "    eagle_dir = Path(__file__).parent\n",
        "    result = subprocess.run(\n",
        "        cmd,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        cwd=eagle_dir,\n",
        "        env=env,\n",
        "    )\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"[Eagle] Processing took {elapsed:.1f}s\", file=sys.stderr)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(f\"[Eagle] Warning: Process returned {result.returncode}\", file=sys.stderr)\n",
        "        if result.stderr:\n",
        "            print(f\"[Eagle] Stderr: {result.stderr[:500]}\", file=sys.stderr)\n",
        "\n",
        "    video_stem = video_path.stem\n",
        "    eagle_output_base = eagle_dir / \"output\"\n",
        "    eagle_output_dir = eagle_output_base / video_stem\n",
        "\n",
        "    if not eagle_output_dir.exists():\n",
        "        for d in eagle_output_base.iterdir():\n",
        "            if d.is_dir() and video_stem in d.name:\n",
        "                eagle_output_dir = d\n",
        "                break\n",
        "\n",
        "    if not eagle_output_dir.exists():\n",
        "        print(f\"[Eagle] Error: Could not find output directory for {video_stem}\", file=sys.stderr)\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump([], f)\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        print(f\"[Eagle] Consolidating output from {eagle_output_dir}\", file=sys.stderr)\n",
        "        raw_data = get_eagle_output(eagle_output_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"[Eagle] Error while getting Eagle output: {e}\", file=sys.stderr)\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump([], f)\n",
        "        sys.exit(1)\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(raw_data, f, indent=2)\n",
        "\n",
        "    if isinstance(raw_data, dict):\n",
        "        print(f\"[Eagle] Output: {len(raw_data)} frames (raw format)\", file=sys.stderr)\n",
        "    elif isinstance(raw_data, list):\n",
        "        print(f\"[Eagle] Output: {len(raw_data)} frames\", file=sys.stderr)\n",
        "\n",
        "    print(f\"[Eagle] Saved to: {output_path}\", file=sys.stderr)\n",
        "    sys.exit(0)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "\n",
        "eagle_wrapper.chmod(0o755)\n",
        "print_status(\"Eagle  wrapper created\", \"SUCCESS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwKkf9KCsLQY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Base folder on Drive where all results will go\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/tracklab_eval\")  # change name if you like\n",
        "OUTPUT_DIR = BASE_DIR / \"results\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Results will be saved under:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EMRXc5GpkG4d"
      },
      "outputs": [],
      "source": [
        "# Cell: Final System Evaluation\n",
        "\n",
        "import time\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "import psutil\n",
        "import threading\n",
        "from pathlib import Path\n",
        "\n",
        "# System configurations\n",
        "SYSTEM_CONFIGS = {\n",
        "    \"eagle\": {\n",
        "        \"path\": REPOS_DIR / \"eagle\",\n",
        "        \"script\": \"run_eagle.py\",\n",
        "        \"python\": \"python3.13\",\n",
        "    },\n",
        "    \"yolo11_botsort\": {\n",
        "        \"path\": REPOS_DIR / \"ultra_trackers\",\n",
        "        \"script\": \"run_ultra_yolo_tracker.py\",\n",
        "        \"args\": [\"--weights\", \"yolo11m.pt\", \"--tracker\", \"botsort\"],\n",
        "    },\n",
        "    \"darkmyter\": {\n",
        "        \"path\": REPOS_DIR / \"darkmyter\",\n",
        "        \"script\": \"run_darkmyter.py\",\n",
        "    },\n",
        "}\n",
        "\n",
        "def get_gpu_memory():\n",
        "    \"\"\"Get current GPU memory usage in MB using nvidia-smi\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        return int(result.stdout.strip().split('\\n')[0])\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def monitor_memory(stop_event, memory_stats):\n",
        "    \"Background thread to sample memory usage\"\n",
        "    memory_stats['peak_ram_mb'] = 0\n",
        "    memory_stats['peak_gpu_mb'] = 0\n",
        "\n",
        "    while not stop_event.is_set():\n",
        "        ram_mb = psutil.virtual_memory().used / (1024 * 1024)\n",
        "        memory_stats['peak_ram_mb'] = max(memory_stats['peak_ram_mb'], ram_mb)\n",
        "\n",
        "        gpu_mb = get_gpu_memory()\n",
        "        if gpu_mb:\n",
        "            memory_stats['peak_gpu_mb'] = max(memory_stats['peak_gpu_mb'], gpu_mb)\n",
        "\n",
        "        stop_event.wait(0.5)\n",
        "\n",
        "# Ask user for processing mode\n",
        "print(\"EVALUATION MODE SELECTION \\n\")\n",
        "print(\"\\nHow do you want to evaluate the videos?\")\n",
        "print(\"  1. Use clips (faster - 60s segments)\")\n",
        "print(\"  2. Use full videos (comprehensive but slower)\")\n",
        "\n",
        "mode_choice = input(\"\\nEnter your choice (1 or 2): \").strip()\n",
        "USE_CLIPS = mode_choice != '2'\n",
        "\n",
        "if USE_CLIPS:\n",
        "    print_status(\"Mode: CLIP-BASED EVALUATION\", \"INFO\")\n",
        "    ALL_VIDEOS = VIDEO_CLIPS\n",
        "    eval_type = \"clips\"\n",
        "else:\n",
        "    print_status(\"Mode: FULL VIDEO EVALUATION\", \"INFO\")\n",
        "    ALL_VIDEOS = FULL_VIDEOS\n",
        "    eval_type = \"full\"\n",
        "\n",
        "position_to_number = {\"start\": 1, \"middle\": 2, \"end\": 3, \"full\": 1}\n",
        "\n",
        "def run_system_on_video(system_name, system_config, video_name, segment_name, video_path):\n",
        "    \"\"\"Run a tracking system on a video or clip\"\"\"\n",
        "\n",
        "    if USE_CLIPS:\n",
        "        segment_number = position_to_number.get(segment_name, 1)\n",
        "        output_dir = OUTPUT_DIR / video_name / \"clips\" / str(segment_number) / system_name\n",
        "    else:\n",
        "        output_dir = OUTPUT_DIR / video_name / \"full\" / system_name\n",
        "\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if USE_CLIPS:\n",
        "        print_status(f\"Running {system_name} on {video_name}/clip_{segment_number}...\", \"INFO\")\n",
        "    else:\n",
        "        print_status(f\"Running {system_name} on {video_name} (full video)...\", \"INFO\")\n",
        "\n",
        "    output_file = output_dir / f\"{system_name}_output.json\"\n",
        "    system_path = system_config.get(\"path\", REPOS_DIR)\n",
        "\n",
        "    # Build command\n",
        "    if system_name == \"eagle\":\n",
        "        cmd = [\n",
        "            \"uv\", \"run\", \"--python\", system_config.get(\"python\", \"python3.13\"),\n",
        "            \"run_eagle.py\",\n",
        "            \"--video\", str(video_path),\n",
        "            \"--output\", str(output_file),\n",
        "        ]\n",
        "    else:\n",
        "        cmd = [\n",
        "            \"python\", system_config[\"script\"],\n",
        "            \"--video\", str(video_path),\n",
        "            \"--output\", str(output_file),\n",
        "        ] + [str(extra) for extra in system_config.get(\"args\", [])]\n",
        "\n",
        "    # Get baseline memory before starting\n",
        "    baseline_ram = psutil.virtual_memory().used / (1024 * 1024)\n",
        "    baseline_gpu = get_gpu_memory() or 0\n",
        "\n",
        "    # Start memory monitoring\n",
        "    memory_stats = {}\n",
        "    stop_event = threading.Event()\n",
        "    monitor_thread = threading.Thread(target=monitor_memory, args=(stop_event, memory_stats))\n",
        "    monitor_thread.start()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            cwd=str(system_path),\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Stop monitoring\n",
        "        stop_event.set()\n",
        "        monitor_thread.join()\n",
        "\n",
        "        # Calculate peak usage above baseline\n",
        "        peak_ram_delta = max(0, memory_stats.get('peak_ram_mb', 0) - baseline_ram)\n",
        "        peak_gpu_delta = max(0, memory_stats.get('peak_gpu_mb', 0) - baseline_gpu)\n",
        "\n",
        "        if result.returncode == 0 and output_file.exists():\n",
        "            try:\n",
        "                with open(output_file) as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                if isinstance(data, list):\n",
        "                    num_detections = len(data)\n",
        "                elif isinstance(data, dict):\n",
        "                    num_detections = sum(\n",
        "                        len(dets) if isinstance(dets, list) else 0\n",
        "                        for dets in data.values()\n",
        "                    )\n",
        "                else:\n",
        "                    num_detections = 0\n",
        "\n",
        "                print_status(\n",
        "                    f\"{system_name}: SUCCESS - {num_detections} detections in {elapsed:.1f}s \"\n",
        "                    f\"(RAM: +{peak_ram_delta:.0f}MB, GPU: +{peak_gpu_delta:.0f}MB)\",\n",
        "                    \"SUCCESS\",\n",
        "                )\n",
        "                return {\n",
        "                    \"success\": True,\n",
        "                    \"time\": elapsed,\n",
        "                    \"output\": str(output_file),\n",
        "                    \"detections\": num_detections,\n",
        "                    \"peak_ram_mb\": peak_ram_delta,\n",
        "                    \"peak_gpu_mb\": peak_gpu_delta,\n",
        "                }\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print_status(f\"{system_name}: Invalid JSON\", \"ERROR\")\n",
        "                return {\n",
        "                    \"success\": False,\n",
        "                    \"time\": elapsed,\n",
        "                    \"error\": f\"Invalid JSON: {e}\",\n",
        "                    \"peak_ram_mb\": peak_ram_delta,\n",
        "                    \"peak_gpu_mb\": peak_gpu_delta,\n",
        "                }\n",
        "        else:\n",
        "            error_msg = result.stderr[-500:] if result.stderr else \"Unknown error\"\n",
        "            print_status(f\"{system_name}: FAILED\", \"ERROR\")\n",
        "            print(f\"Error: {error_msg}\")\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"time\": elapsed,\n",
        "                \"error\": error_msg,\n",
        "                \"peak_ram_mb\": peak_ram_delta,\n",
        "                \"peak_gpu_mb\": peak_gpu_delta,\n",
        "            }\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        stop_event.set()\n",
        "        monitor_thread.join()\n",
        "        peak_ram_delta = max(0, memory_stats.get('peak_ram_mb', 0) - baseline_ram)\n",
        "        peak_gpu_delta = max(0, memory_stats.get('peak_gpu_mb', 0) - baseline_gpu)\n",
        "        print_status(f\"{system_name}: TIMEOUT\", \"ERROR\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"time\": 10000,\n",
        "            \"error\": \"Timeout\",\n",
        "            \"peak_ram_mb\": peak_ram_delta,\n",
        "            \"peak_gpu_mb\": peak_gpu_delta,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        stop_event.set()\n",
        "        monitor_thread.join()\n",
        "        peak_ram_delta = max(0, memory_stats.get('peak_ram_mb', 0) - baseline_ram)\n",
        "        peak_gpu_delta = max(0, memory_stats.get('peak_gpu_mb', 0) - baseline_gpu)\n",
        "        print_status(f\"{system_name}: EXCEPTION - {str(e)}\", \"ERROR\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"time\": time.time() - start_time,\n",
        "            \"error\": str(e),\n",
        "            \"peak_ram_mb\": peak_ram_delta,\n",
        "            \"peak_gpu_mb\": peak_gpu_delta,\n",
        "        }\n",
        "\n",
        "def save_progress(all_results, eval_type):\n",
        "    \"\"\"Save current progress to disk (in Drive, via OUTPUT_DIR)\"\"\"\n",
        "    progress_file = OUTPUT_DIR / f\"progress_{eval_type}.json\"\n",
        "    with open(progress_file, \"w\") as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "    return progress_file\n",
        "\n",
        "def load_progress(eval_type):\n",
        "    \"Load existing progress if available\"\n",
        "    progress_file = OUTPUT_DIR / f\"progress_{eval_type}.json\"\n",
        "    if progress_file.exists():\n",
        "        try:\n",
        "            with open(progress_file) as f:\n",
        "                return json.load(f)\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "print(f\"STARTING {eval_type.upper()} EVALUATION\\n\")\n",
        "\n",
        "all_results = load_progress(eval_type)\n",
        "if all_results:\n",
        "    print_status(f\"Loaded existing progress with {len(all_results)} videos\", \"INFO\")\n",
        "    print(\"Videos already processed:\")\n",
        "    for video_name in all_results.keys():\n",
        "        print(f\"  - {video_name}\")\n",
        "\n",
        "    print(\"\\nDo you want to:\")\n",
        "    print(\"  1. Continue from where you left off\")\n",
        "    print(\"  2. Start fresh (delete existing progress)\")\n",
        "    continue_choice = input(\"\\nEnter your choice (1 or 2): \").strip()\n",
        "\n",
        "    if continue_choice == '2':\n",
        "        all_results = {}\n",
        "        print_status(\"Starting fresh evaluation\", \"INFO\")\n",
        "\n",
        "for video_name, segments in ALL_VIDEOS.items():\n",
        "    print(f\"\\nVIDEO: {video_name}\")\n",
        "\n",
        "    video_results = all_results.get(video_name, {})\n",
        "\n",
        "    for segment_name, video_path in segments.items():\n",
        "        if USE_CLIPS:\n",
        "            segment_number = position_to_number.get(segment_name, 1)\n",
        "            segment_key = f\"clip_{segment_number}\"\n",
        "            print(f\"\\nProcessing clip {segment_number} ({segment_name})...\")\n",
        "        else:\n",
        "            segment_key = \"full\"\n",
        "            print(f\"\\nProcessing full video...\")\n",
        "\n",
        "        segment_results = video_results.get(segment_key, {})\n",
        "        video_results[segment_key] = segment_results\n",
        "\n",
        "        for system_name, system_config in SYSTEM_CONFIGS.items():\n",
        "            if system_name in segment_results and segment_results[system_name].get(\"success\"):\n",
        "                print_status(f\"{system_name}: Already completed successfully\", \"SKIP\")\n",
        "                continue\n",
        "            elif system_name in segment_results:\n",
        "                print_status(f\"{system_name}: Retrying previous failure\", \"RETRY\")\n",
        "\n",
        "            result = run_system_on_video(\n",
        "                system_name, system_config, video_name, segment_name, video_path\n",
        "            )\n",
        "            segment_results[system_name] = result\n",
        "\n",
        "            all_results[video_name] = video_results\n",
        "            progress_file = save_progress(all_results, eval_type)\n",
        "            print_status(f\"Progress saved to {progress_file.name}\", \"SAVE\")\n",
        "\n",
        "        successful = sum(1 for r in segment_results.values() if r.get(\"success\", False))\n",
        "        total = len(segment_results)\n",
        "\n",
        "        if USE_CLIPS:\n",
        "            print(f\"\\nClip summary: {successful}/{total} systems succeeded\")\n",
        "        else:\n",
        "            print(f\"\\nVideo summary: {successful}/{total} systems succeeded\")\n",
        "\n",
        "    summary_file = OUTPUT_DIR / video_name / f\"summary_{eval_type}.json\"\n",
        "    summary_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(summary_file, \"w\") as f:\n",
        "        json.dump(video_results, f, indent=2)\n",
        "    print_status(f\"Video summary saved to {summary_file.name}\", \"SAVE\")\n",
        "\n",
        "# Save overall summary\n",
        "overall_summary = OUTPUT_DIR / f\"overall_summary_{eval_type}.json\"\n",
        "with open(overall_summary, \"w\") as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"{eval_type.upper()} EVALUATION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "system_stats = {\n",
        "    sys: {\n",
        "        \"success\": 0,\n",
        "        \"total\": 0,\n",
        "        \"avg_time\": [],\n",
        "        \"avg_detections\": [],\n",
        "        \"avg_ram\": [],\n",
        "        \"avg_gpu\": [],\n",
        "    }\n",
        "    for sys in SYSTEM_CONFIGS.keys()\n",
        "}\n",
        "\n",
        "for video_results in all_results.values():\n",
        "    for segment_results in video_results.values():\n",
        "        for system_name, result in segment_results.items():\n",
        "            if system_name in system_stats:\n",
        "                system_stats[system_name][\"total\"] += 1\n",
        "                if result.get(\"success\", False):\n",
        "                    system_stats[system_name][\"success\"] += 1\n",
        "                    if \"time\" in result:\n",
        "                        system_stats[system_name][\"avg_time\"].append(result[\"time\"])\n",
        "                    if \"detections\" in result:\n",
        "                        system_stats[system_name][\"avg_detections\"].append(result[\"detections\"])\n",
        "                    if \"peak_ram_mb\" in result:\n",
        "                        system_stats[system_name][\"avg_ram\"].append(result[\"peak_ram_mb\"])\n",
        "                    if \"peak_gpu_mb\" in result:\n",
        "                        system_stats[system_name][\"avg_gpu\"].append(result[\"peak_gpu_mb\"])\n",
        "\n",
        "print(\"\\nSystem Performance Summary:\")\n",
        "for system_name, stats in system_stats.items():\n",
        "    if stats[\"total\"] > 0:\n",
        "        success_rate = (stats[\"success\"] / stats[\"total\"]) * 100\n",
        "        print(f\"\\n{system_name}:\")\n",
        "        print(f\"  Success Rate: {stats['success']}/{stats['total']} ({success_rate:.1f}%)\")\n",
        "        if stats[\"avg_time\"]:\n",
        "            avg_time = sum(stats[\"avg_time\"]) / len(stats[\"avg_time\"])\n",
        "            print(f\"  Avg Time: {avg_time:.1f}s\")\n",
        "        if stats[\"avg_detections\"]:\n",
        "            avg_det = sum(stats[\"avg_detections\"]) / len(stats[\"avg_detections\"])\n",
        "            print(f\"  Avg Detections: {avg_det:.0f}\")\n",
        "        if stats[\"avg_ram\"]:\n",
        "            avg_ram = sum(stats[\"avg_ram\"]) / len(stats[\"avg_ram\"])\n",
        "            max_ram = max(stats[\"avg_ram\"])\n",
        "            print(f\"  Avg RAM: +{avg_ram:.0f}MB (peak: +{max_ram:.0f}MB)\")\n",
        "        if stats[\"avg_gpu\"]:\n",
        "            avg_gpu = sum(stats[\"avg_gpu\"]) / len(stats[\"avg_gpu\"])\n",
        "            max_gpu = max(stats[\"avg_gpu\"])\n",
        "            print(f\"  Avg GPU: +{avg_gpu:.0f}MB (peak: +{max_gpu:.0f}MB)\")\n",
        "\n",
        "print(f\"\\nResults Directory: {OUTPUT_DIR}\")\n",
        "print(f\"Overall Summary: {overall_summary}\")\n",
        "print(f\"Progress File: {OUTPUT_DIR / f'progress_{eval_type}.json'}\")\n",
        "\n",
        "total_expected = len(ALL_VIDEOS) * len(next(iter(ALL_VIDEOS.values())))\n",
        "total_processed = sum(len(video_results) for video_results in all_results.values())\n",
        "\n",
        "if total_processed == total_expected:\n",
        "    print(\"\\n\")\n",
        "    print(\"ALL VIDEOS PROCESSED SUCCESSFULLY!\")\n",
        "    print(\"Progress file kept for reference.\")\n",
        "else:\n",
        "    print(\"\\n\")\n",
        "    print(f\"PARTIAL COMPLETION: {total_processed}/{total_expected} segments processed\")\n",
        "    print(\"Run the script again to continue from where you left off.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download"
      },
      "outputs": [],
      "source": [
        "# Cell 9: Download results\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print_status(\"Creating archive...\", \"INFO\")\n",
        "\n",
        "archive_name = \"tracking_results\"\n",
        "archive_path = BASE_DIR / archive_name\n",
        "\n",
        "shutil.make_archive(str(archive_path), 'zip', OUTPUT_DIR)\n",
        "\n",
        "print_status(\"Downloading...\", \"SUCCESS\")\n",
        "files.download(f\"{archive_path}.zip\")\n",
        "\n",
        "print_status(\"Complete!\", \"SUCCESS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0imTFkAc84oC"
      },
      "outputs": [],
      "source": [
        "#Cell 10: Download pre-run results from google drive.\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"/content/results\"):\n",
        "    !gdown 1-TR0bycui1zpL5TRZLzhdxcuJIv9qLUO -O results.zip\n",
        "    !unzip results.zip -d /content\n",
        "    print(\"Using cached results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "OUTPUT_DIR = \"/content/comparison_output\"\n",
        "\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    print(f\"Error: {OUTPUT_DIR} does not exist!\")\n",
        "elif not os.listdir(OUTPUT_DIR):\n",
        "    print(f\"Error: {OUTPUT_DIR} is empty!\")\n",
        "else:\n",
        "    archive_path = \"/content/comparison_output\"\n",
        "    shutil.make_archive(archive_path, 'zip', OUTPUT_DIR)\n",
        "\n",
        "    zip_file = f\"{archive_path}.zip\"\n",
        "    if os.path.exists(zip_file):\n",
        "        print(f\"Archive created: {zip_file} ({os.path.getsize(zip_file)} bytes)\")\n",
        "        files.download(zip_file)\n",
        "    else:\n",
        "        print(\"Error: Archive was not created!\")"
      ],
      "metadata": {
        "id": "TOf0Se9XghXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL A: SHARED PARSERS AND DATA STRUCTURES\n",
        "# =============================================================================\n",
        "# Run this cell FIRST - it defines the parsing functions used by both\n",
        "# evaluation (Cell B) and visualization (Cell C)\n",
        "# =============================================================================\n",
        "\n",
        "\"\"\"\n",
        "DATA NORMALIZATION DOCUMENTATION\n",
        "\n",
        "\n",
        "This module normalizes outputs from three tracking systems into a unified format\n",
        "for comparison. The normalization process is MINIMALLY INVASIVE, preserving\n",
        "original data wherever possible.\n",
        "\n",
        "UNIFIED DETECTION FORMAT:\n",
        "    - frame_id: int (0-indexed frame number)\n",
        "    - track_id: int (unique identifier for tracked object)\n",
        "    - bbox: [x1, y1, x2, y2] (top-left and bottom-right corners in pixels)\n",
        "    - score: float (confidence score, 0.0-1.0)\n",
        "    - class_name: str (\"player\", \"referee\", \"ball\", \"goalkeeper\")\n",
        "\n",
        "SYSTEM-SPECIFIC NORMALIZATION:\n",
        "\n",
        "1. DARKMYTER (YOLOv8 + ByteTrack)\n",
        "   Original: {\"framework\": \"Darkmyter\", \"detections\": [...]}\n",
        "   Changes: None - already in target format\n",
        "\n",
        "2. EAGLE (YOLOv8 + BoT-SORT + HRNet)\n",
        "   Original: Hierarchical JSON with string keys\n",
        "   Changes:\n",
        "     - frame_id/track_id: string -> int\n",
        "     - \"BBox\" -> \"bbox\", \"Confidence\" -> \"score\"\n",
        "     - Ball detections: track_id = -1\n",
        "\n",
        "3. YOLO v11 + BoT-SORT\n",
        "   Original: List of detection dicts\n",
        "   Changes: class_name added based on class_id\n",
        "\n",
        "WHAT IS NOT CHANGED:\n",
        "    - Bounding box pixel coordinates\n",
        "    - Confidence/score values\n",
        "    - Frame IDs (0-indexing preserved)\n",
        "    - Track IDs (except Ball = -1 in Eagle)\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass, field\n",
        "from collections import defaultdict\n",
        "\n",
        "# DATA STRUCTURES\n",
        "\n",
        "@dataclass\n",
        "class Detection:\n",
        "    \"\"\"\n",
        "    Unified detection format across all tracking systems.\n",
        "    Original data preserved in 'raw' field when available.\n",
        "    \"\"\"\n",
        "    frame_id: int\n",
        "    track_id: int\n",
        "    bbox: List[float]  # [x1, y1, x2, y2]\n",
        "    score: float\n",
        "    class_name: str = \"player\"\n",
        "    class_id: Optional[int] = None\n",
        "    raw: Optional[Dict] = field(default=None, repr=False)\n",
        "\n",
        "    @property\n",
        "    def center(self) -> Tuple[float, float]:\n",
        "        return ((self.bbox[0] + self.bbox[2]) / 2,\n",
        "                (self.bbox[1] + self.bbox[3]) / 2)\n",
        "\n",
        "    @property\n",
        "    def width(self) -> float:\n",
        "        return self.bbox[2] - self.bbox[0]\n",
        "\n",
        "    @property\n",
        "    def height(self) -> float:\n",
        "        return self.bbox[3] - self.bbox[1]\n",
        "\n",
        "    @property\n",
        "    def area(self) -> float:\n",
        "        return self.width * self.height\n",
        "\n",
        "# PARSERS\n",
        "\n",
        "def parse_darkmyter(data: Dict, preserve_raw: bool = True) -> List[Detection]:\n",
        "    \"\"\"\n",
        "    Parse Darkmyter (YOLOv8 + ByteTrack) output.\n",
        "    No coordinate changes needed - already in target format.\n",
        "    \"\"\"\n",
        "    detections = []\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        det_list = data.get(\"detections\", [])\n",
        "    elif isinstance(data, list):\n",
        "        det_list = data\n",
        "    else:\n",
        "        return detections\n",
        "\n",
        "    for det in det_list:\n",
        "        if not isinstance(det, dict):\n",
        "            continue\n",
        "        bbox = det.get(\"bbox\", [])\n",
        "        if len(bbox) != 4:\n",
        "            continue\n",
        "\n",
        "        detections.append(Detection(\n",
        "            frame_id=int(det.get(\"frame_id\", 0)),\n",
        "            track_id=int(det.get(\"track_id\", 0)),\n",
        "            bbox=[float(b) for b in bbox],\n",
        "            score=float(det.get(\"score\", 0.0)),\n",
        "            class_name=det.get(\"class_name\", \"player\"),\n",
        "            class_id=det.get(\"class_id\"),\n",
        "            raw=det if preserve_raw else None\n",
        "        ))\n",
        "\n",
        "    return detections\n",
        "\n",
        "\n",
        "def parse_eagle(data: Dict, preserve_raw: bool = True) -> List[Detection]:\n",
        "    \"\"\"\n",
        "    Parse Eagle output.\n",
        "\n",
        "    Eagle format:\n",
        "    {\n",
        "        \"0\": {  # frame_id as string\n",
        "            \"Coordinates\": {\n",
        "                \"Player\": {\n",
        "                    \"1\": {\"BBox\": [x1, y1, x2, y2], \"Confidence\": 0.95},\n",
        "                    \"2\": {\"BBox\": [...], \"Confidence\": ...}\n",
        "                },\n",
        "                \"Ball\": {...},\n",
        "                \"Referee\": {...}\n",
        "            }\n",
        "        },\n",
        "        \"1\": {...}\n",
        "    }\n",
        "\n",
        "    Changes:\n",
        "        - frame_id/track_id: string -> int\n",
        "        - \"BBox\" -> \"bbox\", \"Confidence\" -> \"score\"\n",
        "        - Ball detections: track_id = -1\n",
        "    \"\"\"\n",
        "    detections = []\n",
        "    class_mapping = {\n",
        "        \"Player\": \"player\",\n",
        "        \"Ball\": \"ball\",\n",
        "        \"Referee\": \"referee\",\n",
        "        \"Goalkeeper\": \"goalkeeper\"\n",
        "    }\n",
        "\n",
        "    for frame_id_str, frame_data in data.items():\n",
        "        try:\n",
        "            frame_id = int(frame_id_str)\n",
        "        except (ValueError, TypeError):\n",
        "            continue\n",
        "\n",
        "        if not isinstance(frame_data, dict):\n",
        "            continue\n",
        "        coords = frame_data.get(\"Coordinates\", {})\n",
        "        if not coords:\n",
        "            continue\n",
        "\n",
        "        for category, objects in coords.items():\n",
        "            if not isinstance(objects, dict):\n",
        "                continue\n",
        "            class_name = class_mapping.get(category, category.lower())\n",
        "\n",
        "            for track_id_str, obj_data in objects.items():\n",
        "                if not isinstance(obj_data, dict):\n",
        "                    continue\n",
        "                bbox = obj_data.get(\"BBox\", [])\n",
        "                if len(bbox) != 4:\n",
        "                    continue\n",
        "\n",
        "                track_id = -1 if category == \"Ball\" else int(track_id_str)\n",
        "\n",
        "                detections.append(Detection(\n",
        "                    frame_id=frame_id,\n",
        "                    track_id=track_id,\n",
        "                    bbox=[float(b) for b in bbox],\n",
        "                    score=float(obj_data.get(\"Confidence\", 0.0)),\n",
        "                    class_name=class_name,\n",
        "                    class_id=None,\n",
        "                    raw=obj_data if preserve_raw else None\n",
        "                ))\n",
        "\n",
        "    return detections\n",
        "\n",
        "\n",
        "def parse_yolo_botsort(data: List, preserve_raw: bool = True) -> List[Detection]:\n",
        "    \"\"\"\n",
        "    Parse YOLO v11 + BoT-SORT output.\n",
        "    Adds class_name from class_id mapping.\n",
        "    \"\"\"\n",
        "    detections = []\n",
        "    class_mapping = {0: \"player\", 1: \"goalkeeper\", 2: \"referee\", 3: \"ball\"}\n",
        "\n",
        "    if not isinstance(data, list):\n",
        "        return detections\n",
        "\n",
        "    for det in data:\n",
        "        if not isinstance(det, dict):\n",
        "            continue\n",
        "        bbox = det.get(\"bbox\", [])\n",
        "        if len(bbox) != 4:\n",
        "            continue\n",
        "\n",
        "        class_id = det.get(\"class_id\", 0)\n",
        "        detections.append(Detection(\n",
        "            frame_id=int(det.get(\"frame_id\", 0)),\n",
        "            track_id=int(det.get(\"track_id\", 0)),\n",
        "            bbox=[float(b) for b in bbox],\n",
        "            score=float(det.get(\"score\", 0.0)),\n",
        "            class_name=class_mapping.get(class_id, \"player\"),\n",
        "            class_id=class_id,\n",
        "            raw=det if preserve_raw else None\n",
        "        ))\n",
        "\n",
        "    return detections\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# FILE LOADING UTILITIES\n",
        "# =============================================================================\n",
        "\n",
        "def detect_format(filepath: str) -> str:\n",
        "    \"\"\"Auto-detect tracking output format from file contents.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "    except Exception:\n",
        "        return \"unknown\"\n",
        "\n",
        "    if isinstance(data, list):\n",
        "        return \"yolo_botsort\"\n",
        "    elif isinstance(data, dict):\n",
        "        if \"detections\" in data or \"framework\" in data:\n",
        "            return \"darkmyter\"\n",
        "        # Check if keys are frame numbers (Eagle format)\n",
        "        if any(k.isdigit() for k in data.keys()):\n",
        "            # Verify it has the Coordinates structure\n",
        "            first_val = next(iter(data.values()), None)\n",
        "            if isinstance(first_val, dict) and \"Coordinates\" in first_val:\n",
        "                return \"eagle\"\n",
        "    return \"unknown\"\n",
        "\n",
        "\n",
        "def load_tracking_file(filepath: str, agent_name: str = \"\") -> List[Detection]:\n",
        "    \"\"\"\n",
        "    Load tracking data from file, auto-detecting format.\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to JSON file\n",
        "        agent_name: Optional hint for format detection\n",
        "\n",
        "    Returns:\n",
        "        List of Detection objects\n",
        "    \"\"\"\n",
        "    fmt = detect_format(filepath)\n",
        "\n",
        "    # Use agent name as hint if format unknown\n",
        "    if fmt == \"unknown\" and agent_name:\n",
        "        name_lower = agent_name.lower()\n",
        "        if \"darkmyter\" in name_lower:\n",
        "            fmt = \"darkmyter\"\n",
        "        elif \"eagle\" in name_lower:\n",
        "            fmt = \"eagle\"\n",
        "        elif \"yolo\" in name_lower or \"botsort\" in name_lower:\n",
        "            fmt = \"yolo_botsort\"\n",
        "\n",
        "    with open(filepath, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    parsers = {\n",
        "        \"darkmyter\": parse_darkmyter,\n",
        "        \"eagle\": parse_eagle,\n",
        "        \"yolo_botsort\": parse_yolo_botsort\n",
        "    }\n",
        "\n",
        "    if fmt in parsers:\n",
        "        return parsers[fmt](data)\n",
        "\n",
        "    # Try each parser as fallback\n",
        "    for parser in [parse_yolo_botsort, parse_darkmyter, parse_eagle]:\n",
        "        try:\n",
        "            dets = parser(data)\n",
        "            if dets:\n",
        "                return dets\n",
        "        except:\n",
        "            continue\n",
        "    return []\n",
        "\n",
        "\n",
        "def organize_detections_by_frame(detections: List[Detection]) -> Dict[int, List[Detection]]:\n",
        "    \"\"\"Group detections by frame_id.\"\"\"\n",
        "    by_frame = defaultdict(list)\n",
        "    for det in detections:\n",
        "        by_frame[det.frame_id].append(det)\n",
        "    return dict(by_frame)\n",
        "\n",
        "\n",
        "def organize_detections_by_track(detections: List[Detection],\n",
        "                                  class_filter: str = \"player\") -> Dict[int, List[Detection]]:\n",
        "    \"\"\"Group detections by track_id, optionally filtering by class.\"\"\"\n",
        "    tracks = defaultdict(list)\n",
        "    for det in detections:\n",
        "        if class_filter and det.class_name != class_filter:\n",
        "            continue\n",
        "        tracks[det.track_id].append(det)\n",
        "\n",
        "    # Sort each track by frame_id\n",
        "    for track_id in tracks:\n",
        "        tracks[track_id].sort(key=lambda d: d.frame_id)\n",
        "\n",
        "    return dict(tracks)\n",
        "\n",
        "\n",
        "\n",
        "print_status(\"Shared Parsers Loaded\", \"SUCCESS\")\n"
      ],
      "metadata": {
        "id": "cQ7Ova4P4KuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# EVALUATION AND METRICS\n",
        "\n",
        "\"\"\"\n",
        "Comparative evaluation of player tracking systems with multi-clip aggregation.\n",
        "\n",
        "DIRECTORY STRUCTURE EXPECTED:\n",
        "    results_dir/\n",
        "        video1/\n",
        "            full/\n",
        "                eagle/eagle_output.json\n",
        "                darkmyter/darkmyter_output.json\n",
        "                yolo11_botsort/yolo11_botsort_output.json\n",
        "        video2/\n",
        "            full/\n",
        "                ...\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# CONFIGURATION\n",
        "\n",
        "# Update this path to your results directory\n",
        "RESULTS_DIR = Path(\"/content/comp-4009-tracking-results\")\n",
        "OUTPUT_DIR = Path(\"/content/comparison_output\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "AGENTS = [\"darkmyter\", \"eagle\", \"yolo11_botsort\"]\n",
        "\n",
        "# SECTION 3.1: QUANTITATIVE METRICS\n",
        "\n",
        "class TrajectoryMetrics:\n",
        "    \"\"\"\n",
        "    Section 3.1 - Quantitative Metrics:\n",
        "        - Trajectory Smoothness (Jerk Score)\n",
        "        - Speed Plausibility\n",
        "        - Detection Completeness\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, detections: List[Detection], fps: float = 30.0,\n",
        "                 pixels_per_meter: float = 10.0):\n",
        "        self.detections = detections\n",
        "        self.fps = fps\n",
        "        self.pixels_per_meter = pixels_per_meter\n",
        "        self.dt = 1.0 / fps\n",
        "        self.tracks = self._organize_by_track()\n",
        "\n",
        "    def _organize_by_track(self) -> Dict[int, List[Detection]]:\n",
        "        tracks = defaultdict(list)\n",
        "        for det in self.detections:\n",
        "            if det.class_name == \"player\":\n",
        "                tracks[det.track_id].append(det)\n",
        "        for track_id in tracks:\n",
        "            tracks[track_id].sort(key=lambda d: d.frame_id)\n",
        "        return dict(tracks)\n",
        "\n",
        "    # 3.1.1 Trajectory Smoothness\n",
        "\n",
        "    def compute_jerk_score(self, track_id: int) -> Optional[float]:\n",
        "        \"\"\"Compute jerk (3rd derivative of position) for a track.\"\"\"\n",
        "        track = self.tracks.get(track_id, [])\n",
        "        if len(track) < 4:\n",
        "            return None\n",
        "\n",
        "        positions = np.array([det.center for det in track])\n",
        "        frames = np.array([det.frame_id for det in track])\n",
        "        jerks = []\n",
        "\n",
        "        for i in range(len(track) - 3):\n",
        "            p0, p1, p2, p3 = positions[i:i+4]\n",
        "            f0, f1, f2, f3 = frames[i:i+4]\n",
        "\n",
        "            dt01, dt12, dt23 = (f1-f0)*self.dt, (f2-f1)*self.dt, (f3-f2)*self.dt\n",
        "            if dt01 == 0 or dt12 == 0 or dt23 == 0:\n",
        "                continue\n",
        "\n",
        "            v1, v2, v3 = (p1-p0)/dt01, (p2-p1)/dt12, (p3-p2)/dt23\n",
        "            a1 = (v2 - v1) / ((dt01 + dt12) / 2)\n",
        "            a2 = (v3 - v2) / ((dt12 + dt23) / 2)\n",
        "            jerk = (a2 - a1) / ((dt01 + dt12 + dt23) / 3)\n",
        "            jerks.append(np.linalg.norm(jerk))\n",
        "\n",
        "        return np.mean(jerks) if jerks else None\n",
        "\n",
        "    def compute_all_jerk_scores(self) -> Dict[int, float]:\n",
        "        scores = {}\n",
        "        for track_id in self.tracks:\n",
        "            score = self.compute_jerk_score(track_id)\n",
        "            if score is not None:\n",
        "                scores[track_id] = score\n",
        "        return scores\n",
        "\n",
        "    def get_jerk_summary(self) -> Dict:\n",
        "        scores = self.compute_all_jerk_scores()\n",
        "        if not scores:\n",
        "            return {\"mean\": None, \"std\": None, \"min\": None, \"max\": None, \"count\": 0}\n",
        "        values = list(scores.values())\n",
        "        return {\n",
        "            \"mean\": float(np.mean(values)), \"std\": float(np.std(values)),\n",
        "            \"min\": float(np.min(values)), \"max\": float(np.max(values)),\n",
        "            \"count\": len(values)\n",
        "        }\n",
        "\n",
        "    # 3.1.2 Speed Plausibility\n",
        "\n",
        "    def compute_speeds(self, track_id: int) -> List[float]:\n",
        "        \"\"\"Compute instantaneous speeds in m/s.\"\"\"\n",
        "        track = self.tracks.get(track_id, [])\n",
        "        if len(track) < 2:\n",
        "            return []\n",
        "\n",
        "        speeds = []\n",
        "        for i in range(len(track) - 1):\n",
        "            p1, p2 = np.array(track[i].center), np.array(track[i+1].center)\n",
        "            frame_diff = track[i+1].frame_id - track[i].frame_id\n",
        "            if frame_diff <= 0:\n",
        "                continue\n",
        "            dist_meters = np.linalg.norm(p2 - p1) / self.pixels_per_meter\n",
        "            speeds.append(dist_meters / (frame_diff * self.dt))\n",
        "        return speeds\n",
        "\n",
        "    def compute_accelerations(self, track_id: int) -> List[float]:\n",
        "        speeds = self.compute_speeds(track_id)\n",
        "        if len(speeds) < 2:\n",
        "            return []\n",
        "        return [abs(speeds[i+1] - speeds[i]) / self.dt for i in range(len(speeds) - 1)]\n",
        "\n",
        "    def get_speed_plausibility(self, max_speed_ms: float = 11.11,\n",
        "                                max_accel_ms2: float = 5.0) -> Dict:\n",
        "        all_speeds, all_accels = [], []\n",
        "        for track_id in self.tracks:\n",
        "            all_speeds.extend(self.compute_speeds(track_id))\n",
        "            all_accels.extend(self.compute_accelerations(track_id))\n",
        "\n",
        "        if not all_speeds:\n",
        "            return {\n",
        "                \"speed_violation_rate\": None, \"accel_violation_rate\": None,\n",
        "                \"mean_speed_ms\": None, \"max_speed_ms\": None,\n",
        "                \"mean_accel_ms2\": None, \"max_accel_ms2\": None,\n",
        "                \"total_speed_samples\": 0, \"total_accel_samples\": 0\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"speed_violation_rate\": sum(1 for s in all_speeds if s > max_speed_ms) / len(all_speeds),\n",
        "            \"accel_violation_rate\": sum(1 for a in all_accels if a > max_accel_ms2) / len(all_accels) if all_accels else None,\n",
        "            \"mean_speed_ms\": float(np.mean(all_speeds)),\n",
        "            \"max_speed_ms\": float(np.max(all_speeds)),\n",
        "            \"mean_accel_ms2\": float(np.mean(all_accels)) if all_accels else None,\n",
        "            \"max_accel_ms2\": float(np.max(all_accels)) if all_accels else None,\n",
        "            \"total_speed_samples\": len(all_speeds),\n",
        "            \"total_accel_samples\": len(all_accels)\n",
        "        }\n",
        "\n",
        "    # 3.1.3 Detection Completeness\n",
        "\n",
        "    def get_detection_completeness(self, expected_players: int = 22) -> Dict:\n",
        "        frame_counts = defaultdict(int)\n",
        "        for det in self.detections:\n",
        "            if det.class_name == \"player\":\n",
        "                frame_counts[det.frame_id] += 1\n",
        "\n",
        "        if not frame_counts:\n",
        "            return {\n",
        "                \"frames_with_20_22_players\": 0, \"coverage_rate\": 0.0,\n",
        "                \"mean_players_per_frame\": 0.0, \"std_players_per_frame\": 0.0,\n",
        "                \"min_players_per_frame\": 0, \"max_players_per_frame\": 0,\n",
        "                \"total_frames\": 0, \"total_tracks\": 0,\n",
        "                \"mean_track_length\": 0.0, \"fragmentation_rate\": 0.0\n",
        "            }\n",
        "\n",
        "        counts = list(frame_counts.values())\n",
        "        frames_in_range = sum(1 for c in counts if 20 <= c <= 22)\n",
        "        track_lengths = [len(track) for track in self.tracks.values()]\n",
        "        total_frames = max(frame_counts.keys()) - min(frame_counts.keys()) + 1\n",
        "        short_tracks = sum(1 for l in track_lengths if l < total_frames * 0.1)\n",
        "\n",
        "        return {\n",
        "            \"frames_with_20_22_players\": frames_in_range,\n",
        "            \"coverage_rate\": frames_in_range / len(frame_counts),\n",
        "            \"mean_players_per_frame\": float(np.mean(counts)),\n",
        "            \"std_players_per_frame\": float(np.std(counts)),\n",
        "            \"min_players_per_frame\": int(np.min(counts)),\n",
        "            \"max_players_per_frame\": int(np.max(counts)),\n",
        "            \"total_frames\": len(frame_counts),\n",
        "            \"total_tracks\": len(self.tracks),\n",
        "            \"mean_track_length\": float(np.mean(track_lengths)) if track_lengths else 0.0,\n",
        "            \"fragmentation_rate\": short_tracks / len(self.tracks) if self.tracks else 0.0\n",
        "        }\n",
        "\n",
        "# SECTION 3.2: INTER-SYSTEM AGREEMENT\n",
        "\n",
        "class InterSystemAgreement:\n",
        "    \"\"\"\n",
        "    Section 3.2 - Inter-System Agreement:\n",
        "        - Position Distance\n",
        "        - Bounding Box Overlap (IoU)\n",
        "        - Velocity Correlation\n",
        "        - Disagreement Zones\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, system1_detections: List[Detection],\n",
        "                 system2_detections: List[Detection],\n",
        "                 system1_name: str = \"System1\",\n",
        "                 system2_name: str = \"System2\"):\n",
        "        self.sys1_dets = system1_detections\n",
        "        self.sys2_dets = system2_detections\n",
        "        self.sys1_name = system1_name\n",
        "        self.sys2_name = system2_name\n",
        "\n",
        "        self.sys1_by_frame = self._organize_by_frame(system1_detections)\n",
        "        self.sys2_by_frame = self._organize_by_frame(system2_detections)\n",
        "        self.common_frames = sorted(\n",
        "            set(self.sys1_by_frame.keys()) & set(self.sys2_by_frame.keys())\n",
        "        )\n",
        "\n",
        "    def _organize_by_frame(self, detections: List[Detection]) -> Dict[int, List[Detection]]:\n",
        "        by_frame = defaultdict(list)\n",
        "        for det in detections:\n",
        "            if det.class_name == \"player\":\n",
        "                by_frame[det.frame_id].append(det)\n",
        "        return dict(by_frame)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_iou(bbox1: List[float], bbox2: List[float]) -> float:\n",
        "        x1, y1 = max(bbox1[0], bbox2[0]), max(bbox1[1], bbox2[1])\n",
        "        x2, y2 = min(bbox1[2], bbox2[2]), min(bbox1[3], bbox2[3])\n",
        "\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            return 0.0\n",
        "\n",
        "        intersection = (x2 - x1) * (y2 - y1)\n",
        "        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
        "        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
        "        return intersection / (area1 + area2 - intersection)\n",
        "\n",
        "    def match_detections_hungarian(self, frame_id: int,\n",
        "                                    iou_threshold: float = 0.3) -> List[Tuple[Detection, Detection, float]]:\n",
        "        sys1_dets = self.sys1_by_frame.get(frame_id, [])\n",
        "        sys2_dets = self.sys2_by_frame.get(frame_id, [])\n",
        "\n",
        "        if not sys1_dets or not sys2_dets:\n",
        "            return []\n",
        "\n",
        "        matches, used_sys2 = [], set()\n",
        "        for det1 in sys1_dets:\n",
        "            best_iou, best_det2, best_idx = 0, None, -1\n",
        "            for idx, det2 in enumerate(sys2_dets):\n",
        "                if idx in used_sys2:\n",
        "                    continue\n",
        "                iou = self.compute_iou(det1.bbox, det2.bbox)\n",
        "                if iou > best_iou and iou >= iou_threshold:\n",
        "                    best_iou, best_det2, best_idx = iou, det2, idx\n",
        "            if best_det2 is not None:\n",
        "                matches.append((det1, best_det2, best_iou))\n",
        "                used_sys2.add(best_idx)\n",
        "        return matches\n",
        "\n",
        "    # 3.2.1 Position Distance\n",
        "\n",
        "    def get_position_distance_metrics(self) -> Dict:\n",
        "        all_distances = []\n",
        "        for frame_id in self.common_frames:\n",
        "            for det1, det2, _ in self.match_detections_hungarian(frame_id):\n",
        "                dist = np.linalg.norm(np.array(det1.center) - np.array(det2.center))\n",
        "                all_distances.append(dist)\n",
        "\n",
        "        if not all_distances:\n",
        "            return {\"mean_distance_px\": None, \"std_distance_px\": None,\n",
        "                    \"max_distance_px\": None, \"median_distance_px\": None, \"total_matches\": 0}\n",
        "\n",
        "        return {\n",
        "            \"mean_distance_px\": float(np.mean(all_distances)),\n",
        "            \"std_distance_px\": float(np.std(all_distances)),\n",
        "            \"max_distance_px\": float(np.max(all_distances)),\n",
        "            \"median_distance_px\": float(np.median(all_distances)),\n",
        "            \"total_matches\": len(all_distances)\n",
        "        }\n",
        "\n",
        "    # 3.2.2 IoU Metrics\n",
        "\n",
        "    def get_iou_metrics(self, iou_threshold: float = 0.3) -> Dict:\n",
        "        all_ious = []\n",
        "        total_sys1, total_sys2, total_matched = 0, 0, 0\n",
        "\n",
        "        for frame_id in self.common_frames:\n",
        "            sys1_dets = self.sys1_by_frame.get(frame_id, [])\n",
        "            sys2_dets = self.sys2_by_frame.get(frame_id, [])\n",
        "            total_sys1 += len(sys1_dets)\n",
        "            total_sys2 += len(sys2_dets)\n",
        "\n",
        "            matches = self.match_detections_hungarian(frame_id, iou_threshold)\n",
        "            total_matched += len(matches)\n",
        "            all_ious.extend([iou for _, _, iou in matches])\n",
        "\n",
        "        if not all_ious:\n",
        "            return {\"mean_iou\": None, \"std_iou\": None, \"min_iou\": None, \"max_iou\": None,\n",
        "                    \"match_rate_sys1\": 0.0, \"match_rate_sys2\": 0.0, \"total_matched\": 0,\n",
        "                    \"total_sys1_detections\": total_sys1, \"total_sys2_detections\": total_sys2}\n",
        "\n",
        "        return {\n",
        "            \"mean_iou\": float(np.mean(all_ious)), \"std_iou\": float(np.std(all_ious)),\n",
        "            \"min_iou\": float(np.min(all_ious)), \"max_iou\": float(np.max(all_ious)),\n",
        "            \"match_rate_sys1\": total_matched / total_sys1 if total_sys1 > 0 else 0.0,\n",
        "            \"match_rate_sys2\": total_matched / total_sys2 if total_sys2 > 0 else 0.0,\n",
        "            \"total_matched\": total_matched,\n",
        "            \"total_sys1_detections\": total_sys1, \"total_sys2_detections\": total_sys2\n",
        "        }\n",
        "\n",
        "    # 3.2.3 Velocity Correlation\n",
        "\n",
        "    def compute_velocities_for_frame_pairs(self) -> Tuple[List[float], List[float]]:\n",
        "        sys1_velocities, sys2_velocities = [], []\n",
        "\n",
        "        for i in range(len(self.common_frames) - 1):\n",
        "            frame1, frame2 = self.common_frames[i], self.common_frames[i + 1]\n",
        "            if frame2 - frame1 != 1:\n",
        "                continue\n",
        "\n",
        "            matches1 = self.match_detections_hungarian(frame1)\n",
        "            matches2 = self.match_detections_hungarian(frame2)\n",
        "\n",
        "            sys1_pos_f1 = {det1.track_id: det1.center for det1, _, _ in matches1}\n",
        "            sys1_pos_f2 = {det1.track_id: det1.center for det1, _, _ in matches2}\n",
        "            sys2_pos_f1 = {det2.track_id: det2.center for _, det2, _ in matches1}\n",
        "            sys2_pos_f2 = {det2.track_id: det2.center for _, det2, _ in matches2}\n",
        "\n",
        "            for tid in set(sys1_pos_f1.keys()) & set(sys1_pos_f2.keys()):\n",
        "                sys1_velocities.append(np.linalg.norm(\n",
        "                    np.array(sys1_pos_f2[tid]) - np.array(sys1_pos_f1[tid])))\n",
        "            for tid in set(sys2_pos_f1.keys()) & set(sys2_pos_f2.keys()):\n",
        "                sys2_velocities.append(np.linalg.norm(\n",
        "                    np.array(sys2_pos_f2[tid]) - np.array(sys2_pos_f1[tid])))\n",
        "\n",
        "        return sys1_velocities, sys2_velocities\n",
        "\n",
        "    def get_velocity_correlation(self) -> Dict:\n",
        "        sys1_vels, sys2_vels = self.compute_velocities_for_frame_pairs()\n",
        "\n",
        "        if not sys1_vels or not sys2_vels:\n",
        "            return {\n",
        "                f\"{self.sys1_name}_mean_velocity_px\": None,\n",
        "                f\"{self.sys2_name}_mean_velocity_px\": None,\n",
        "                f\"{self.sys1_name}_std_velocity_px\": None,\n",
        "                f\"{self.sys2_name}_std_velocity_px\": None,\n",
        "                \"velocity_difference_px\": None,\n",
        "                \"velocity_samples_sys1\": 0, \"velocity_samples_sys2\": 0\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            f\"{self.sys1_name}_mean_velocity_px\": float(np.mean(sys1_vels)),\n",
        "            f\"{self.sys2_name}_mean_velocity_px\": float(np.mean(sys2_vels)),\n",
        "            f\"{self.sys1_name}_std_velocity_px\": float(np.std(sys1_vels)),\n",
        "            f\"{self.sys2_name}_std_velocity_px\": float(np.std(sys2_vels)),\n",
        "            \"velocity_difference_px\": abs(float(np.mean(sys1_vels)) - float(np.mean(sys2_vels))),\n",
        "            \"velocity_samples_sys1\": len(sys1_vels), \"velocity_samples_sys2\": len(sys2_vels)\n",
        "        }\n",
        "\n",
        "    # 3.2.4 Disagreement Zones\n",
        "\n",
        "    def identify_disagreement_zones(self, detection_diff_threshold: int = 3,\n",
        "                                     iou_threshold: float = 0.3) -> Dict:\n",
        "        disagreement_frames, detection_count_diffs = [], []\n",
        "        unmatched_count = 0\n",
        "\n",
        "        for frame_id in self.common_frames:\n",
        "            sys1_dets = self.sys1_by_frame.get(frame_id, [])\n",
        "            sys2_dets = self.sys2_by_frame.get(frame_id, [])\n",
        "            count_diff = abs(len(sys1_dets) - len(sys2_dets))\n",
        "            detection_count_diffs.append(count_diff)\n",
        "\n",
        "            matches = self.match_detections_hungarian(frame_id, iou_threshold)\n",
        "            max_dets = max(len(sys1_dets), len(sys2_dets), 1)\n",
        "            match_rate = len(matches) / max_dets\n",
        "            unmatched_count += (len(sys1_dets) + len(sys2_dets) - 2 * len(matches))\n",
        "\n",
        "            if count_diff > detection_diff_threshold or match_rate < 0.5:\n",
        "                disagreement_frames.append({\n",
        "                    \"frame_id\": frame_id,\n",
        "                    f\"{self.sys1_name}_count\": len(sys1_dets),\n",
        "                    f\"{self.sys2_name}_count\": len(sys2_dets),\n",
        "                    \"matched\": len(matches), \"match_rate\": match_rate\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            \"total_disagreement_frames\": len(disagreement_frames),\n",
        "            \"disagreement_rate\": len(disagreement_frames) / len(self.common_frames) if self.common_frames else 0.0,\n",
        "            \"mean_detection_count_diff\": float(np.mean(detection_count_diffs)) if detection_count_diffs else 0.0,\n",
        "            \"max_detection_count_diff\": int(np.max(detection_count_diffs)) if detection_count_diffs else 0,\n",
        "            \"total_unmatched_detections\": unmatched_count,\n",
        "            \"common_frames_analyzed\": len(self.common_frames),\n",
        "            \"disagreement_frames_sample\": disagreement_frames[:10]\n",
        "        }\n",
        "\n",
        "# MULTI-CLIP DISCOVERY AND PROCESSING\n",
        "\n",
        "def discover_clips(results_dir: str, agents: List[str]) -> Dict[str, Dict[str, str]]:\n",
        "    \"Find all clips and their tracking outputs.\"\n",
        "    results_dir = Path(results_dir)\n",
        "    clips = {}\n",
        "\n",
        "    for item in results_dir.iterdir():\n",
        "        if not item.is_dir():\n",
        "            continue\n",
        "\n",
        "        clip_name = item.name\n",
        "        clip_agents = {}\n",
        "\n",
        "        full_dir = item / \"full\"\n",
        "        if full_dir.exists():\n",
        "            for agent in agents:\n",
        "                agent_dir = full_dir / agent\n",
        "                if agent_dir.exists():\n",
        "                    for f in agent_dir.iterdir():\n",
        "                        if f.suffix == '.json' and 'output' in f.name.lower():\n",
        "                            clip_agents[agent] = str(f)\n",
        "                            break\n",
        "\n",
        "        if not clip_agents:\n",
        "            for agent in agents:\n",
        "                for pattern in [item / f\"{agent}_output.json\", item / agent / f\"{agent}_output.json\"]:\n",
        "                    if pattern.exists():\n",
        "                        clip_agents[agent] = str(pattern)\n",
        "                        break\n",
        "\n",
        "        if clip_agents:\n",
        "            clips[clip_name] = clip_agents\n",
        "\n",
        "    return clips\n",
        "\n",
        "\n",
        "def compute_clip_metrics(clip_name: str, agent_files: Dict[str, str],\n",
        "                          fps: float = 30.0, pixels_per_meter: float = 10.0) -> Dict:\n",
        "    \"Compute all metrics for a single clip.\"\n",
        "    agent_detections = {}\n",
        "    for agent_name, filepath in agent_files.items():\n",
        "        dets = load_tracking_file(filepath, agent_name)\n",
        "        if dets:\n",
        "            agent_detections[agent_name] = dets\n",
        "\n",
        "    if not agent_detections:\n",
        "        return {\"clip_name\": clip_name, \"error\": \"No valid detections loaded\"}\n",
        "\n",
        "    quantitative = {}\n",
        "    for agent_name, dets in agent_detections.items():\n",
        "        tm = TrajectoryMetrics(dets, fps=fps, pixels_per_meter=pixels_per_meter)\n",
        "        quantitative[agent_name] = {\n",
        "            \"jerk\": tm.get_jerk_summary(),\n",
        "            \"speed\": tm.get_speed_plausibility(),\n",
        "            \"completeness\": tm.get_detection_completeness(),\n",
        "            \"detection_count\": len(dets)\n",
        "        }\n",
        "\n",
        "    agreement = {}\n",
        "    agent_names = list(agent_detections.keys())\n",
        "    for i in range(len(agent_names)):\n",
        "        for j in range(i + 1, len(agent_names)):\n",
        "            n1, n2 = agent_names[i], agent_names[j]\n",
        "            isa = InterSystemAgreement(agent_detections[n1], agent_detections[n2], n1, n2)\n",
        "            agreement[f\"{n1}_vs_{n2}\"] = {\n",
        "                \"position\": isa.get_position_distance_metrics(),\n",
        "                \"iou\": isa.get_iou_metrics(),\n",
        "                \"velocity\": isa.get_velocity_correlation(),\n",
        "                \"disagreement\": isa.identify_disagreement_zones()\n",
        "            }\n",
        "\n",
        "    return {\n",
        "        \"clip_name\": clip_name,\n",
        "        \"agents_loaded\": list(agent_detections.keys()),\n",
        "        \"quantitative\": quantitative,\n",
        "        \"agreement\": agreement\n",
        "    }\n",
        "\n",
        "# AGGREGATION FUNCTIONS\n",
        "\n",
        "def aggregate_metric(values: List[float], weights: Optional[List[float]] = None) -> Dict:\n",
        "    \"Aggregate metric values with optional weights.\"\n",
        "    values = [v for v in values if v is not None]\n",
        "    if not values:\n",
        "        return {\"mean\": None, \"std\": None, \"min\": None, \"max\": None, \"count\": 0}\n",
        "\n",
        "    weighted_mean = np.average(values, weights=weights) if weights else np.mean(values)\n",
        "    return {\n",
        "        \"mean\": float(weighted_mean), \"std\": float(np.std(values)),\n",
        "        \"min\": float(np.min(values)), \"max\": float(np.max(values)), \"count\": len(values)\n",
        "    }\n",
        "\n",
        "\n",
        "def aggregate_quantitative_metrics(all_clip_results: List[Dict]) -> Dict:\n",
        "    \"Aggregate Section 3.1 metrics across all clips.\"\n",
        "    agent_metrics = defaultdict(lambda: defaultdict(list))\n",
        "    agent_weights = defaultdict(list)\n",
        "\n",
        "    for clip_result in all_clip_results:\n",
        "        if \"error\" in clip_result:\n",
        "            continue\n",
        "\n",
        "        for agent_name, metrics in clip_result.get(\"quantitative\", {}).items():\n",
        "            frame_count = metrics.get(\"completeness\", {}).get(\"total_frames\", 1)\n",
        "            agent_weights[agent_name].append(frame_count)\n",
        "\n",
        "            jerk = metrics.get(\"jerk\", {})\n",
        "            if jerk.get(\"mean\") is not None:\n",
        "                agent_metrics[agent_name][\"jerk_mean\"].append(jerk[\"mean\"])\n",
        "                agent_metrics[agent_name][\"jerk_count\"].append(jerk.get(\"count\", 0))\n",
        "\n",
        "            speed = metrics.get(\"speed\", {})\n",
        "            if speed.get(\"mean_speed_ms\") is not None:\n",
        "                agent_metrics[agent_name][\"speed_mean\"].append(speed[\"mean_speed_ms\"])\n",
        "                agent_metrics[agent_name][\"speed_max\"].append(speed[\"max_speed_ms\"])\n",
        "                agent_metrics[agent_name][\"speed_violation_rate\"].append(speed[\"speed_violation_rate\"])\n",
        "            if speed.get(\"accel_violation_rate\") is not None:\n",
        "                agent_metrics[agent_name][\"accel_violation_rate\"].append(speed[\"accel_violation_rate\"])\n",
        "\n",
        "            comp = metrics.get(\"completeness\", {})\n",
        "            for key in [\"total_frames\", \"mean_players_per_frame\", \"coverage_rate\",\n",
        "                       \"total_tracks\", \"mean_track_length\", \"fragmentation_rate\"]:\n",
        "                agent_metrics[agent_name][key].append(comp.get(key, 0))\n",
        "            agent_metrics[agent_name][\"detection_count\"].append(metrics.get(\"detection_count\", 0))\n",
        "\n",
        "    aggregated = {}\n",
        "    for agent_name, metrics in agent_metrics.items():\n",
        "        weights = agent_weights[agent_name]\n",
        "        aggregated[agent_name] = {\n",
        "            \"clips_analyzed\": len(weights),\n",
        "            \"total_frames\": sum(metrics[\"total_frames\"]),\n",
        "            \"total_detections\": sum(metrics[\"detection_count\"]),\n",
        "            \"total_tracks\": sum(metrics[\"total_tracks\"]),\n",
        "            \"jerk\": {\"mean\": aggregate_metric(metrics[\"jerk_mean\"], weights),\n",
        "                     \"tracks_analyzed\": sum(metrics[\"jerk_count\"])},\n",
        "            \"speed\": {\"mean_speed_ms\": aggregate_metric(metrics[\"speed_mean\"], weights),\n",
        "                      \"max_speed_ms\": aggregate_metric(metrics[\"speed_max\"]),\n",
        "                      \"violation_rate\": aggregate_metric(metrics[\"speed_violation_rate\"], weights),\n",
        "                      \"accel_violation_rate\": aggregate_metric(metrics[\"accel_violation_rate\"], weights)},\n",
        "            \"completeness\": {\n",
        "                \"mean_players_per_frame\": aggregate_metric(metrics[\"mean_players_per_frame\"], weights),\n",
        "                \"coverage_rate_20_22\": aggregate_metric(metrics[\"coverage_rate\"], weights),\n",
        "                \"mean_track_length\": aggregate_metric(metrics[\"mean_track_length\"], weights),\n",
        "                \"fragmentation_rate\": aggregate_metric(metrics[\"fragmentation_rate\"], weights)\n",
        "            }\n",
        "        }\n",
        "    return aggregated\n",
        "\n",
        "\n",
        "def aggregate_agreement_metrics(all_clip_results: List[Dict]) -> Dict:\n",
        "    \"Aggregate Section 3.2 metrics across all clips.\"\n",
        "    pair_metrics = defaultdict(lambda: defaultdict(list))\n",
        "    pair_weights = defaultdict(list)\n",
        "\n",
        "    for clip_result in all_clip_results:\n",
        "        if \"error\" in clip_result:\n",
        "            continue\n",
        "\n",
        "        for pair_name, metrics in clip_result.get(\"agreement\", {}).items():\n",
        "            common_frames = metrics.get(\"disagreement\", {}).get(\"common_frames_analyzed\", 1)\n",
        "            pair_weights[pair_name].append(common_frames)\n",
        "\n",
        "            pos = metrics.get(\"position\", {})\n",
        "            if pos.get(\"mean_distance_px\") is not None:\n",
        "                pair_metrics[pair_name][\"position_mean\"].append(pos[\"mean_distance_px\"])\n",
        "                pair_metrics[pair_name][\"position_max\"].append(pos[\"max_distance_px\"])\n",
        "                pair_metrics[pair_name][\"total_matches\"].append(pos[\"total_matches\"])\n",
        "\n",
        "            iou = metrics.get(\"iou\", {})\n",
        "            if iou.get(\"mean_iou\") is not None:\n",
        "                pair_metrics[pair_name][\"iou_mean\"].append(iou[\"mean_iou\"])\n",
        "                pair_metrics[pair_name][\"iou_min\"].append(iou[\"min_iou\"])\n",
        "                pair_metrics[pair_name][\"match_rate_sys1\"].append(iou[\"match_rate_sys1\"])\n",
        "                pair_metrics[pair_name][\"match_rate_sys2\"].append(iou[\"match_rate_sys2\"])\n",
        "\n",
        "            disagree = metrics.get(\"disagreement\", {})\n",
        "            pair_metrics[pair_name][\"disagreement_rate\"].append(disagree.get(\"disagreement_rate\", 0))\n",
        "            pair_metrics[pair_name][\"mean_count_diff\"].append(disagree.get(\"mean_detection_count_diff\", 0))\n",
        "            pair_metrics[pair_name][\"common_frames\"].append(disagree.get(\"common_frames_analyzed\", 0))\n",
        "\n",
        "    aggregated = {}\n",
        "    for pair_name, metrics in pair_metrics.items():\n",
        "        weights = pair_weights[pair_name]\n",
        "        aggregated[pair_name] = {\n",
        "            \"clips_analyzed\": len(weights),\n",
        "            \"total_common_frames\": sum(metrics[\"common_frames\"]),\n",
        "            \"total_matches\": sum(metrics[\"total_matches\"]) if metrics[\"total_matches\"] else 0,\n",
        "            \"position_distance\": {\"mean_px\": aggregate_metric(metrics[\"position_mean\"], weights),\n",
        "                                  \"max_px\": aggregate_metric(metrics[\"position_max\"])},\n",
        "            \"iou\": {\"mean\": aggregate_metric(metrics[\"iou_mean\"], weights),\n",
        "                    \"min\": aggregate_metric(metrics[\"iou_min\"]),\n",
        "                    \"match_rate_sys1\": aggregate_metric(metrics[\"match_rate_sys1\"], weights),\n",
        "                    \"match_rate_sys2\": aggregate_metric(metrics[\"match_rate_sys2\"], weights)},\n",
        "            \"disagreement\": {\"rate\": aggregate_metric(metrics[\"disagreement_rate\"], weights),\n",
        "                             \"mean_count_diff\": aggregate_metric(metrics[\"mean_count_diff\"], weights)}\n",
        "        }\n",
        "    return aggregated\n",
        "\n",
        "# REPORT GENERATION\n",
        "\n",
        "def generate_comparison_table(aggregated_quant: Dict) -> str:\n",
        "    \"Generate comparison table for quantitative metrics.\"\n",
        "    agents = list(aggregated_quant.keys())\n",
        "    if not agents:\n",
        "        return \"No data to display.\"\n",
        "\n",
        "    lines = [\"\\n\" + \"=\" * 120, \"AGGREGATED QUANTITATIVE METRICS COMPARISON TABLE\", \"=\" * 120]\n",
        "\n",
        "    header = f\"{'Metric':<50} | \" + \" | \".join(f\"{a:>18}\" for a in agents)\n",
        "    lines.extend([header, \"-\" * len(header)])\n",
        "\n",
        "    metrics_to_show = [\n",
        "        (\"Total Clips\", lambda m: m[\"clips_analyzed\"], \"{:d}\"),\n",
        "        (\"Total Frames\", lambda m: m[\"total_frames\"], \"{:,d}\"),\n",
        "        (\"Total Detections\", lambda m: m[\"total_detections\"], \"{:,d}\"),\n",
        "        (\"Mean Jerk Score\", lambda m: m[\"jerk\"][\"mean\"][\"mean\"], \"{:.2f}\"),\n",
        "        (\"Mean Speed (m/s)\", lambda m: m[\"speed\"][\"mean_speed_ms\"][\"mean\"], \"{:.2f}\"),\n",
        "        (\"Speed Violation Rate (%)\", lambda m: m[\"speed\"][\"violation_rate\"][\"mean\"] * 100 if m[\"speed\"][\"violation_rate\"][\"mean\"] else None, \"{:.1f}\"),\n",
        "        (\"Mean Players/Frame\", lambda m: m[\"completeness\"][\"mean_players_per_frame\"][\"mean\"], \"{:.1f}\"),\n",
        "        (\"Coverage Rate 20-22 (%)\", lambda m: m[\"completeness\"][\"coverage_rate_20_22\"][\"mean\"] * 100, \"{:.1f}\"),\n",
        "        (\"Mean Track Length (frames)\", lambda m: m[\"completeness\"][\"mean_track_length\"][\"mean\"], \"{:.1f}\"),\n",
        "        (\"Fragmentation Rate (%)\", lambda m: m[\"completeness\"][\"fragmentation_rate\"][\"mean\"] * 100, \"{:.1f}\"),\n",
        "    ]\n",
        "\n",
        "    for label, getter, fmt in metrics_to_show:\n",
        "        values = []\n",
        "        for agent in agents:\n",
        "            try:\n",
        "                val = getter(aggregated_quant[agent])\n",
        "                values.append(f\"{'N/A':>18}\" if val is None else f\"{fmt.format(val):>18}\")\n",
        "            except (KeyError, TypeError):\n",
        "                values.append(f\"{'N/A':>18}\")\n",
        "        lines.append(f\"{label:<50} | \" + \" | \".join(values))\n",
        "\n",
        "    lines.append(\"=\" * 120)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def export_aggregated_csv(aggregated_quant: Dict, aggregated_agree: Dict, output_dir: str):\n",
        "    \"Export aggregated results to CSV files.\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Quantitative metrics CSV\n",
        "    quant_rows = []\n",
        "    for agent_name, m in aggregated_quant.items():\n",
        "        quant_rows.append({\n",
        "            \"agent\": agent_name,\n",
        "            \"clips_analyzed\": m[\"clips_analyzed\"],\n",
        "            \"total_frames\": m[\"total_frames\"],\n",
        "            \"total_detections\": m[\"total_detections\"],\n",
        "            \"total_tracks\": m[\"total_tracks\"],\n",
        "            \"jerk_mean\": m[\"jerk\"][\"mean\"][\"mean\"],\n",
        "            \"jerk_std\": m[\"jerk\"][\"mean\"][\"std\"],\n",
        "            \"speed_mean_ms\": m[\"speed\"][\"mean_speed_ms\"][\"mean\"],\n",
        "            \"speed_violation_rate\": m[\"speed\"][\"violation_rate\"][\"mean\"],\n",
        "            \"accel_violation_rate\": m[\"speed\"][\"accel_violation_rate\"][\"mean\"],\n",
        "            \"mean_players_per_frame\": m[\"completeness\"][\"mean_players_per_frame\"][\"mean\"],\n",
        "            \"coverage_rate\": m[\"completeness\"][\"coverage_rate_20_22\"][\"mean\"],\n",
        "            \"mean_track_length\": m[\"completeness\"][\"mean_track_length\"][\"mean\"],\n",
        "            \"fragmentation_rate\": m[\"completeness\"][\"fragmentation_rate\"][\"mean\"],\n",
        "        })\n",
        "\n",
        "    if quant_rows:\n",
        "        quant_path = output_dir / \"aggregated_quantitative_metrics.csv\"\n",
        "        with open(quant_path, 'w', newline='') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=quant_rows[0].keys())\n",
        "            writer.writeheader()\n",
        "            writer.writerows(quant_rows)\n",
        "        print(f\"Exported: {quant_path}\")\n",
        "\n",
        "    # Agreement metrics CSV\n",
        "    agree_rows = []\n",
        "    for pair_name, m in aggregated_agree.items():\n",
        "        agree_rows.append({\n",
        "            \"system_pair\": pair_name,\n",
        "            \"clips_analyzed\": m[\"clips_analyzed\"],\n",
        "            \"total_common_frames\": m[\"total_common_frames\"],\n",
        "            \"total_matches\": m[\"total_matches\"],\n",
        "            \"position_mean_px\": m[\"position_distance\"][\"mean_px\"][\"mean\"],\n",
        "            \"position_std_px\": m[\"position_distance\"][\"mean_px\"][\"std\"],\n",
        "            \"iou_mean\": m[\"iou\"][\"mean\"][\"mean\"],\n",
        "            \"iou_std\": m[\"iou\"][\"mean\"][\"std\"],\n",
        "            \"match_rate_sys1\": m[\"iou\"][\"match_rate_sys1\"][\"mean\"],\n",
        "            \"match_rate_sys2\": m[\"iou\"][\"match_rate_sys2\"][\"mean\"],\n",
        "            \"disagreement_rate\": m[\"disagreement\"][\"rate\"][\"mean\"],\n",
        "            \"mean_count_diff\": m[\"disagreement\"][\"mean_count_diff\"][\"mean\"],\n",
        "        })\n",
        "\n",
        "    if agree_rows:\n",
        "        agree_path = output_dir / \"aggregated_agreement_metrics.csv\"\n",
        "        with open(agree_path, 'w', newline='') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=agree_rows[0].keys())\n",
        "            writer.writeheader()\n",
        "            writer.writerows(agree_rows)\n",
        "        print(f\"Exported: {agree_path}\")\n",
        "\n",
        "# MAIN FUNCTION\n",
        "\n",
        "def run_evaluation(results_dir: str = None, output_dir: str = None,\n",
        "                   agents: List[str] = None, fps: float = 30.0,\n",
        "                   pixels_per_meter: float = 10.0) -> Dict:\n",
        "    \"\"\"\n",
        "    Run full evaluation across all clips.\n",
        "\n",
        "    Args:\n",
        "        results_dir: Directory containing tracking results\n",
        "        output_dir: Directory for output files\n",
        "        agents: List of agent names to compare\n",
        "        fps: Video FPS for speed calculations\n",
        "        pixels_per_meter: Pixel to meter conversion\n",
        "\n",
        "    Returns:\n",
        "        Dict with aggregated results\n",
        "    \"\"\"\n",
        "    results_dir = Path(results_dir or RESULTS_DIR)\n",
        "    output_dir = Path(output_dir or OUTPUT_DIR)\n",
        "    agents = agents or AGENTS\n",
        "\n",
        "    print(\"=\" * 90)\n",
        "    print(\"MULTI-CLIP TRACKING SYSTEM EVALUATION\")\n",
        "    print(\"=\" * 90)\n",
        "    print(f\"Results Directory: {results_dir}\")\n",
        "    print(f\"Output Directory: {output_dir}\")\n",
        "    print(f\"Agents: {agents}\\n\")\n",
        "\n",
        "    # Discover clips\n",
        "    clips = discover_clips(str(results_dir), agents)\n",
        "    print(f\"Discovered {len(clips)} clips:\")\n",
        "    for clip_name, agent_files in clips.items():\n",
        "        print(f\"  {clip_name}: {list(agent_files.keys())}\")\n",
        "    print()\n",
        "\n",
        "    if not clips:\n",
        "        print(\"ERROR: No clips found!\")\n",
        "        return {\"error\": \"No clips found\"}\n",
        "\n",
        "    # Process each clip\n",
        "    all_clip_results = []\n",
        "    for clip_name, agent_files in clips.items():\n",
        "        print(f\"Processing {clip_name}...\")\n",
        "        result = compute_clip_metrics(clip_name, agent_files, fps, pixels_per_meter)\n",
        "        all_clip_results.append(result)\n",
        "\n",
        "        if \"error\" not in result:\n",
        "            for agent in result.get(\"agents_loaded\", []):\n",
        "                det_count = result[\"quantitative\"].get(agent, {}).get(\"detection_count\", 0)\n",
        "                print(f\"  {agent}: {det_count} detections\")\n",
        "    print()\n",
        "\n",
        "    # Aggregate results\n",
        "    print(\"Aggregating results...\")\n",
        "    aggregated_quant = aggregate_quantitative_metrics(all_clip_results)\n",
        "    aggregated_agree = aggregate_agreement_metrics(all_clip_results)\n",
        "\n",
        "    # Print comparison table\n",
        "    table = generate_comparison_table(aggregated_quant)\n",
        "    print(table)\n",
        "\n",
        "    # Export CSV files\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    export_aggregated_csv(aggregated_quant, aggregated_agree, str(output_dir))\n",
        "\n",
        "    # Save full results as JSON\n",
        "    full_results = {\n",
        "        \"generated_at\": datetime.now().isoformat(),\n",
        "        \"parameters\": {\"fps\": fps, \"pixels_per_meter\": pixels_per_meter, \"agents\": agents},\n",
        "        \"clips_analyzed\": len([r for r in all_clip_results if \"error\" not in r]),\n",
        "        \"aggregated_quantitative\": aggregated_quant,\n",
        "        \"aggregated_agreement\": aggregated_agree,\n",
        "        \"per_clip_results\": all_clip_results\n",
        "    }\n",
        "\n",
        "    json_path = output_dir / \"evaluation_results.json\"\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(full_results, f, indent=2, default=str)\n",
        "    print(f\"\\nSaved full results to: {json_path}\")\n",
        "\n",
        "    return full_results\n",
        "\n",
        "# RUN EVALUATION\n",
        "\n",
        "print_status(\"Evaluation Module Loaded\",\"SUCCESS\")\n",
        "print(\"\\nTo run evaluation:\")\n",
        "print(\"  results = run_evaluation()\")\n",
        "print(\"\\nOr with custom paths:\")\n",
        "print(\"  results = run_evaluation(\")\n",
        "print(\"      results_dir='/content/comp-4009-tracking-results',\")\n",
        "print(\"      output_dir='/content/comparison_output'\")\n",
        "print(\"  )\")\n",
        "print(\"Running evaluation now...\\n\")\n",
        "\n",
        "# Run evaluation automatically\n",
        "evaluation_results = run_evaluation()"
      ],
      "metadata": {
        "id": "FA0KDEd640RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "OUTPUT_DIR = Path(\"/content/comparison_output/report\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "COLORS = {\n",
        "    'darkmyter': '#FF69B4',\n",
        "    'eagle': '#FFD700',\n",
        "    'yolo11_botsort': '#4169E1'\n",
        "}\n",
        "\n",
        "SYSTEM_LABELS = {\n",
        "    'darkmyter': 'Darkmyter\\n(YOLOv8+ByteTrack)',\n",
        "    'eagle': 'Eagle\\n(YOLOv8+BoT-SORT)',\n",
        "    'yolo11_botsort': 'YOLO11\\n(BoT-SORT)'\n",
        "}\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette([COLORS['darkmyter'], COLORS['eagle'], COLORS['yolo11_botsort']])\n",
        "\n",
        "print(\"Loading data...\")\n",
        "quant_df = pd.read_csv('/content/comparison_output/aggregated_quantitative_metrics.csv')\n",
        "agree_df = pd.read_csv('/content/comparison_output/aggregated_agreement_metrics.csv')\n",
        "\n",
        "try:\n",
        "    with open('/content/comparison_output/multi_clip_full_results.json', 'r') as f:\n",
        "        full_results = json.load(f)\n",
        "    has_full_results = True\n",
        "except:\n",
        "    has_full_results = False\n",
        "\n",
        "print(f\"Systems: {list(quant_df['agent'])}\")\n",
        "print(f\"Clips analyzed: {quant_df['clips_analyzed'].iloc[0]}\")\n",
        "\n",
        "def save_figure(fig, name):\n",
        "    path = OUTPUT_DIR / f\"{name}.png\"\n",
        "    fig.savefig(path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    print(f\"Saved: {path}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "def add_value_labels(ax, bars, fmt='.1f', fontsize=9):\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:{fmt}}',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3), textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=fontsize)\n",
        "\n",
        "print(\"\\nGenerating figures...\")\n",
        "\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "fig.suptitle('Player Tracking System Comparison - Executive Summary', fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "x = np.arange(len(quant_df))\n",
        "width = 0.6\n",
        "\n",
        "ax1 = fig.add_subplot(2, 3, 1)\n",
        "bars = ax1.bar(x, quant_df['mean_players_per_frame'], width, color=[COLORS[a] for a in quant_df['agent']])\n",
        "ax1.axhline(y=22, color='green', linestyle='--', alpha=0.7, label='Ideal (22)')\n",
        "ax1.axhline(y=20, color='orange', linestyle='--', alpha=0.7, label='Minimum (20)')\n",
        "ax1.set_ylabel('Players per Frame')\n",
        "ax1.set_title('Detection Completeness', fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels([SYSTEM_LABELS[a] for a in quant_df['agent']], fontsize=9)\n",
        "ax1.legend(loc='lower right', fontsize=8)\n",
        "ax1.set_ylim(0, 25)\n",
        "add_value_labels(ax1, bars)\n",
        "\n",
        "ax2 = fig.add_subplot(2, 3, 2)\n",
        "bars = ax2.bar(x, quant_df['jerk_mean'] / 1000, width, color=[COLORS[a] for a in quant_df['agent']])\n",
        "ax2.set_ylabel('Mean Jerk Score (x1000)')\n",
        "ax2.set_title('Trajectory Smoothness (Lower=Better)', fontweight='bold')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels([SYSTEM_LABELS[a] for a in quant_df['agent']], fontsize=9)\n",
        "add_value_labels(ax2, bars)\n",
        "\n",
        "ax3 = fig.add_subplot(2, 3, 3)\n",
        "bars = ax3.bar(x, quant_df['speed_violation_rate'] * 100, width, color=[COLORS[a] for a in quant_df['agent']])\n",
        "ax3.set_ylabel('Violation Rate (%)')\n",
        "ax3.set_title('Speed Violations (>40 km/h)', fontweight='bold')\n",
        "ax3.set_xticks(x)\n",
        "ax3.set_xticklabels([SYSTEM_LABELS[a] for a in quant_df['agent']], fontsize=9)\n",
        "add_value_labels(ax3, bars)\n",
        "\n",
        "ax4 = fig.add_subplot(2, 3, 4)\n",
        "bars = ax4.bar(x, quant_df['mean_track_length'], width, color=[COLORS[a] for a in quant_df['agent']])\n",
        "ax4.set_ylabel('Frames')\n",
        "ax4.set_title('Mean Track Length (Higher=Better)', fontweight='bold')\n",
        "ax4.set_xticks(x)\n",
        "ax4.set_xticklabels([SYSTEM_LABELS[a] for a in quant_df['agent']], fontsize=9)\n",
        "ax4.set_yscale('log')\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    ax4.annotate(f'{height:.0f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "ax5 = fig.add_subplot(2, 3, 5)\n",
        "bars = ax5.bar(x, quant_df['fragmentation_rate'] * 100, width, color=[COLORS[a] for a in quant_df['agent']])\n",
        "ax5.set_ylabel('Fragmentation Rate (%)')\n",
        "ax5.set_title('Track Fragmentation (Lower=Better)', fontweight='bold')\n",
        "ax5.set_xticks(x)\n",
        "ax5.set_xticklabels([SYSTEM_LABELS[a] for a in quant_df['agent']], fontsize=9)\n",
        "ax5.set_ylim(0, 105)\n",
        "add_value_labels(ax5, bars)\n",
        "\n",
        "ax6 = fig.add_subplot(2, 3, 6)\n",
        "bars = ax6.bar(x, quant_df['coverage_rate'] * 100, width, color=[COLORS[a] for a in quant_df['agent']])\n",
        "ax6.set_ylabel('Coverage Rate (%)')\n",
        "ax6.set_title('Frames with 20-22 Players', fontweight='bold')\n",
        "ax6.set_xticks(x)\n",
        "ax6.set_xticklabels([SYSTEM_LABELS[a] for a in quant_df['agent']], fontsize=9)\n",
        "ax6.set_ylim(0, 100)\n",
        "add_value_labels(ax6, bars)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "save_figure(fig, '01_executive_summary')\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "fig.suptitle('Inter-System Agreement Analysis', fontsize=14, fontweight='bold')\n",
        "\n",
        "pair_labels = ['Darkmyter\\nvs Eagle', 'Darkmyter\\nvs YOLO11', 'Eagle\\nvs YOLO11']\n",
        "pair_colors = ['#FF69B4', '#FFD700', '#4169E1']\n",
        "\n",
        "ax = axes[0]\n",
        "bars = ax.bar(pair_labels, agree_df['position_mean_px'], color=pair_colors, yerr=agree_df['position_std_px'], capsize=5)\n",
        "ax.set_ylabel('Mean Position Distance (pixels)')\n",
        "ax.set_title('Position Agreement (Lower=Better)', fontweight='bold')\n",
        "add_value_labels(ax, bars, fmt='.2f')\n",
        "\n",
        "ax = axes[1]\n",
        "bars = ax.bar(pair_labels, agree_df['iou_mean'], color=pair_colors, yerr=agree_df['iou_std'], capsize=5)\n",
        "ax.set_ylabel('Mean IoU')\n",
        "ax.set_title('Bounding Box Overlap (Higher=Better)', fontweight='bold')\n",
        "ax.set_ylim(0, 1)\n",
        "add_value_labels(ax, bars, fmt='.3f')\n",
        "\n",
        "ax = axes[2]\n",
        "bars = ax.bar(pair_labels, agree_df['disagreement_rate'] * 100, color=pair_colors)\n",
        "ax.set_ylabel('Disagreement Rate (%)')\n",
        "ax.set_title('Detection Disagreement (Lower=Better)', fontweight='bold')\n",
        "ax.set_ylim(0, 100)\n",
        "add_value_labels(ax, bars)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
        "save_figure(fig, '02_inter_system_agreement')\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(111, polar=True)\n",
        "\n",
        "categories = ['Detection\\nCompleteness', 'Trajectory\\nSmoothness', 'Speed\\nPlausibility',\n",
        "              'Track\\nContinuity', 'ID\\nStability', 'Coverage\\nRate']\n",
        "\n",
        "def normalize_metric(values, higher_is_better=True):\n",
        "    min_v, max_v = min(values), max(values)\n",
        "    if max_v == min_v:\n",
        "        return [0.5] * len(values)\n",
        "    normalized = [(v - min_v) / (max_v - min_v) for v in values]\n",
        "    if not higher_is_better:\n",
        "        normalized = [1 - n for n in normalized]\n",
        "    return normalized\n",
        "\n",
        "completeness = normalize_metric(list(quant_df['mean_players_per_frame']), True)\n",
        "smoothness = normalize_metric(list(quant_df['jerk_mean']), False)\n",
        "speed_plaus = normalize_metric(list(quant_df['speed_violation_rate']), False)\n",
        "continuity = normalize_metric(list(quant_df['mean_track_length']), True)\n",
        "stability = normalize_metric(list(quant_df['fragmentation_rate']), False)\n",
        "coverage = normalize_metric(list(quant_df['coverage_rate']), True)\n",
        "\n",
        "radar_data = {}\n",
        "for i, agent in enumerate(quant_df['agent']):\n",
        "    radar_data[agent] = [completeness[i], smoothness[i], speed_plaus[i], continuity[i], stability[i], coverage[i]]\n",
        "\n",
        "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
        "angles += angles[:1]\n",
        "\n",
        "for agent, values in radar_data.items():\n",
        "    values_plot = values + values[:1]\n",
        "    ax.plot(angles, values_plot, 'o-', linewidth=2, label=SYSTEM_LABELS[agent].replace('\\n', ' '), color=COLORS[agent])\n",
        "    ax.fill(angles, values_plot, alpha=0.15, color=COLORS[agent])\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(categories, size=11)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_title('Overall System Comparison (Outer=Better)', fontsize=14, fontweight='bold', pad=20)\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure(fig, '03_radar_comparison')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "fig.suptitle('Detection Statistics', fontsize=14, fontweight='bold')\n",
        "\n",
        "ax = axes[0]\n",
        "bars = ax.bar(x, quant_df['total_detections'] / 1e6, width, color=[COLORS[a] for a in quant_df['agent']])\n",
        "ax.set_ylabel('Total Detections (Millions)')\n",
        "ax.set_title('Total Detections Across All Clips', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([SYSTEM_LABELS[a] for a in quant_df['agent']], fontsize=9)\n",
        "add_value_labels(ax, bars, fmt='.2f')\n",
        "\n",
        "ax = axes[1]\n",
        "bars = ax.bar(x, quant_df['total_tracks'] / 1000, width, color=[COLORS[a] for a in quant_df['agent']])\n",
        "ax.set_ylabel('Total Unique Tracks (Thousands)')\n",
        "ax.set_title('Track Count', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([SYSTEM_LABELS[a] for a in quant_df['agent']], fontsize=9)\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{quant_df[\"total_tracks\"].iloc[i]:,}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
        "save_figure(fig, '04_detection_statistics')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "systems = list(quant_df['agent'])\n",
        "n_sys = len(systems)\n",
        "iou_matrix = np.ones((n_sys, n_sys))\n",
        "\n",
        "pair_iou = {\n",
        "    ('darkmyter', 'eagle'): agree_df[agree_df['system_pair'] == 'darkmyter_vs_eagle']['iou_mean'].values[0],\n",
        "    ('darkmyter', 'yolo11_botsort'): agree_df[agree_df['system_pair'] == 'darkmyter_vs_yolo11_botsort']['iou_mean'].values[0],\n",
        "    ('eagle', 'yolo11_botsort'): agree_df[agree_df['system_pair'] == 'eagle_vs_yolo11_botsort']['iou_mean'].values[0],\n",
        "}\n",
        "\n",
        "for i, sys1 in enumerate(systems):\n",
        "    for j, sys2 in enumerate(systems):\n",
        "        if i != j:\n",
        "            key = (sys1, sys2) if (sys1, sys2) in pair_iou else (sys2, sys1)\n",
        "            if key in pair_iou:\n",
        "                iou_matrix[i, j] = pair_iou[key]\n",
        "\n",
        "sns.heatmap(iou_matrix, annot=True, fmt='.3f', cmap='RdYlGn',\n",
        "            xticklabels=[s.replace('yolo11_botsort', 'YOLO11').title() for s in systems],\n",
        "            yticklabels=[s.replace('yolo11_botsort', 'YOLO11').title() for s in systems],\n",
        "            ax=ax, vmin=0.5, vmax=1.0, cbar_kws={'label': 'Mean IoU'})\n",
        "ax.set_title('Inter-System Bounding Box Agreement (IoU)', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure(fig, '05_agreement_heatmap')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "fig.suptitle('Speed Analysis', fontsize=14, fontweight='bold')\n",
        "\n",
        "ax = axes[0]\n",
        "bars = ax.bar(x, quant_df['speed_mean_ms'] * 3.6, width, color=[COLORS[a] for a in quant_df['agent']])\n",
        "ax.axhline(y=20, color='orange', linestyle='--', alpha=0.7, label='Typical jog')\n",
        "ax.axhline(y=35, color='red', linestyle='--', alpha=0.7, label='Elite sprint')\n",
        "ax.set_ylabel('Mean Speed (km/h)')\n",
        "ax.set_title('Average Player Speed', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([SYSTEM_LABELS[a] for a in quant_df['agent']], fontsize=9)\n",
        "ax.legend(loc='upper right', fontsize=8)\n",
        "add_value_labels(ax, bars)\n",
        "\n",
        "ax = axes[1]\n",
        "realistic = (1 - quant_df['speed_violation_rate']) * 100\n",
        "bars = ax.bar(x, realistic, width, color=[COLORS[a] for a in quant_df['agent']])\n",
        "ax.set_ylabel('Realistic Speed Frames (%)')\n",
        "ax.set_title('Frames with Plausible Speeds (<40 km/h)', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([SYSTEM_LABELS[a] for a in quant_df['agent']], fontsize=9)\n",
        "ax.set_ylim(90, 100)\n",
        "add_value_labels(ax, bars)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
        "save_figure(fig, '06_speed_analysis')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "for i, row in quant_df.iterrows():\n",
        "    agent = row['agent']\n",
        "    size = np.log10(row['mean_track_length'] + 1) * 200\n",
        "    ax.scatter(row['fragmentation_rate'] * 100, row['mean_players_per_frame'],\n",
        "               s=size, c=COLORS[agent], alpha=0.7, edgecolors='black', linewidth=2,\n",
        "               label=f\"{SYSTEM_LABELS[agent].replace(chr(10), ' ')} (Track: {row['mean_track_length']:.0f}f)\")\n",
        "\n",
        "ax.axhline(y=22, color='green', linestyle='--', alpha=0.5, label='Ideal 22 players')\n",
        "ax.axhline(y=20, color='orange', linestyle='--', alpha=0.5, label='Minimum 20 players')\n",
        "ax.set_xlabel('Fragmentation Rate (%)', fontsize=12)\n",
        "ax.set_ylabel('Mean Players per Frame', fontsize=12)\n",
        "ax.set_title('Detection vs. Tracking Trade-off (Bubble size = Track Length)', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='lower left', fontsize=9)\n",
        "ax.set_xlim(50, 105)\n",
        "ax.set_ylim(15, 25)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure(fig, '07_tradeoff_analysis')\n",
        "\n",
        "print(\"\\nGenerating text report...\")\n",
        "\n",
        "report_lines = []\n",
        "report_lines.append(\"=\" * 80)\n",
        "report_lines.append(\"PLAYER TRACKING SYSTEM COMPARISON REPORT\")\n",
        "report_lines.append(\"=\" * 80)\n",
        "report_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "report_lines.append(f\"Clips Analyzed: {quant_df['clips_analyzed'].iloc[0]}\")\n",
        "report_lines.append(f\"Systems: {', '.join(quant_df['agent'])}\")\n",
        "report_lines.append(\"\")\n",
        "\n",
        "best_detection = quant_df.loc[quant_df['mean_players_per_frame'].idxmax(), 'agent']\n",
        "best_smoothness = quant_df.loc[quant_df['jerk_mean'].idxmin(), 'agent']\n",
        "best_speed = quant_df.loc[quant_df['speed_violation_rate'].idxmin(), 'agent']\n",
        "best_continuity = quant_df.loc[quant_df['mean_track_length'].idxmax(), 'agent']\n",
        "best_stability = quant_df.loc[quant_df['fragmentation_rate'].idxmin(), 'agent']\n",
        "best_coverage = quant_df.loc[quant_df['coverage_rate'].idxmax(), 'agent']\n",
        "\n",
        "report_lines.append(\"CATEGORY WINNERS:\")\n",
        "report_lines.append(f\"  Detection Completeness: {best_detection.upper()}\")\n",
        "report_lines.append(f\"  Trajectory Smoothness:  {best_smoothness.upper()}\")\n",
        "report_lines.append(f\"  Speed Plausibility:     {best_speed.upper()}\")\n",
        "report_lines.append(f\"  Track Continuity:       {best_continuity.upper()}\")\n",
        "report_lines.append(f\"  ID Stability:           {best_stability.upper()}\")\n",
        "report_lines.append(f\"  Coverage Rate:          {best_coverage.upper()}\")\n",
        "report_lines.append(\"\")\n",
        "\n",
        "report_lines.append(\"METRIC DEFINITIONS:\")\n",
        "report_lines.append(\"  Jerk Score: Third derivative of position. Lower = smoother motion.\")\n",
        "report_lines.append(\"  Speed Violation: % of frames exceeding 40 km/h. Lower = more realistic.\")\n",
        "report_lines.append(\"  Detection Completeness: Avg players/frame. Ideal = 20-22.\")\n",
        "report_lines.append(\"  Coverage Rate: % frames with 20-22 detections.\")\n",
        "report_lines.append(\"  Track Length: Avg duration of continuous tracks. Higher = better continuity.\")\n",
        "report_lines.append(\"  Fragmentation: Ratio of broken tracks. Lower = more stable IDs.\")\n",
        "report_lines.append(\"\")\n",
        "\n",
        "for _, row in quant_df.iterrows():\n",
        "    agent = row['agent']\n",
        "    report_lines.append(f\"--- {agent.upper()} ---\")\n",
        "    report_lines.append(f\"  Frames: {row['total_frames']:,} | Detections: {row['total_detections']:,} | Tracks: {row['total_tracks']:,}\")\n",
        "    report_lines.append(f\"  Jerk: {row['jerk_mean']:.0f} | Speed: {row['speed_mean_ms']*3.6:.1f} km/h | Violations: {row['speed_violation_rate']*100:.1f}%\")\n",
        "    report_lines.append(f\"  Players/Frame: {row['mean_players_per_frame']:.1f} | Coverage: {row['coverage_rate']*100:.1f}%\")\n",
        "    report_lines.append(f\"  Track Length: {row['mean_track_length']:.0f} frames | Fragmentation: {row['fragmentation_rate']*100:.1f}%\")\n",
        "    report_lines.append(\"\")\n",
        "\n",
        "report_lines.append(\"INTER-SYSTEM AGREEMENT:\")\n",
        "for _, row in agree_df.iterrows():\n",
        "    pair = row['system_pair'].replace('_vs_', ' vs ')\n",
        "    report_lines.append(f\"  {pair}: IoU={row['iou_mean']:.3f}, Distance={row['position_mean_px']:.2f}px, Disagreement={row['disagreement_rate']*100:.1f}%\")\n",
        "\n",
        "def calc_score(row):\n",
        "    idx = quant_df[quant_df['agent'] == row['agent']].index[0]\n",
        "    return 0.20*completeness[idx] + 0.15*smoothness[idx] + 0.15*speed_plaus[idx] + 0.20*continuity[idx] + 0.20*stability[idx] + 0.10*coverage[idx]\n",
        "\n",
        "scores = {row['agent']: calc_score(row) for _, row in quant_df.iterrows()}\n",
        "ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "report_lines.append(\"\")\n",
        "report_lines.append(\"OVERALL RANKING:\")\n",
        "for i, (agent, score) in enumerate(ranked, 1):\n",
        "    report_lines.append(f\"  {i}. {agent.upper()} (Score: {score:.3f})\")\n",
        "\n",
        "report_lines.append(\"\")\n",
        "report_lines.append(\"RECOMMENDATIONS:\")\n",
        "report_lines.append(f\"  Tactical Analysis: {best_detection.upper()} (best detection)\")\n",
        "report_lines.append(f\"  Player Tracking:   {best_continuity.upper()} (best continuity)\")\n",
        "report_lines.append(f\"  General Use:       {ranked[0][0].upper()} (best overall)\")\n",
        "\n",
        "report_path = OUTPUT_DIR / \"comparison_report.txt\"\n",
        "with open(report_path, 'w') as f:\n",
        "    f.write('\\n'.join(report_lines))\n",
        "print(f\"Saved: {report_path}\")\n",
        "\n",
        "summary_data = {\n",
        "    'System': [SYSTEM_LABELS[a].replace('\\n', ' ') for a in quant_df['agent']],\n",
        "    'Players/Frame': quant_df['mean_players_per_frame'].round(1),\n",
        "    'Jerk': quant_df['jerk_mean'].round(0),\n",
        "    'Speed (km/h)': (quant_df['speed_mean_ms'] * 3.6).round(1),\n",
        "    'Violation %': (quant_df['speed_violation_rate'] * 100).round(1),\n",
        "    'Track Length': quant_df['mean_track_length'].round(0),\n",
        "    'Fragment %': (quant_df['fragmentation_rate'] * 100).round(1),\n",
        "    'Coverage %': (quant_df['coverage_rate'] * 100).round(1),\n",
        "    'Score': [scores[a] for a in quant_df['agent']]\n",
        "}\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_path = OUTPUT_DIR / \"summary_table.csv\"\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "print(f\"Saved: {summary_path}\")\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(\"/content/tracking_report\", 'zip', OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\nFiles in {OUTPUT_DIR}:\")\n",
        "for f in sorted(OUTPUT_DIR.iterdir()):\n",
        "    print(f\"  {f.name}\")\n",
        "\n",
        "print(\"\\nDownloading...\")\n",
        "files.download(\"/content/tracking_report.zip\")\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "r1EwhLtcfIqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL C: VISUALIZATION (FIXED)\n",
        "\"\"\"\n",
        "Tracking Visualization Module\n",
        "\n",
        "Creates overlay and side-by-side comparison videos showing tracking results\n",
        "from multiple systems on the same video frames.\n",
        "\n",
        "Visualization Options:\n",
        "    1. Overlay: All systems drawn on the same video (different colors)\n",
        "    2. Side-by-side: Three panels showing each system separately\n",
        "\n",
        "NOTE: Videos are re-encoded with ffmpeg for browser/Colab compatibility.\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from IPython.display import display, HTML, Video\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# CONFIGURATION\n",
        "\n",
        "# Update these paths to match your setup\n",
        "TRACKING_RESULTS_DIR = Path(\"/content/comp-4009-tracking-results\")\n",
        "VIDEOS_DIR = Path(\"/content/videos\")\n",
        "CLIPS_DIR = Path(\"/content/clips\")\n",
        "\n",
        "# Output directory - use /content/output/visualization/ which persists in Colab\n",
        "VIZ_OUTPUT_DIR = Path(\"/content/output/visualization\")\n",
        "VIZ_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Visualization output directory: {VIZ_OUTPUT_DIR}\")\n",
        "\n",
        "AGENTS = [\"eagle\", \"darkmyter\", \"yolo11_botsort\"]\n",
        "\n",
        "# Colors for each system (BGR for OpenCV)\n",
        "SYSTEM_COLORS = {\n",
        "    \"eagle\": (0, 165, 255),        # Orange\n",
        "    \"yolo11_botsort\": (0, 255, 0), # Green\n",
        "    \"darkmyter\": (255, 0, 0),      # Blue\n",
        "}\n",
        "\n",
        "SYSTEM_LABELS = {\n",
        "    \"eagle\": \"Eagle\",\n",
        "    \"yolo11_botsort\": \"YOLO11+BoT-SORT\",\n",
        "    \"darkmyter\": \"Darkmyter\",\n",
        "}\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "\n",
        "def reencode_video(input_path: Path, output_path: Path):\n",
        "    # Re-encode video with H264 for browser compatibility\n",
        "    import shutil\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            \"ffmpeg\", \"-y\", \"-i\", str(input_path),\n",
        "            \"-c:v\", \"libx264\", \"-preset\", \"fast\", \"-crf\", \"23\",\n",
        "            \"-pix_fmt\", \"yuv420p\",\n",
        "            str(output_path)\n",
        "        ], check=True, capture_output=True, text=True)\n",
        "\n",
        "        # Verify output was created\n",
        "        if output_path.exists() and output_path.stat().st_size > 0:\n",
        "            # Remove temp file only after confirming output exists\n",
        "            if input_path.exists() and input_path != output_path:\n",
        "                input_path.unlink()\n",
        "            print(f\"[OK] Video saved: {output_path}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"[WARNING] ffmpeg ran but output file is missing or empty\")\n",
        "            if input_path.exists():\n",
        "                shutil.move(str(input_path), str(output_path))\n",
        "            return False\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"[WARNING] ffmpeg failed - saving original file\")\n",
        "        if input_path.exists():\n",
        "            shutil.move(str(input_path), str(output_path))\n",
        "            print(f\"[OK] Saved original file: {output_path}\")\n",
        "        return False\n",
        "    except FileNotFoundError:\n",
        "        print(\"[WARNING] ffmpeg not found - saving original file\")\n",
        "        if input_path.exists():\n",
        "            shutil.move(str(input_path), str(output_path))\n",
        "            print(f\"[OK] Saved original file: {output_path}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def load_tracking_for_viz(video_name: str, system: str, class_filter: str = \"player\") -> list:\n",
        "    \"\"\"\n",
        "    Load tracking data for visualization.\n",
        "    Uses parsers from Cell A (parse_eagle, parse_darkmyter, parse_yolo_botsort).\n",
        "    \"\"\"\n",
        "    json_path = TRACKING_RESULTS_DIR / video_name / \"full\" / system / f\"{system}_output.json\"\n",
        "\n",
        "    if not json_path.exists():\n",
        "        print(f\"  [WARNING] Not found: {json_path}\")\n",
        "        return []\n",
        "\n",
        "    with open(json_path) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Use appropriate parser from Cell A\n",
        "    if system == \"eagle\":\n",
        "        detections = parse_eagle(data)\n",
        "    elif system == \"darkmyter\":\n",
        "        detections = parse_darkmyter(data)\n",
        "    elif system == \"yolo11_botsort\":\n",
        "        detections = parse_yolo_botsort(data)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    # Filter by class and convert to dict format for visualization\n",
        "    result = []\n",
        "    for det in detections:\n",
        "        if class_filter and det.class_name != class_filter:\n",
        "            continue\n",
        "        result.append({\n",
        "            \"frame_id\": det.frame_id,\n",
        "            \"track_id\": det.track_id,\n",
        "            \"bbox\": det.bbox,\n",
        "            \"score\": det.score,\n",
        "            \"class_name\": det.class_name\n",
        "        })\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def organize_by_frame_viz(detections: list) -> dict:\n",
        "    # Group detections by frame_id for visualization\n",
        "    by_frame = defaultdict(list)\n",
        "    for det in detections:\n",
        "        by_frame[det[\"frame_id\"]].append(det)\n",
        "    return dict(by_frame)\n",
        "\n",
        "\n",
        "def discover_videos_with_results() -> list:\n",
        "    # Find all videos that have tracking results\n",
        "    videos = []\n",
        "    for item in TRACKING_RESULTS_DIR.iterdir():\n",
        "        if item.is_dir():\n",
        "            full_dir = item / \"full\"\n",
        "            if full_dir.exists():\n",
        "                has_outputs = any((full_dir / agent).exists() for agent in AGENTS)\n",
        "                if has_outputs:\n",
        "                    videos.append(item.name)\n",
        "    return sorted(videos)\n",
        "\n",
        "\n",
        "def find_source_video(video_name: str) -> Path:\n",
        "    # Find the source video file for a given video name\n",
        "    search_locations = [VIDEOS_DIR, CLIPS_DIR, Path(\"/content\")]\n",
        "    extensions = ['.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV', '.MKV']\n",
        "\n",
        "    for loc in search_locations:\n",
        "        if not loc.exists():\n",
        "            continue\n",
        "\n",
        "        # Try exact match\n",
        "        for ext in extensions:\n",
        "            video_path = loc / f\"{video_name}{ext}\"\n",
        "            if video_path.exists():\n",
        "                return video_path\n",
        "\n",
        "        # Try partial match (video names can be truncated)\n",
        "        for f in loc.glob(\"*\"):\n",
        "            if f.is_file() and f.suffix.lower() in [e.lower() for e in extensions]:\n",
        "                # Check if names are related\n",
        "                if video_name[:25] in f.stem or f.stem[:25] in video_name:\n",
        "                    return f\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# DRAWING FUNCTIONS\n",
        "\n",
        "def draw_detection(frame, det: dict, color: tuple, prefix: str):\n",
        "    # Draw a single detection bounding box on the frame\n",
        "    x1, y1, x2, y2 = [int(v) for v in det[\"bbox\"]]\n",
        "    track_id = det[\"track_id\"]\n",
        "\n",
        "    # Draw bounding box\n",
        "    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "    # Draw label background and text\n",
        "    label = f\"{prefix}{track_id}\"\n",
        "    (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
        "    cv2.rectangle(frame, (x1, y1 - h - 6), (x1 + w + 4, y1), color, -1)\n",
        "    cv2.putText(frame, label, (x1 + 2, y1 - 4),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
        "\n",
        "\n",
        "def draw_legend(frame, systems: list, start_y: int = 30):\n",
        "    # Draw color legend showing which system uses which color\n",
        "    for i, system in enumerate(systems):\n",
        "        color = SYSTEM_COLORS.get(system, (128, 128, 128))\n",
        "        label = SYSTEM_LABELS.get(system, system)\n",
        "        y = start_y + i * 25\n",
        "\n",
        "        # Color box\n",
        "        cv2.rectangle(frame, (10, y - 15), (30, y), color, -1)\n",
        "        # Label text\n",
        "        cv2.putText(frame, label, (35, y - 2),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "\n",
        "\n",
        "def draw_frame_counter(frame, frame_idx: int, width: int):\n",
        "    # Draw frame counter in corner\n",
        "    cv2.putText(frame, f\"Frame: {frame_idx}\", (width - 150, 30),\n",
        "               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "\n",
        "# VIDEO CREATION FUNCTIONS\n",
        "\n",
        "def create_overlay_video(video_name: str, systems: list = None,\n",
        "                         max_frames: int = 300, show_legend: bool = True,\n",
        "                         output_filename: str = None) -> Path:\n",
        "    \"\"\"\n",
        "    Create a video with all tracking systems overlaid on the same frames.\n",
        "\n",
        "    Args:\n",
        "        video_name: Name of the video (directory name in results)\n",
        "        systems: List of systems to include (default: all)\n",
        "        max_frames: Maximum number of frames to process\n",
        "        show_legend: Whether to show color legend\n",
        "        output_filename: Custom output filename\n",
        "\n",
        "    Returns:\n",
        "        Path to output video, or None if failed\n",
        "    \"\"\"\n",
        "    systems = systems or AGENTS\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Creating OVERLAY video: {video_name}\")\n",
        "    print(f\"Systems: {', '.join(systems)}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Find source video\n",
        "    video_path = find_source_video(video_name)\n",
        "    if video_path is None:\n",
        "        print(f\"[ERROR] Could not find source video for: {video_name}\")\n",
        "        print(f\"  Searched in: {VIDEOS_DIR}, {CLIPS_DIR}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Source video: {video_path}\")\n",
        "\n",
        "    # Load tracking data for each system\n",
        "    all_tracks = {}\n",
        "    for system in systems:\n",
        "        print(f\"Loading {system}...\", end=\" \")\n",
        "        detections = load_tracking_for_viz(video_name, system)\n",
        "        if detections:\n",
        "            all_tracks[system] = organize_by_frame_viz(detections)\n",
        "            print(f\"{len(detections)} detections\")\n",
        "        else:\n",
        "            print(\"No data\")\n",
        "\n",
        "    if not all_tracks:\n",
        "        print(\"[ERROR] No tracking data loaded for any system\")\n",
        "        return None\n",
        "\n",
        "    # Open source video\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        print(f\"[ERROR] Could not open video: {video_path}\")\n",
        "        return None\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"Video: {width}x{height} @ {fps:.1f} fps, {total_frames} frames\")\n",
        "\n",
        "    # Setup output paths - save to visualization/<video_name>/\n",
        "    safe_name = \"\".join(c if c.isalnum() or c in \"._- \" else \"_\" for c in video_name)\n",
        "    video_output_dir = VIZ_OUTPUT_DIR / safe_name\n",
        "    video_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if output_filename is None:\n",
        "        systems_str = \"_\".join(systems)\n",
        "        output_filename = f\"{safe_name}_{systems_str}.mp4\"\n",
        "\n",
        "    temp_output_path = video_output_dir / f\"temp_{output_filename}\"\n",
        "    final_output_path = video_output_dir / output_filename\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(str(temp_output_path), fourcc, fps, (width, height))\n",
        "\n",
        "    frames_to_process = min(max_frames, total_frames)\n",
        "    print(f\"\\nProcessing {frames_to_process} frames...\")\n",
        "\n",
        "    frame_idx = 0\n",
        "    while frame_idx < frames_to_process:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Draw detections from each system\n",
        "        for system in systems:\n",
        "            if system not in all_tracks:\n",
        "                continue\n",
        "\n",
        "            color = SYSTEM_COLORS.get(system, (128, 128, 128))\n",
        "            prefix = SYSTEM_LABELS.get(system, system)[0]  # First letter as prefix\n",
        "\n",
        "            for det in all_tracks[system].get(frame_idx, []):\n",
        "                draw_detection(frame, det, color, prefix)\n",
        "\n",
        "        # Draw legend\n",
        "        if show_legend:\n",
        "            active_systems = [s for s in systems if s in all_tracks]\n",
        "            draw_legend(frame, active_systems)\n",
        "\n",
        "        # Draw frame counter\n",
        "        draw_frame_counter(frame, frame_idx, width)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_idx += 1\n",
        "\n",
        "        if frame_idx % 100 == 0:\n",
        "            print(f\"  Processed {frame_idx}/{frames_to_process}\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Re-encode for browser compatibility\n",
        "    print(\"Re-encoding for browser compatibility...\")\n",
        "    reencode_video(temp_output_path, final_output_path)\n",
        "\n",
        "    # Verify file exists\n",
        "    if final_output_path.exists():\n",
        "        file_size_mb = final_output_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"SAVED TO: {final_output_path}\")\n",
        "        print(f\"File size: {file_size_mb:.1f} MB\")\n",
        "        print(f\"{'='*60}\")\n",
        "    else:\n",
        "        print(f\"\\n[ERROR] File was not saved! Check permissions for: {final_output_path.parent}\")\n",
        "\n",
        "    return final_output_path\n",
        "\n",
        "\n",
        "def create_side_by_side_video(video_name: str, max_frames: int = 300,\n",
        "                               output_filename: str = None) -> Path:\n",
        "    \"\"\"\n",
        "    Create a 3-panel side-by-side video showing each system separately.\n",
        "\n",
        "    Args:\n",
        "        video_name: Name of the video (directory name in results)\n",
        "        max_frames: Maximum number of frames to process\n",
        "        output_filename: Custom output filename\n",
        "\n",
        "    Returns:\n",
        "        Path to output video, or None if failed\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Creating SIDE-BY-SIDE video: {video_name}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Find source video\n",
        "    video_path = find_source_video(video_name)\n",
        "    if video_path is None:\n",
        "        print(f\"[ERROR] Could not find source video\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Source video: {video_path}\")\n",
        "\n",
        "    # Load all tracking data\n",
        "    all_tracks = {}\n",
        "    for system in AGENTS:\n",
        "        detections = load_tracking_for_viz(video_name, system)\n",
        "        if detections:\n",
        "            all_tracks[system] = organize_by_frame_viz(detections)\n",
        "            print(f\"Loaded {system}: {len(detections)} detections\")\n",
        "\n",
        "    # Open source video\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate panel dimensions (3 panels side by side)\n",
        "    panel_width = width // 2\n",
        "    panel_height = height // 2\n",
        "    output_width = panel_width * 3\n",
        "    output_height = panel_height\n",
        "\n",
        "    # Setup output paths - save to visualization/<video_name>/\n",
        "    safe_name = \"\".join(c if c.isalnum() or c in \"._- \" else \"_\" for c in video_name)\n",
        "    video_output_dir = VIZ_OUTPUT_DIR / safe_name\n",
        "    video_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if output_filename is None:\n",
        "        output_filename = f\"{safe_name}_sidebyside.mp4\"\n",
        "\n",
        "    temp_output_path = video_output_dir / f\"temp_{output_filename}\"\n",
        "    final_output_path = video_output_dir / output_filename\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(str(temp_output_path), fourcc, fps, (output_width, output_height))\n",
        "\n",
        "    frames_to_process = min(max_frames, total_frames)\n",
        "    print(f\"\\nProcessing {frames_to_process} frames...\")\n",
        "    print(f\"Output size: {output_width}x{output_height}\")\n",
        "\n",
        "    frame_idx = 0\n",
        "    while frame_idx < frames_to_process:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        panels = []\n",
        "        for system in AGENTS:\n",
        "            # Create scaled panel\n",
        "            panel = cv2.resize(frame.copy(), (panel_width, panel_height))\n",
        "\n",
        "            color = SYSTEM_COLORS.get(system, (128, 128, 128))\n",
        "            label = SYSTEM_LABELS.get(system, system)\n",
        "\n",
        "            # Draw system label at top\n",
        "            cv2.putText(panel, label, (10, 25),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "\n",
        "            # Draw detections (with scaled coordinates)\n",
        "            if system in all_tracks:\n",
        "                scale_x = panel_width / width\n",
        "                scale_y = panel_height / height\n",
        "\n",
        "                for det in all_tracks[system].get(frame_idx, []):\n",
        "                    bbox = det[\"bbox\"]\n",
        "                    scaled_bbox = [\n",
        "                        bbox[0] * scale_x, bbox[1] * scale_y,\n",
        "                        bbox[2] * scale_x, bbox[3] * scale_y\n",
        "                    ]\n",
        "                    x1, y1, x2, y2 = [int(v) for v in scaled_bbox]\n",
        "                    cv2.rectangle(panel, (x1, y1), (x2, y2), color, 1)\n",
        "\n",
        "                    # Small track ID label\n",
        "                    tid = det[\"track_id\"]\n",
        "                    cv2.putText(panel, str(tid), (x1, y1 - 2),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)\n",
        "\n",
        "            panels.append(panel)\n",
        "\n",
        "        # Combine panels horizontally\n",
        "        combined = np.hstack(panels)\n",
        "        out.write(combined)\n",
        "        frame_idx += 1\n",
        "\n",
        "        if frame_idx % 100 == 0:\n",
        "            print(f\"  Processed {frame_idx}/{frames_to_process}\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Re-encode for browser compatibility\n",
        "    print(\"Re-encoding for browser compatibility...\")\n",
        "    reencode_video(temp_output_path, final_output_path)\n",
        "\n",
        "    # Verify file exists\n",
        "    if final_output_path.exists():\n",
        "        file_size_mb = final_output_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"SAVED TO: {final_output_path}\")\n",
        "        print(f\"File size: {file_size_mb:.1f} MB\")\n",
        "        print(f\"{'='*60}\")\n",
        "    else:\n",
        "        print(f\"\\n[ERROR] File was not saved! Check permissions for: {final_output_path.parent}\")\n",
        "\n",
        "    return final_output_path\n",
        "\n",
        "\n",
        "# FRAME EXTRACTION (for report screenshots)\n",
        "\n",
        "def extract_comparison_frame(video_name: str, frame_number: int,\n",
        "                              systems: list = None, output_filename: str = None) -> Path:\n",
        "    \"\"\"\n",
        "    Extract a single frame with all tracking overlays for use in reports.\n",
        "\n",
        "    Args:\n",
        "        video_name: Name of the video\n",
        "        frame_number: Frame number to extract\n",
        "        systems: Systems to overlay (default: all)\n",
        "        output_filename: Output image filename\n",
        "\n",
        "    Returns:\n",
        "        Path to saved image\n",
        "    \"\"\"\n",
        "    systems = systems or AGENTS\n",
        "\n",
        "    video_path = find_source_video(video_name)\n",
        "    if video_path is None:\n",
        "        print(f\"[ERROR] Could not find source video\")\n",
        "        return None\n",
        "\n",
        "    # Load tracking data\n",
        "    all_tracks = {}\n",
        "    for system in systems:\n",
        "        detections = load_tracking_for_viz(video_name, system)\n",
        "        if detections:\n",
        "            all_tracks[system] = organize_by_frame_viz(detections)\n",
        "\n",
        "    # Open video and seek to frame\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
        "    ret, frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    if not ret:\n",
        "        print(f\"[ERROR] Could not read frame {frame_number}\")\n",
        "        return None\n",
        "\n",
        "    # Draw detections\n",
        "    for system in systems:\n",
        "        if system not in all_tracks:\n",
        "            continue\n",
        "\n",
        "        color = SYSTEM_COLORS.get(system, (128, 128, 128))\n",
        "        prefix = SYSTEM_LABELS.get(system, system)[0]\n",
        "\n",
        "        for det in all_tracks[system].get(frame_number, []):\n",
        "            draw_detection(frame, det, color, prefix)\n",
        "\n",
        "    # Draw legend\n",
        "    draw_legend(frame, [s for s in systems if s in all_tracks])\n",
        "    draw_frame_counter(frame, frame_number, width)\n",
        "\n",
        "    # Save frame to visualization/<video_name>/\n",
        "    safe_name = \"\".join(c if c.isalnum() or c in \"._- \" else \"_\" for c in video_name)\n",
        "    video_output_dir = VIZ_OUTPUT_DIR / safe_name\n",
        "    video_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if output_filename is None:\n",
        "        output_filename = f\"{safe_name}_frame_{frame_number}.png\"\n",
        "\n",
        "    output_path = video_output_dir / output_filename\n",
        "    cv2.imwrite(str(output_path), frame)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"SAVED TO: {output_path}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "# INTERACTIVE UI\n",
        "\n",
        "def run_visualization_ui():\n",
        "    # Interactive widget-based UI for creating comparison videos\n",
        "\n",
        "    videos = discover_videos_with_results()\n",
        "\n",
        "    if not videos:\n",
        "        print(\"[ERROR] No videos with tracking results found!\")\n",
        "        print(f\"  Searched in: {TRACKING_RESULTS_DIR}\")\n",
        "        return\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TRACKING VISUALIZATION TOOL\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nFound {len(videos)} videos with tracking results.\\n\")\n",
        "\n",
        "    # Create widgets\n",
        "    video_dropdown = widgets.Dropdown(\n",
        "        options=videos,\n",
        "        value=videos[0],\n",
        "        description='Video:',\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=widgets.Layout(width='90%')\n",
        "    )\n",
        "\n",
        "    eagle_cb = widgets.Checkbox(value=True, description='Eagle (Orange)')\n",
        "    darkmyter_cb = widgets.Checkbox(value=True, description='Darkmyter (Blue)')\n",
        "    yolo_cb = widgets.Checkbox(value=True, description='YOLO11+BoT-SORT (Green)')\n",
        "\n",
        "    frame_slider = widgets.IntSlider(\n",
        "        value=300, min=60, max=1800, step=60,\n",
        "        description='Max Frames:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    output_type = widgets.RadioButtons(\n",
        "        options=['Overlay (all on one video)', 'Side-by-side (3 panels)'],\n",
        "        value='Overlay (all on one video)',\n",
        "        description='Style:'\n",
        "    )\n",
        "\n",
        "    generate_btn = widgets.Button(\n",
        "        description='Generate Comparison Video',\n",
        "        button_style='success',\n",
        "        layout=widgets.Layout(width='250px')\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def on_generate(b):\n",
        "        with output_area:\n",
        "            output_area.clear_output()\n",
        "\n",
        "            video_name = video_dropdown.value\n",
        "\n",
        "            # Get selected systems\n",
        "            systems = []\n",
        "            if eagle_cb.value:\n",
        "                systems.append(\"eagle\")\n",
        "            if darkmyter_cb.value:\n",
        "                systems.append(\"darkmyter\")\n",
        "            if yolo_cb.value:\n",
        "                systems.append(\"yolo11_botsort\")\n",
        "\n",
        "            if not systems:\n",
        "                print(\"[ERROR] Select at least one system!\")\n",
        "                return\n",
        "\n",
        "            # Create video\n",
        "            if output_type.value.startswith('Side'):\n",
        "                path = create_side_by_side_video(video_name, frame_slider.value)\n",
        "            else:\n",
        "                path = create_overlay_video(video_name, systems, frame_slider.value)\n",
        "\n",
        "            # Display result\n",
        "            if path and path.exists():\n",
        "                print(\"\\n\" + \"=\" * 60)\n",
        "                print(\"VIDEO READY!\")\n",
        "                print(\"=\" * 60)\n",
        "                display(Video(str(path), embed=True, width=900))\n",
        "\n",
        "    generate_btn.on_click(on_generate)\n",
        "\n",
        "    # Layout widgets\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\"<h3>1. Select Video</h3>\"),\n",
        "        video_dropdown,\n",
        "        widgets.HTML(\"<h3>2. Select Systems to Compare</h3>\"),\n",
        "        widgets.HBox([eagle_cb, darkmyter_cb, yolo_cb]),\n",
        "        widgets.HTML(\"<h3>3. Options</h3>\"),\n",
        "        frame_slider,\n",
        "        output_type,\n",
        "        widgets.HTML(\"<br>\"),\n",
        "        generate_btn,\n",
        "        output_area\n",
        "    ]))\n",
        "\n",
        "\n",
        "# QUICK FUNCTIONS\n",
        "\n",
        "def quick_overlay(video_name: str, max_frames: int = 300) -> Path:\n",
        "    # Quick function: Create overlay video with all 3 systems\n",
        "    return create_overlay_video(video_name, AGENTS, max_frames)\n",
        "\n",
        "\n",
        "def quick_sidebyside(video_name: str, max_frames: int = 300) -> Path:\n",
        "    # Quick function: Create side-by-side comparison video\n",
        "    return create_side_by_side_video(video_name, max_frames)\n",
        "\n",
        "\n",
        "def quick_frame(video_name: str, frame_number: int) -> Path:\n",
        "    # Quick function: Extract a single comparison frame\n",
        "    return extract_comparison_frame(video_name, frame_number)\n",
        "\n",
        "\n",
        "# RUN\n",
        "\n",
        "print(\"CELL C: Visualization Module Loaded\")\n",
        "\n",
        "print(\"\\nQuick functions:\")\n",
        "print(\"  quick_overlay('VIDEO_NAME')     - All systems on one video\")\n",
        "print(\"  quick_sidebyside('VIDEO_NAME')  - Side-by-side panels\")\n",
        "print(\"  quick_frame('VIDEO_NAME', 100)  - Extract frame 100 as image\")\n",
        "print(\"\\nExample:\")\n",
        "print(\"  quick_overlay('FULL MATCH  Belgium 1-2 Italy  VIP Tactical Camera 720-seg14')\")\n",
        "print(\"\\nOr use the interactive UI below:\\n\")\n",
        "\n",
        "# Launch interactive UI\n",
        "run_visualization_ui()"
      ],
      "metadata": {
        "id": "9sCLIf-eykHv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "generative_ai_disabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}