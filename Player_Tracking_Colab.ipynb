{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Player Tracking System Comparison\n",
        "\n",
        "This notebook compares 3 different player tracking systems:\n",
        "- Eagle\n",
        "- Darkmyter (using Ultralytics YOLO)\n",
        "- TrackLab\n",
        "\n",
        "**Important**: Run cells in order from top to bottom!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Setup directories and utilities\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "BASE_DIR = Path(\"/content\")\n",
        "REPOS_DIR = BASE_DIR / \"repositories\"\n",
        "VIDEOS_DIR = BASE_DIR / \"videos\"\n",
        "CLIPS_DIR = BASE_DIR / \"clips\"\n",
        "OUTPUT_DIR = BASE_DIR / \"output\"\n",
        "\n",
        "for d in [REPOS_DIR, VIDEOS_DIR, CLIPS_DIR, OUTPUT_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def print_status(msg, status=\"INFO\"):\n",
        "    \"\"\"Print colored status messages\"\"\"\n",
        "    colors = {\n",
        "        \"INFO\": \"\\033[94m\",\n",
        "        \"SUCCESS\": \"\\033[92m\",\n",
        "        \"WARNING\": \"\\033[93m\",\n",
        "        \"ERROR\": \"\\033[91m\",\n",
        "        \"RESET\": \"\\033[0m\"\n",
        "    }\n",
        "    print(f\"{colors.get(status, '')}[{status}] {msg}{colors['RESET']}\")\n",
        "\n",
        "print_status(\"Directory structure created\", \"SUCCESS\")\n",
        "print(f\"Working directory: {BASE_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Clone all repositories\n",
        "\n",
        "import subprocess\n",
        "\n",
        "REPOSITORIES = {\n",
        "    \"eagle\": \"https://github.com/nreHieW/Eagle.git\",\n",
        "    \"darkmyter\": \"https://github.com/Darkmyter/Football-Players-Tracking.git\",\n",
        "}\n",
        "\n",
        "print_status(\"Cloning repositories...\", \"INFO\")\n",
        "\n",
        "for name, url in REPOSITORIES.items():\n",
        "    repo_path = REPOS_DIR / name\n",
        "\n",
        "    if repo_path.exists():\n",
        "        print_status(f\"{name}: Already exists, skipping\", \"WARNING\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        print_status(f\"{name}: Cloning...\", \"INFO\")\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"clone\", url, str(repo_path)],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=300\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print_status(f\"{name}: Cloned successfully\", \"SUCCESS\")\n",
        "        else:\n",
        "            print_status(f\"{name}: Clone failed - {result.stderr[:100]}\", \"ERROR\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"{name}: Clone failed - {str(e)}\", \"ERROR\")\n",
        "\n",
        "print_status(\"Repository cloning complete\", \"SUCCESS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "install",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb6b6bd-2f37-463a-b453-dcfd1cb2e09f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m[INFO] Installing dependencies...\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.4/335.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.9/827.9 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.5/159.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[92m[SUCCESS] Dependencies installed\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Install dependencies\n",
        "\n",
        "print_status(\"Installing dependencies...\", \"INFO\")\n",
        "\n",
        "!pip install -q torch torchvision torchaudio tracklab\n",
        "!pip install -q opencv-python numpy scipy pandas scikit-learn matplotlib\n",
        "!pip install -q ultralytics supervision\n",
        "!pip install -q gdown Pillow tqdm requests\n",
        "\n",
        "print_status(\"Dependencies installed\", \"SUCCESS\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO(\"yolo11m.pt\")\n",
        "print(\"Loaded weights from:\", getattr(model, \"ckpt_path\", \"unknown path\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzy7qI3atgID",
        "outputId": "c5635672-7ab3-47be-8678-f2edb3d8f2f5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt to 'yolo11m.pt': 100% ━━━━━━━━━━━━ 38.8MB 103.2MB/s 0.4s\n",
            "Loaded weights from: yolo11m.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_videos"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Download videos from Google Drive\n",
        "\n",
        "!pip install -q gdown\n",
        "\n",
        "import gdown\n",
        "from pathlib import Path\n",
        "\n",
        "# Shared folder ID\n",
        "FOLDER_ID = \"1Cs4kTX6GYwfcpKyDZdqRKBezz49wT7_N\"\n",
        "\n",
        "print_status(\"Downloading videos from shared folder...\", \"INFO\")\n",
        "\n",
        "try:\n",
        "    gdown.download_folder(\n",
        "        id=FOLDER_ID,\n",
        "        output=str(VIDEOS_DIR),\n",
        "        quiet=False,\n",
        "        use_cookies=False\n",
        "    )\n",
        "\n",
        "    # List downloaded videos\n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV', '.MKV']\n",
        "    available_videos = []\n",
        "\n",
        "    for ext in video_extensions:\n",
        "        available_videos.extend(list(VIDEOS_DIR.glob(f\"*{ext}\")))\n",
        "\n",
        "    if not available_videos:\n",
        "        print_status(\"No video files found\", \"ERROR\")\n",
        "    else:\n",
        "        print(f\"DOWNLOADED {len(available_videos)} VIDEO(S)\")\n",
        "\n",
        "\n",
        "        for idx, video in enumerate(available_videos, 1):\n",
        "            size_mb = video.stat().st_size / (1024 * 1024)\n",
        "            print(f\"{idx}. {video.name} ({size_mb:.1f} MB)\")\n",
        "\n",
        "\n",
        "        print(\"Enter video selection:\")\n",
        "        print(\"  - Leave blank to process ALL videos\")\n",
        "        print(\"  - Enter a number (e.g., '1')\")\n",
        "        print(\"  - Enter comma-separated numbers (e.g., '1,2')\")\n",
        "\n",
        "        selection = input(\"\\nYour choice: \").strip()\n",
        "\n",
        "        VIDEO_PATHS = []\n",
        "\n",
        "        if not selection:\n",
        "            VIDEO_PATHS = available_videos\n",
        "            print_status(f\"Selected ALL {len(VIDEO_PATHS)} videos\", \"SUCCESS\")\n",
        "        elif selection.isdigit():\n",
        "            idx = int(selection)\n",
        "            if 1 <= idx <= len(available_videos):\n",
        "                VIDEO_PATHS = [available_videos[idx - 1]]\n",
        "                print_status(f\"Selected: {VIDEO_PATHS[0].name}\", \"SUCCESS\")\n",
        "        elif ',' in selection:\n",
        "            try:\n",
        "                indices = [int(x.strip()) for x in selection.split(',')]\n",
        "                for idx in indices:\n",
        "                    if 1 <= idx <= len(available_videos):\n",
        "                        VIDEO_PATHS.append(available_videos[idx - 1])\n",
        "                print_status(f\"Selected {len(VIDEO_PATHS)} videos\", \"SUCCESS\")\n",
        "            except ValueError:\n",
        "                print_status(\"Invalid input\", \"ERROR\")\n",
        "\n",
        "        if not VIDEO_PATHS:\n",
        "            print_status(\"No videos selected\", \"ERROR\")\n",
        "\n",
        "except Exception as e:\n",
        "    print_status(f\"Download failed: {str(e)}\", \"ERROR\")\n",
        "    print(\"\\nNote: Make sure the folder is set to 'Anyone with the link can view'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract_clips"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Extract clips\n",
        "\n",
        "import cv2\n",
        "\n",
        "CLIP_DURATION = 60\n",
        "ALL_CLIPS = {}\n",
        "\n",
        "for VIDEO_PATH in VIDEO_PATHS:\n",
        "    VIDEO_NAME = VIDEO_PATH.stem\n",
        "\n",
        "\n",
        "    print(f\"PROCESSING: {VIDEO_NAME}\\n\")\n",
        "\n",
        "\n",
        "    cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = total_frames / fps\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"Duration: {duration:.1f}s | FPS: {fps:.1f} | Frames: {total_frames}\")\n",
        "\n",
        "    if duration < CLIP_DURATION * 3:\n",
        "        if duration < CLIP_DURATION:\n",
        "            CLIPS = [(0, duration, \"full\")]\n",
        "        else:\n",
        "            CLIPS = [(0, CLIP_DURATION, \"start\"), (max(duration - CLIP_DURATION, 0), CLIP_DURATION, \"end\")]\n",
        "    else:\n",
        "        CLIPS = [\n",
        "            (0, CLIP_DURATION, \"start\"),\n",
        "            ((duration - CLIP_DURATION) / 2, CLIP_DURATION, \"middle\"),\n",
        "            (duration - CLIP_DURATION, CLIP_DURATION, \"end\")\n",
        "        ]\n",
        "\n",
        "    CLIP_PATHS = {}\n",
        "\n",
        "    for start_time, clip_dur, position in CLIPS:\n",
        "        clip_name = f\"{VIDEO_NAME}_{position}.mp4\"\n",
        "        clip_path = CLIPS_DIR / clip_name\n",
        "\n",
        "        cmd = [\"ffmpeg\", \"-i\", str(VIDEO_PATH), \"-ss\", str(start_time), \"-t\", str(clip_dur),\n",
        "               \"-c\", \"copy\", str(clip_path), \"-y\", \"-loglevel\", \"error\"]\n",
        "\n",
        "        result = subprocess.run(cmd, capture_output=True)\n",
        "\n",
        "        if result.returncode == 0 and clip_path.exists():\n",
        "            CLIP_PATHS[position] = clip_path\n",
        "            print_status(f\"Clip '{position}' extracted\", \"SUCCESS\")\n",
        "\n",
        "    ALL_CLIPS[VIDEO_NAME] = CLIP_PATHS\n",
        "\n",
        "print(f\"\\nTotal: {sum(len(clips) for clips in ALL_CLIPS.values())} clips from {len(VIDEO_PATHS)} video(s)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell: Setup Eagle with Python 3.13\n",
        "\n",
        "print_status(\"Setting up Eagle with Python 3.13...\", \"INFO\")\n",
        "\n",
        "eagle_dir = REPOS_DIR / \"eagle\"\n",
        "\n",
        "# Install Python 3.13 (Eagle's required version)\n",
        "print_status(\"Installing Python 3.13...\", \"INFO\")\n",
        "!apt-get update -qq\n",
        "!apt-get install -y software-properties-common\n",
        "!add-apt-repository -y ppa:deadsnakes/ppa\n",
        "!apt-get update -qq\n",
        "!apt-get install -y python3.13 python3.13-venv python3.13-dev python3.13-distutils\n",
        "\n",
        "# Install pip for Python 3.13\n",
        "!curl -sS https://bootstrap.pypa.io/get-pip.py | python3.13\n",
        "\n",
        "# Install uv if not already installed\n",
        "print_status(\"Installing uv...\", \"INFO\")\n",
        "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "\n",
        "# Add uv to PATH\n",
        "import os\n",
        "os.environ['PATH'] = f\"/root/.local/bin:{os.environ['PATH']}\"\n",
        "\n",
        "# Create Eagle environment with Python 3.13\n",
        "os.chdir(eagle_dir)\n",
        "print_status(\"Creating Eagle environment with Python 3.13...\", \"INFO\")\n",
        "!uv venv --python python3.13\n",
        "!uv sync\n",
        "\n",
        "# Download model weights\n",
        "print_status(\"Downloading Eagle model weights...\", \"INFO\")\n",
        "models_dir = eagle_dir / \"eagle\" / \"models\"\n",
        "if models_dir.exists():\n",
        "    os.chdir(models_dir)\n",
        "    !bash get_weights.sh\n",
        "    os.chdir(eagle_dir)\n",
        "    print_status(\"Eagle weights downloaded\", \"SUCCESS\")\n",
        "else:\n",
        "    print_status(\"Eagle models directory not found\", \"ERROR\")\n",
        "\n",
        "# Create Eagle wrapper that uses Python 3.13\n",
        "eagle_wrapper = eagle_dir / \"run_eagle.py\"\n",
        "\n",
        "eagle_dir = REPOS_DIR / \"eagle\"\n",
        "\n",
        "eagle_wrapper = eagle_dir / \"run_eagle.py\"\n",
        "eagle_wrapper = eagle_dir / \"run_eagle.py\"\n",
        "eagle_wrapper.write_text('''\n",
        "import argparse\n",
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--video\", required=True)\n",
        "    parser.add_argument(\"--output\", required=True)\n",
        "    parser.add_argument(\"--fps\", default=10, type=int)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    video_path = Path(args.video)\n",
        "    output_path = Path(args.output)\n",
        "\n",
        "    if not video_path.exists():\n",
        "        print(f\"Error: Video not found: {video_path}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Set up environment\n",
        "    env = os.environ.copy()\n",
        "    env[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "    # Run Eagle with Python 3.13 - FULL PIPELINE\n",
        "    cmd = [\n",
        "        \"uv\", \"run\", \"--python\", \"python3.13\",\n",
        "        \"main.py\",\n",
        "        \"--video_path\", str(video_path),\n",
        "        \"--fps\", str(args.fps),\n",
        "    ]\n",
        "\n",
        "    print(f\"Processing {video_path} at {args.fps} FPS...\", file=sys.stderr)\n",
        "    start = time.time()\n",
        "\n",
        "    result = subprocess.run(\n",
        "        cmd,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        cwd=Path(__file__).parent,\n",
        "        timeout=600,  # 10 minute timeout\n",
        "        env=env,\n",
        "    )\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"Eagle processing took {elapsed:.1f}s ({elapsed/60:.1f} minutes)\", file=sys.stderr)\n",
        "\n",
        "    # Log errors but keep going so we can at least copy whatever exists\n",
        "    if result.returncode != 0:\n",
        "        print(f\"[Eagle] warnings/errors (continuing anyway): {result.stderr[:500]}\", file=sys.stderr)\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # Copy RAW Eagle output into the requested folder\n",
        "    # ---------------------------------------------------\n",
        "    video_stem = video_path.stem\n",
        "    eagle_base = Path(__file__).parent / \"output\"\n",
        "\n",
        "    # 1) Locate Eagle's internal output folder for this video\n",
        "    eagle_output_dir = eagle_base / video_stem\n",
        "    if not eagle_output_dir.exists():\n",
        "        # fallback: any subdir containing the video stem\n",
        "        for d in eagle_base.iterdir():\n",
        "            if d.is_dir() and video_stem in d.name:\n",
        "                eagle_output_dir = d\n",
        "                break\n",
        "\n",
        "    if not eagle_output_dir.exists():\n",
        "        print(f\"[Eagle] Could not find internal output folder for {video_stem}\", file=sys.stderr)\n",
        "        # Still create an empty file so the pipeline doesn't crash\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with output_path.open(\"w\") as f:\n",
        "            json.dump([], f)\n",
        "        sys.exit(0)\n",
        "\n",
        "    # 2) If Eagle uses raw_coordinates/, descend there\n",
        "    coords_root = eagle_output_dir / \"raw_coordinates\"\n",
        "    if coords_root.exists():\n",
        "        source_root = coords_root\n",
        "    else:\n",
        "        source_root = eagle_output_dir\n",
        "\n",
        "    print(f\"[Eagle] Copying raw outputs from {source_root}\", file=sys.stderr)\n",
        "\n",
        "    # 3) Copy all .json files into the requested output folder\n",
        "    target_dir = output_path.parent\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    copied_files = []\n",
        "    for src in sorted(source_root.glob(\"*.json\")):\n",
        "        dst = target_dir / src.name\n",
        "        shutil.copy2(src, dst)\n",
        "        copied_files.append(dst.name)\n",
        "\n",
        "    print(f\"[Eagle] Copied JSON files: {copied_files}\", file=sys.stderr)\n",
        "\n",
        "    # 4) Choose one \"canonical\" file as output.json (no modification)\n",
        "    processed = source_root / \"processed_data.json\"\n",
        "    if processed.exists():\n",
        "        shutil.copy2(processed, output_path)\n",
        "        print(f\"[Eagle] Using processed_data.json as output.json\", file=sys.stderr)\n",
        "    else:\n",
        "        any_json = next(source_root.glob(\"*.json\"), None)\n",
        "        if any_json is not None:\n",
        "            shutil.copy2(any_json, output_path)\n",
        "            print(f\"[Eagle] Using {any_json.name} as output.json\", file=sys.stderr)\n",
        "        else:\n",
        "            # No JSONs found: write an empty list to keep the pipeline alive\n",
        "            with output_path.open(\"w\") as f:\n",
        "                json.dump([], f)\n",
        "            print(f\"[Eagle] No JSON files found, wrote empty output.json\", file=sys.stderr)\n",
        "\n",
        "    sys.exit(0)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "\n",
        "eagle_wrapper.chmod(0o755)\n",
        "print_status(\"Eagle FULL capability wrapper created\", \"SUCCESS\")"
      ],
      "metadata": {
        "id": "jIMyv7HMfsbb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import ultralytics\n",
        "\n",
        "REPOS_DIR = Path(\"/content/repositories\")\n",
        "ultra_dir = REPOS_DIR / \"ultra_trackers\"\n",
        "ultra_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) Runner script: choose custom configs for bytetrack + botsort\n",
        "runner_script = ultra_dir / \"run_ultra_yolo_tracker.py\"\n",
        "runner_script.write_text(textwrap.dedent(\"\"\"\\\n",
        "    #!/usr/bin/env python\n",
        "    \\\"\\\"\\\"Run Ultralytics YOLO (v5 / v8 / v11 weights) with a chosen tracker and dump JSON tracks.\n",
        "\n",
        "    Usage:\n",
        "      python run_ultra_yolo_tracker.py \\\\\n",
        "          --video input.mp4 \\\\\n",
        "          --output output.json \\\\\n",
        "          --weights yolo11m.pt \\\\\n",
        "          --tracker botsort\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    import argparse\n",
        "    import json\n",
        "    from pathlib import Path\n",
        "\n",
        "    from ultralytics import YOLO\n",
        "    model = YOLO(\"yolo11m.pt\")\n",
        "\n",
        "    def main():\n",
        "        parser = argparse.ArgumentParser(description=\"YOLO + tracker to JSON\")\n",
        "        parser.add_argument(\"--video\", required=True, help=\"Path to input video\")\n",
        "        parser.add_argument(\"--output\", required=True, help=\"Path to output JSON\")\n",
        "        parser.add_argument(\n",
        "            \"--weights\",\n",
        "            default=\"yolov5s.pt\",\n",
        "            help=\"YOLO weights (e.g., yolov5s.pt, yolov8n.pt, yolo11m.pt, ...)\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--tracker\",\n",
        "            default=\"botsort\",\n",
        "            choices=[\"botsort\", \"deepsort\", \"bytetrack\"],\n",
        "            help=\"Which tracker config to use\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--conf\",\n",
        "            type=float,\n",
        "            default=0.3,\n",
        "            help=\"Confidence threshold (detector)\",\n",
        "        )\n",
        "        args = parser.parse_args()\n",
        "\n",
        "        video_path = Path(args.video)\n",
        "        out_path = Path(args.output)\n",
        "\n",
        "        if not video_path.exists():\n",
        "            raise SystemExit(f\"Video not found: {video_path}\")\n",
        "\n",
        "        # Load YOLO model (Ultralytics supports yolov5/8/11 weights)\n",
        "        model = YOLO(args.weights)\n",
        "\n",
        "        # Resolve tracker config:\n",
        "        #  - bytetrack -> our TrackLab-style ByteTrack config\n",
        "        #  - botsort   -> our tuned BoTSORT config\n",
        "        #  - deepsort  -> Ultralytics built-in deepsort.yaml\n",
        "        ultra_root = Path(__file__).resolve().parent\n",
        "\n",
        "        if args.tracker == \"bytetrack\":\n",
        "            tracker_cfg = \"bytetrack.yaml\"\n",
        "        elif args.tracker == \"botsort\":\n",
        "            tracker_cfg = \"botsort.yaml\"\n",
        "        else:  # deepsort\n",
        "            tracker_cfg = \"deepsort.yaml\"\n",
        "\n",
        "        # Run tracking; stream=True yields a generator of per-frame Results\n",
        "        results = model.track(\n",
        "            source=str(video_path),\n",
        "            tracker=tracker_cfg,\n",
        "            conf=args.conf,\n",
        "            iou=0.5,\n",
        "            stream=True,\n",
        "            device=0,       # GPU 0 if available, otherwise CPU\n",
        "            save=False,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        print(\"YOLO model device after track:\", model.device)\n",
        "\n",
        "        all_detections = []\n",
        "        frame_idx = 0\n",
        "\n",
        "        for r in results:\n",
        "            boxes = r.boxes\n",
        "            if boxes is None:\n",
        "                frame_idx += 1\n",
        "                continue\n",
        "\n",
        "            ids = boxes.id\n",
        "            if ids is None:\n",
        "                frame_idx += 1\n",
        "                continue\n",
        "\n",
        "            xyxy = boxes.xyxy\n",
        "            confs = boxes.conf\n",
        "            clses = boxes.cls\n",
        "\n",
        "            ids = ids.cpu().tolist()\n",
        "            xyxy = xyxy.cpu().tolist()\n",
        "            confs = confs.cpu().tolist()\n",
        "            clses = clses.cpu().tolist()\n",
        "\n",
        "            for tid, (x1, y1, x2, y2), score, c in zip(ids, xyxy, confs, clses):\n",
        "                all_detections.append({\n",
        "                    \"frame_id\": frame_idx,\n",
        "                    \"track_id\": int(tid),\n",
        "                    \"bbox\": [float(x1), float(y1), float(x2), float(y2)],\n",
        "                    \"score\": float(score),\n",
        "                    \"class_id\": int(c),\n",
        "                })\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with out_path.open(\"w\") as f:\n",
        "            json.dump(all_detections, f)\n",
        "\n",
        "        print(f\"Wrote {len(all_detections)} tracked detections to {out_path}\")\n",
        "\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        main()\n",
        "    \"\"\"))\n",
        "\n",
        "runner_script.chmod(0o755)\n",
        "print(\"Created runner:\", runner_script)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I5iE6qiWWTy",
        "outputId": "41e826b0-c229-4618-ea95-4fdf0ce4a196"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created runner: /content/repositories/ultra_trackers/run_ultra_yolo_tracker.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell: Setup Darkmyter (ByteTrack + YOLO)\n",
        "\n",
        "print_status(\"Setting up Darkmyter tracking...\", \"INFO\")\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "darkmyter_dir = REPOS_DIR / \"darkmyter\"\n",
        "\n",
        "# Download football-specific weights\n",
        "weights_dir = darkmyter_dir / \"yolov8-weights\"\n",
        "weights_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "custom_weights = weights_dir / \"yolov8l-football-players.pt\"\n",
        "gdrive_id = \"12dWRBsegmyGE3feTdy9LBf1eZ-hTZ9Sx\"\n",
        "\n",
        "def download_darkmyter_weights():\n",
        "    print_status(\"Downloading Darkmyter football weights...\", \"INFO\")\n",
        "    try:\n",
        "        try:\n",
        "            import gdown\n",
        "        except ImportError:\n",
        "            subprocess.run([\"pip\", \"install\", \"gdown\"], check=True)\n",
        "            import gdown\n",
        "\n",
        "        url = f\"https://drive.google.com/uc?id={gdrive_id}\"\n",
        "        gdown.download(url, str(custom_weights), quiet=False)\n",
        "        print_status(\"Darkmyter weights downloaded\", \"SUCCESS\")\n",
        "    except Exception as e:\n",
        "        print_status(f\"Failed to download weights: {e}\", \"ERROR\")\n",
        "\n",
        "# Check if weights exist and are valid\n",
        "if custom_weights.exists():\n",
        "    try:\n",
        "        with open(custom_weights, \"rb\") as f:\n",
        "            header = f.read(16)\n",
        "        if header.startswith(b\"<\"):\n",
        "            print_status(\"Weights file is HTML, re-downloading...\", \"ERROR\")\n",
        "            custom_weights.unlink(missing_ok=True)\n",
        "            download_darkmyter_weights()\n",
        "        else:\n",
        "            print_status(\"Darkmyter weights already present\", \"SUCCESS\")\n",
        "    except Exception:\n",
        "        custom_weights.unlink(missing_ok=True)\n",
        "        download_darkmyter_weights()\n",
        "else:\n",
        "    download_darkmyter_weights()\n",
        "\n",
        "# Create corrected Darkmyter wrapper\n",
        "darkmyter_wrapper = darkmyter_dir / \"run_darkmyter.py\"\n",
        "darkmyter_wrapper.write_text('''#!/usr/bin/env python\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    import torch\n",
        "except ImportError:\n",
        "    print(\"Error: ultralytics or torch not installed in this env\", file=sys.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    parser = argparse.ArgumentParser(description=\"Darkmyter: YOLOv8 + ByteTrack (football)\")\n",
        "    parser.add_argument(\"--video\", required=True, help=\"Path to input video\")\n",
        "    parser.add_argument(\"--output\", required=True, help=\"Path to output JSON (standard format)\")\n",
        "    parser.add_argument(\"--conf\", type=float, default=0.3, help=\"Confidence threshold\")\n",
        "    parser.add_argument(\"--iou\", type=float, default=0.5, help=\"IoU threshold\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    video_path = Path(args.video)\n",
        "    output_path = Path(args.output)\n",
        "\n",
        "    if not video_path.exists():\n",
        "        print(f\"Error: video not found: {video_path}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    repo_root = Path(__file__).resolve().parent\n",
        "    custom_weights = repo_root / \"yolov8-weights\" / \"yolov8l-football-players.pt\"\n",
        "\n",
        "    if custom_weights.exists():\n",
        "        print(f\"[Darkmyter] Using football-specific weights: {custom_weights}\", file=sys.stderr)\n",
        "        model = YOLO(str(custom_weights))\n",
        "        model_name = \"yolov8l-football\"\n",
        "        football_specific = True\n",
        "    else:\n",
        "        print(\"[Darkmyter] Football weights not found, falling back to yolov8x.pt\", file=sys.stderr)\n",
        "        model = YOLO(\"yolov8x.pt\")\n",
        "        model_name = \"yolov8x\"\n",
        "        football_specific = False\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"[Darkmyter] Device: {device}\", file=sys.stderr)\n",
        "\n",
        "    # Stream tracking results frame by frame\n",
        "    results_gen = model.track(\n",
        "        source=str(video_path),\n",
        "        tracker=\"bytetrack.yaml\",\n",
        "        conf=args.conf,\n",
        "        iou=args.iou,\n",
        "        stream=True,\n",
        "        device=device,\n",
        "        persist=True,\n",
        "        verbose=False,\n",
        "        save=False,\n",
        "    )\n",
        "\n",
        "    detections = []\n",
        "    total_tracks = set()\n",
        "    conf_values = []\n",
        "    frames_processed = 0\n",
        "\n",
        "    for frame_idx, r in enumerate(results_gen):\n",
        "        frames_processed = frame_idx + 1\n",
        "\n",
        "        boxes = getattr(r, \"boxes\", None)\n",
        "        if boxes is None or boxes.id is None:\n",
        "            continue\n",
        "\n",
        "        ids = boxes.id.cpu().tolist()\n",
        "        xyxy = boxes.xyxy.cpu().tolist()\n",
        "        scores = boxes.conf.cpu().tolist()\n",
        "        if boxes.cls is not None:\n",
        "            classes = boxes.cls.cpu().tolist()\n",
        "        else:\n",
        "            classes = [0] * len(ids)\n",
        "\n",
        "        for tid, box, score, cls in zip(ids, xyxy, scores, classes):\n",
        "            detections.append(\n",
        "                {\n",
        "                    \"frame_id\": int(frame_idx),\n",
        "                    \"track_id\": int(tid),\n",
        "                    \"bbox\": [float(box[0]), float(box[1]), float(box[2]), float(box[3])],\n",
        "                    \"score\": float(score),\n",
        "                    \"class_id\": int(cls),\n",
        "                }\n",
        "            )\n",
        "            total_tracks.add(int(tid))\n",
        "            conf_values.append(float(score))\n",
        "\n",
        "        if frame_idx % 100 == 0:\n",
        "            print(f\"[Darkmyter] Processed {frame_idx} frames…\", file=sys.stderr)\n",
        "\n",
        "    stats = {\n",
        "        \"total_tracks\": len(total_tracks),\n",
        "        \"frames_processed\": int(frames_processed),\n",
        "        \"avg_confidence\": (sum(conf_values) / len(conf_values)) if conf_values else 0.0,\n",
        "    }\n",
        "\n",
        "    full_output = {\n",
        "        \"framework\": \"Darkmyter\",\n",
        "        \"model\": model_name,\n",
        "        \"tracker\": \"ByteTrack\",\n",
        "        \"features\": {\n",
        "            \"football_specific\": football_specific,\n",
        "            \"dual_threshold\": True,\n",
        "            \"optimized_for\": \"tactical_camera\",\n",
        "        },\n",
        "        \"detections\": detections,\n",
        "        \"statistics\": stats,\n",
        "    }\n",
        "\n",
        "    # Save full rich output\n",
        "    full_output_path = output_path.with_suffix(\".darkmyter.json\")\n",
        "    full_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with full_output_path.open(\"w\") as f:\n",
        "        json.dump(full_output, f, indent=2)\n",
        "\n",
        "    # Save plain list for your evaluation pipeline\n",
        "    with output_path.open(\"w\") as f:\n",
        "        json.dump(detections, f)\n",
        "\n",
        "    print(f\"[Darkmyter] Total detections: {len(detections)}\", file=sys.stderr)\n",
        "    print(f\"[Darkmyter] Unique tracks: {stats['total_tracks']}\", file=sys.stderr)\n",
        "    print(f\"[Darkmyter] Frames processed: {stats['frames_processed']}\", file=sys.stderr)\n",
        "    print(f\"[Darkmyter] Avg confidence: {stats['avg_confidence']:.3f}\", file=sys.stderr)\n",
        "    print(f\"[Darkmyter] Standard output: {output_path}\", file=sys.stderr)\n",
        "    print(f\"[Darkmyter] Full output: {full_output_path}\", file=sys.stderr)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "\n",
        "darkmyter_wrapper.chmod(0o755)\n",
        "print_status(\"Darkmyter full capability wrapper created\", \"SUCCESS\")"
      ],
      "metadata": {
        "id": "H_etsZN8K1QG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bc6e43-40d9-472f-b38b-3e41f90ac74f",
        "collapsed": true
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m[INFO] Setting up Darkmyter tracking...\u001b[0m\n",
            "\u001b[92m[SUCCESS] Darkmyter weights already present\u001b[0m\n",
            "\u001b[92m[SUCCESS] Darkmyter full capability wrapper created\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Cell: Final System Evaluation\n",
        "# ================================\n",
        "\n",
        "import time\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "\"\"\"SYSTEM_CONFIGS = {\n",
        "      \"eagle\": {\n",
        "        \"path\": REPOS_DIR / \"eagle\",\n",
        "        \"script\": \"run_eagle.py\",\n",
        "        \"python\": \"python3.13\",\n",
        "    },\n",
        "    \"darkmyter\": {\n",
        "        \"path\": REPOS_DIR / \"darkmyter\",\n",
        "        \"script\": \"run_darkmyter.py\",\n",
        "    },\n",
        "    \"yolo11_botsort\": {\n",
        "        \"path\": REPOS_DIR / \"ultra_trackers\",\n",
        "        \"script\": \"run_ultra_yolo_tracker.py\",\n",
        "        \"args\": [\"--weights\", \"yolo11m.pt\", \"--tracker\", \"botsort\", \"--conf\", \"0.4\"],\n",
        "    },\n",
        "    \"yolo11_bytetrack\": {\n",
        "        \"path\": REPOS_DIR / \"ultra_trackers\",\n",
        "        \"script\": \"run_ultra_yolo_tracker.py\",\n",
        "        \"args\": [\"--weights\", \"yolo11m.pt\", \"--tracker\", \"bytetrack\", \"--conf\", \"0.4\"],\n",
        "    },\n",
        "}\n",
        "\"\"\"\n",
        "# System configurations\n",
        "SYSTEM_CONFIGS = {\n",
        "\n",
        "    \"darkmyter\": {\n",
        "        \"path\": REPOS_DIR / \"darkmyter\",\n",
        "        \"script\": \"run_darkmyter.py\",\n",
        "    },\n",
        "    \"yolo11_botsort\": {\n",
        "        \"path\": REPOS_DIR / \"ultra_trackers\",\n",
        "        \"script\": \"run_ultra_yolo_tracker.py\",\n",
        "        \"args\": [\"--weights\", \"yolo11m.pt\", \"--tracker\", \"botsort\", \"--conf\", \"0.4\"],\n",
        "    },\n",
        "    \"yolo11_bytetrack\": {\n",
        "        \"path\": REPOS_DIR / \"ultra_trackers\",\n",
        "        \"script\": \"run_ultra_yolo_tracker.py\",\n",
        "        \"args\": [\"--weights\", \"yolo11m.pt\", \"--tracker\", \"bytetrack\", \"--conf\", \"0.4\"],\n",
        "    },\n",
        "    \"eagle\": {\n",
        "        \"path\": REPOS_DIR / \"eagle\",\n",
        "        \"script\": \"run_eagle.py\",\n",
        "        \"python\": \"python3.13\",\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "position_to_number = {\"start\": 1, \"middle\": 2, \"end\": 3, \"full\": 1}\n",
        "\n",
        "def run_system_on_clip(system_name, system_config, video_name, clip_number, clip_path):\n",
        "    \"\"\"Run a tracking system on a clip\"\"\"\n",
        "\n",
        "    output_dir = OUTPUT_DIR / video_name / \"clips\" / str(clip_number) / system_name\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print_status(f\"Running {system_name} on {video_name}/clip_{clip_number}...\", \"INFO\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    output_file = output_dir / \"output.json\"\n",
        "    system_path = system_config.get(\"path\", REPOS_DIR)\n",
        "\n",
        "    # Build command as before for non-TrackLab systems\n",
        "    if system_name == \"eagle\":\n",
        "        cmd = [\n",
        "            \"uv\", \"run\", \"--python\", system_config.get(\"python\", \"python3.13\"),\n",
        "            \"run_eagle.py\",\n",
        "            \"--video\", str(clip_path),\n",
        "            \"--output\", str(output_file),\n",
        "        ]\n",
        "    else:\n",
        "        cmd = [\n",
        "            \"python\", system_config[\"script\"],\n",
        "            \"--video\", str(clip_path),\n",
        "            \"--output\", str(output_file),\n",
        "        ]\n",
        "        for extra in system_config.get(\"args\", []):\n",
        "            cmd.append(str(extra))\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=600,\n",
        "            cwd=str(system_path),\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        if result.returncode == 0 and output_file.exists():\n",
        "            try:\n",
        "                with open(output_file) as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                if isinstance(data, list):\n",
        "                    num_detections = len(data)\n",
        "                elif isinstance(data, dict):\n",
        "                    num_detections = sum(\n",
        "                        len(dets) if isinstance(dets, list) else 0\n",
        "                        for dets in data.values()\n",
        "                    )\n",
        "                else:\n",
        "                    num_detections = 0\n",
        "\n",
        "                print_status(\n",
        "                    f\"{system_name}: SUCCESS - {num_detections} detections in {elapsed:.1f}s\",\n",
        "                    \"SUCCESS\"\n",
        "                )\n",
        "                return {\n",
        "                    \"success\": True,\n",
        "                    \"time\": elapsed,\n",
        "                    \"output\": str(output_file),\n",
        "                    \"detections\": num_detections,\n",
        "                }\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print_status(f\"{system_name}: Invalid JSON\", \"ERROR\")\n",
        "                return {\"success\": False, \"time\": elapsed, \"error\": f\"Invalid JSON: {e}\"}\n",
        "        else:\n",
        "            error_msg = result.stderr[-500:] if result.stderr else \"Unknown error\"\n",
        "            print_status(f\"{system_name}: FAILED\", \"ERROR\")\n",
        "            print(f\"Error: {error_msg}\")\n",
        "            return {\"success\": False, \"time\": elapsed, \"error\": error_msg}\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print_status(f\"{system_name}: TIMEOUT\", \"ERROR\")\n",
        "        return {\"success\": False, \"time\": 600, \"error\": \"Timeout\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"{system_name}: EXCEPTION - {str(e)}\", \"ERROR\")\n",
        "        return {\"success\": False, \"time\": time.time() - start_time, \"error\": str(e)}\n",
        "\n",
        "\n",
        "# Main evaluation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING EVALUATION\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for video_name, clip_paths in ALL_CLIPS.items():\n",
        "    print(f\"\\nVIDEO: {video_name}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    video_results = {}\n",
        "\n",
        "    for clip_position, clip_path in clip_paths.items():\n",
        "        clip_number = position_to_number.get(clip_position, 1)\n",
        "\n",
        "        print(f\"\\nProcessing clip {clip_number} ({clip_position})...\")\n",
        "        video_results[f\"clip_{clip_number}\"] = {}\n",
        "\n",
        "        for system_name, system_config in SYSTEM_CONFIGS.items():\n",
        "            result = run_system_on_clip(system_name, system_config, video_name, clip_number, clip_path)\n",
        "            video_results[f\"clip_{clip_number}\"][system_name] = result\n",
        "\n",
        "        successful = sum(1 for r in video_results[f\"clip_{clip_number}\"].values() if r[\"success\"])\n",
        "        total = len(video_results[f\"clip_{clip_number}\"])\n",
        "        print(f\"Clip summary: {successful}/{total} systems succeeded\")\n",
        "\n",
        "    all_results[video_name] = video_results\n",
        "\n",
        "    summary_file = OUTPUT_DIR / video_name / \"summary.json\"\n",
        "    with open(summary_file, \"w\") as f:\n",
        "        json.dump(video_results, f, indent=2)\n",
        "\n",
        "overall_summary = OUTPUT_DIR / \"overall_summary.json\"\n",
        "with open(overall_summary, \"w\") as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "# Final statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "system_stats = {sys: {\"success\": 0, \"total\": 0} for sys in SYSTEM_CONFIGS.keys()}\n",
        "\n",
        "for video_results in all_results.values():\n",
        "    for clip_results in video_results.values():\n",
        "        for system_name, result in clip_results.items():\n",
        "            system_stats[system_name][\"total\"] += 1\n",
        "            if result[\"success\"]:\n",
        "                system_stats[system_name][\"success\"] += 1\n",
        "\n",
        "print(\"\\nSystem Success Rates:\")\n",
        "for system_name, stats in system_stats.items():\n",
        "    if stats[\"total\"] > 0:\n",
        "        success_rate = (stats[\"success\"] / stats[\"total\"]) * 100\n",
        "        print(f\"  {system_name}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)\")\n",
        "\n",
        "print(f\"\\nResults: {OUTPUT_DIR}\")\n",
        "print(f\"Summary: {overall_summary}\")\n",
        "\n",
        "# Cell 8: Display results\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "print(\"RESULTS SUMMARY \\n\")\n",
        "\n",
        "\n",
        "summary_data = []\n",
        "\n",
        "for video_name, clips in all_results.items():\n",
        "    for clip_key, systems in clips.items():\n",
        "        for system_name, result in systems.items():\n",
        "            summary_data.append({\n",
        "                \"Video\": video_name,\n",
        "                \"Clip\": clip_key,\n",
        "                \"System\": system_name,\n",
        "                \"Status\": \"Valid\" if result[\"success\"] else \"Invalid\",\n",
        "                \"Time (s)\": f\"{result['time']:.1f}\"\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(summary_data)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "total_runs = len(summary_data)\n",
        "successful_runs = sum(1 for row in summary_data if row[\"Status\"] == \"Valid\")\n",
        "\n",
        "\n",
        "print(f\"Success Rate: {successful_runs}/{total_runs} ({100*successful_runs/total_runs:.1f}%)\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMRXc5GpkG4d",
        "outputId": "00ffcdd1-d2c3-4156-f044-48774e025a6e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STARTING EVALUATION\n",
            "============================================================\n",
            "\n",
            "\n",
            "VIDEO: FULL MATCH  Brazil v Mexico  World Cup 2018 720p\n",
            "========================================\n",
            "\n",
            "Processing clip 1 (start)...\n",
            "\u001b[94m[INFO] Running darkmyter on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_1...\u001b[0m\n",
            "\u001b[92m[SUCCESS] darkmyter: SUCCESS - 57442 detections in 90.2s\u001b[0m\n",
            "\u001b[94m[INFO] Running yolov11_botsort on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_1...\u001b[0m\n",
            "\u001b[92m[SUCCESS] yolov11_botsort: SUCCESS - 8032 detections in 120.3s\u001b[0m\n",
            "\u001b[94m[INFO] Running yolov11_bytetrack on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_1...\u001b[0m\n",
            "\u001b[92m[SUCCESS] yolov11_bytetrack: SUCCESS - 7985 detections in 58.5s\u001b[0m\n",
            "\u001b[94m[INFO] Running eagle on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_1...\u001b[0m\n",
            "\u001b[92m[SUCCESS] eagle: SUCCESS - 0 detections in 134.0s\u001b[0m\n",
            "Clip summary: 4/4 systems succeeded\n",
            "\n",
            "Processing clip 2 (middle)...\n",
            "\u001b[94m[INFO] Running darkmyter on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_2...\u001b[0m\n",
            "\u001b[92m[SUCCESS] darkmyter: SUCCESS - 63806 detections in 98.3s\u001b[0m\n",
            "\u001b[94m[INFO] Running yolov11_botsort on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_2...\u001b[0m\n",
            "\u001b[92m[SUCCESS] yolov11_botsort: SUCCESS - 31378 detections in 126.4s\u001b[0m\n",
            "\u001b[94m[INFO] Running yolov11_bytetrack on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_2...\u001b[0m\n",
            "\u001b[92m[SUCCESS] yolov11_bytetrack: SUCCESS - 31196 detections in 65.9s\u001b[0m\n",
            "\u001b[94m[INFO] Running eagle on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_2...\u001b[0m\n",
            "\u001b[92m[SUCCESS] eagle: SUCCESS - 0 detections in 141.1s\u001b[0m\n",
            "Clip summary: 4/4 systems succeeded\n",
            "\n",
            "Processing clip 3 (end)...\n",
            "\u001b[94m[INFO] Running darkmyter on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_3...\u001b[0m\n",
            "\u001b[92m[SUCCESS] darkmyter: SUCCESS - 52753 detections in 92.4s\u001b[0m\n",
            "\u001b[94m[INFO] Running yolov11_botsort on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_3...\u001b[0m\n",
            "\u001b[92m[SUCCESS] yolov11_botsort: SUCCESS - 119 detections in 107.1s\u001b[0m\n",
            "\u001b[94m[INFO] Running yolov11_bytetrack on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_3...\u001b[0m\n",
            "\u001b[92m[SUCCESS] yolov11_bytetrack: SUCCESS - 119 detections in 55.6s\u001b[0m\n",
            "\u001b[94m[INFO] Running eagle on FULL MATCH  Brazil v Mexico  World Cup 2018 720p/clip_3...\u001b[0m\n",
            "\u001b[92m[SUCCESS] eagle: SUCCESS - 540 detections in 120.2s\u001b[0m\n",
            "Clip summary: 4/4 systems succeeded\n",
            "\n",
            "============================================================\n",
            "EVALUATION COMPLETE\n",
            "============================================================\n",
            "\n",
            "System Success Rates:\n",
            "  darkmyter: 3/3 (100.0%)\n",
            "  yolov11_botsort: 3/3 (100.0%)\n",
            "  yolov11_bytetrack: 3/3 (100.0%)\n",
            "  eagle: 3/3 (100.0%)\n",
            "\n",
            "Results: /content/output\n",
            "Summary: /content/output/overall_summary.json\n",
            "RESULTS SUMMARY \n",
            "\n",
            "                                           Video   Clip            System Status Time (s)\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_1         darkmyter  Valid     90.2\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_1   yolov11_botsort  Valid    120.3\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_1 yolov11_bytetrack  Valid     58.5\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_1             eagle  Valid    134.0\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_2         darkmyter  Valid     98.3\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_2   yolov11_botsort  Valid    126.4\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_2 yolov11_bytetrack  Valid     65.9\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_2             eagle  Valid    141.1\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_3         darkmyter  Valid     92.4\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_3   yolov11_botsort  Valid    107.1\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_3 yolov11_bytetrack  Valid     55.6\n",
            "FULL MATCH  Brazil v Mexico  World Cup 2018 720p clip_3             eagle  Valid    120.2\n",
            "Success Rate: 12/12 (100.0%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "eagle_base = Path(\"/content/repositories/eagle/output\")\n",
        "for p in eagle_base.rglob(\"processed_data.json\"):\n",
        "    print(\"Found:\", p)\n",
        "    with open(p) as f:\n",
        "        data = json.load(f)\n",
        "    print(\"Type:\", type(data))\n",
        "    first = next(iter(data.values())) if isinstance(data, dict) else data[0]\n",
        "    print(\"Frame keys:\", first.keys())\n",
        "    print(\"Sample coords_video:\", first.get(\"Coordinates_video\") or first.get(\"coordinates_video\"))\n",
        "    break\n"
      ],
      "metadata": {
        "id": "JjWjJBU_igAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"device name:\", torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "id": "hSpAoLxLYBUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Cell: Comprehensive Fair Comparison Framework\n",
        "# ================================\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "class ComprehensiveFairEvaluator:\n",
        "    \"\"\"\n",
        "    Fair evaluation that respects each system's design goals and unique features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "\n",
        "    def evaluate_all_systems(self, all_results):\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation that credits each system for what it actually provides\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"COMPREHENSIVE FAIR EVALUATION\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "        # Load all data including system-specific outputs\n",
        "        system_data = self.load_comprehensive_data(all_results)\n",
        "\n",
        "        # 1. Common Metrics (All systems can be compared on these)\n",
        "        print(\"\\n1. COMMON TRACKING METRICS\")\n",
        "        print(\"-\" * 40)\n",
        "        common_metrics = self.evaluate_common_metrics(system_data)\n",
        "\n",
        "        # 2. System-Specific Strengths\n",
        "        print(\"\\n2. SYSTEM-SPECIFIC CAPABILITIES\")\n",
        "        print(\"-\" * 40)\n",
        "        specific_metrics = self.evaluate_system_specific(system_data)\n",
        "\n",
        "        # 3. Use Case Suitability\n",
        "        print(\"\\n3. USE CASE EVALUATION\")\n",
        "        print(\"-\" * 40)\n",
        "        use_case_scores = self.evaluate_use_cases(common_metrics, specific_metrics)\n",
        "\n",
        "        # 4. Final Fair Ranking\n",
        "        print(\"\\n4. CONTEXTUALIZED RANKINGS\")\n",
        "        print(\"-\" * 40)\n",
        "        self.compute_fair_rankings(common_metrics, specific_metrics, use_case_scores)\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def load_comprehensive_data(self, all_results):\n",
        "        \"\"\"Load both standard and system-specific outputs\"\"\"\n",
        "        system_data = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "        for video_name, clips_data in all_results.items():\n",
        "            for clip_key, systems_data in clips_data.items():\n",
        "                for system_name, result_data in systems_data.items():\n",
        "                    if result_data.get('success'):\n",
        "                        # Load standard format\n",
        "                        json_path = Path(result_data['output'])\n",
        "                        if json_path.exists():\n",
        "                            with open(json_path, 'r') as f:\n",
        "                                system_data[system_name][f\"{video_name}_{clip_key}\"][\"standard\"] = json.load(f)\n",
        "\n",
        "                        # Load system-specific format\n",
        "                        if system_name == \"eagle\":\n",
        "                            eagle_path = json_path.with_suffix('.eagle.json')\n",
        "                            if eagle_path.exists():\n",
        "                                with open(eagle_path, 'r') as f:\n",
        "                                    system_data[system_name][f\"{video_name}_{clip_key}\"][\"native\"] = json.load(f)\n",
        "\n",
        "                        elif system_name == \"tracklab\":\n",
        "                            tracklab_path = json_path.with_suffix('.tracklab.json')\n",
        "                            if tracklab_path.exists():\n",
        "                                with open(tracklab_path, 'r') as f:\n",
        "                                    system_data[system_name][f\"{video_name}_{clip_key}\"][\"native\"] = json.load(f)\n",
        "\n",
        "                        elif system_name == \"darkmyter\":\n",
        "                            darkmyter_path = json_path.with_suffix('.darkmyter.json')\n",
        "                            if darkmyter_path.exists():\n",
        "                                with open(darkmyter_path, 'r') as f:\n",
        "                                    system_data[system_name][f\"{video_name}_{clip_key}\"][\"native\"] = json.load(f)\n",
        "\n",
        "        return system_data\n",
        "\n",
        "    def evaluate_common_metrics(self, system_data):\n",
        "        \"\"\"Metrics all systems can be compared on\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for system_name, clips in system_data.items():\n",
        "            print(f\"\\n{system_name} (Common Metrics):\")\n",
        "\n",
        "            all_metrics = []\n",
        "            for clip_name, data in clips.items():\n",
        "                detections = data.get(\"standard\", [])\n",
        "                if isinstance(detections, dict) and \"detections\" in detections:\n",
        "                    detections = detections[\"detections\"]\n",
        "\n",
        "                if detections:\n",
        "                    metrics = {\n",
        "                        \"detection_count\": len(detections),\n",
        "                        \"unique_tracks\": len(set(d.get(\"track_id\", 0) for d in detections)),\n",
        "                        \"avg_confidence\": np.mean([d.get(\"score\", 1.0) for d in detections]),\n",
        "                        \"track_consistency\": self.calculate_track_consistency(detections),\n",
        "                        \"coverage\": self.calculate_coverage(detections)\n",
        "                    }\n",
        "                    all_metrics.append(metrics)\n",
        "\n",
        "            if all_metrics:\n",
        "                results[system_name] = {\n",
        "                    \"avg_detections\": np.mean([m[\"detection_count\"] for m in all_metrics]),\n",
        "                    \"avg_tracks\": np.mean([m[\"unique_tracks\"] for m in all_metrics]),\n",
        "                    \"avg_confidence\": np.mean([m[\"avg_confidence\"] for m in all_metrics]),\n",
        "                    \"track_consistency\": np.mean([m[\"track_consistency\"] for m in all_metrics]),\n",
        "                    \"coverage\": np.mean([m[\"coverage\"] for m in all_metrics])\n",
        "                }\n",
        "\n",
        "                print(f\"  Detections/clip: {results[system_name]['avg_detections']:.0f}\")\n",
        "                print(f\"  Unique tracks: {results[system_name]['avg_tracks']:.1f}\")\n",
        "                print(f\"  Confidence: {results[system_name]['avg_confidence']:.3f}\")\n",
        "                print(f\"  Consistency: {results[system_name]['track_consistency']:.3f}\")\n",
        "                print(f\"  Coverage: {results[system_name]['coverage']:.3f}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def evaluate_system_specific(self, system_data):\n",
        "        \"\"\"Evaluate unique capabilities of each system\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        # Eagle-specific: Tactical analysis features\n",
        "        if \"eagle\" in system_data:\n",
        "            print(f\"\\nEagle (Unique Capabilities):\")\n",
        "            eagle_features = {\n",
        "                \"has_homography\": False,\n",
        "                \"has_team_classification\": False,\n",
        "                \"has_pitch_coordinates\": False,\n",
        "                \"has_ball_tracking\": False,\n",
        "                \"processing_time\": []\n",
        "            }\n",
        "\n",
        "            for clip_name, data in system_data[\"eagle\"].items():\n",
        "                native = data.get(\"native\", {})\n",
        "                if \"eagle_features\" in native:\n",
        "                    features = native[\"eagle_features\"]\n",
        "                    eagle_features[\"has_homography\"] |= features.get(\"homography_matrix\") is not None\n",
        "                    eagle_features[\"has_team_classification\"] |= len(features.get(\"team_assignments\", {})) > 0\n",
        "                    eagle_features[\"has_pitch_coordinates\"] |= len(features.get(\"pitch_coordinates\", [])) > 0\n",
        "                    eagle_features[\"has_ball_tracking\"] |= len(features.get(\"ball_tracking\", [])) > 0\n",
        "\n",
        "                if \"video_info\" in native:\n",
        "                    eagle_features[\"processing_time\"].append(native[\"video_info\"].get(\"processing_time\", 0))\n",
        "\n",
        "            results[\"eagle\"] = eagle_features\n",
        "            print(f\"  ✓ Homography mapping: {eagle_features['has_homography']}\")\n",
        "            print(f\"  ✓ Team classification: {eagle_features['has_team_classification']}\")\n",
        "            print(f\"  ✓ Pitch coordinates: {eagle_features['has_pitch_coordinates']}\")\n",
        "            print(f\"  ✓ Ball tracking: {eagle_features['has_ball_tracking']}\")\n",
        "            if eagle_features[\"processing_time\"]:\n",
        "                print(f\"  Processing time: {np.mean(eagle_features['processing_time']):.1f}s\")\n",
        "\n",
        "        # TrackLab-specific: Modularity\n",
        "        if \"tracklab\" in system_data:\n",
        "            print(f\"\\nTrackLab (Unique Capabilities):\")\n",
        "            tracklab_features = {\n",
        "                \"modular_architecture\": True,\n",
        "                \"swappable_components\": True,\n",
        "                \"supported_detectors\": [\"yolov5\", \"yolov8\"],\n",
        "                \"supported_trackers\": [\"bytetrack\", \"botsort\", \"deepsort\"],\n",
        "                \"research_framework\": True\n",
        "            }\n",
        "            results[\"tracklab\"] = tracklab_features\n",
        "            print(f\"  ✓ Modular architecture: {tracklab_features['modular_architecture']}\")\n",
        "            print(f\"  ✓ Detectors: {', '.join(tracklab_features['supported_detectors'])}\")\n",
        "            print(f\"  ✓ Trackers: {', '.join(tracklab_features['supported_trackers'])}\")\n",
        "            print(f\"  ✓ Research framework: {tracklab_features['research_framework']}\")\n",
        "\n",
        "        # Darkmyter-specific: Football optimization\n",
        "        if \"darkmyter\" in system_data:\n",
        "            print(f\"\\nDarkmyter (Unique Capabilities):\")\n",
        "            darkmyter_features = {\n",
        "                \"football_specific_weights\": False,\n",
        "                \"optimized_for_speed\": True,\n",
        "                \"simple_integration\": True\n",
        "            }\n",
        "\n",
        "            for clip_name, data in system_data[\"darkmyter\"].items():\n",
        "                native = data.get(\"native\", {})\n",
        "                if \"features\" in native:\n",
        "                    darkmyter_features[\"football_specific_weights\"] |= native[\"features\"].get(\"football_specific\", False)\n",
        "\n",
        "            results[\"darkmyter\"] = darkmyter_features\n",
        "            print(f\"  ✓ Football-specific weights: {darkmyter_features['football_specific_weights']}\")\n",
        "            print(f\"  ✓ Speed optimized: {darkmyter_features['optimized_for_speed']}\")\n",
        "            print(f\"  ✓ Simple integration: {darkmyter_features['simple_integration']}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def evaluate_use_cases(self, common_metrics, specific_metrics):\n",
        "        \"\"\"Score each system for different use cases\"\"\"\n",
        "        use_cases = {}\n",
        "\n",
        "        print(\"\\nUse Case Suitability Scores (0-100):\")\n",
        "\n",
        "        # Use Case 1: Real-time tracking\n",
        "        print(\"\\n  Real-time Tracking:\")\n",
        "        use_cases[\"realtime\"] = {}\n",
        "        for system in common_metrics.keys():\n",
        "            score = 0\n",
        "            score += common_metrics[system][\"avg_confidence\"] * 30\n",
        "            score += common_metrics[system][\"coverage\"] * 20\n",
        "\n",
        "            if system == \"darkmyter\":\n",
        "                score += 30  # Speed optimized\n",
        "            elif system == \"tracklab\":\n",
        "                score += 20  # Flexible but not speed-focused\n",
        "            elif system == \"eagle\":\n",
        "                score += 0   # Too slow for real-time\n",
        "\n",
        "            use_cases[\"realtime\"][system] = min(score, 100)\n",
        "            print(f\"    {system}: {use_cases['realtime'][system]:.1f}\")\n",
        "\n",
        "        # Use Case 2: Tactical analysis\n",
        "        print(\"\\n  Tactical Analysis:\")\n",
        "        use_cases[\"tactical\"] = {}\n",
        "        for system in common_metrics.keys():\n",
        "            score = 0\n",
        "            score += common_metrics[system][\"track_consistency\"] * 20\n",
        "\n",
        "            if system == \"eagle\" and system in specific_metrics:\n",
        "                eagle_feats = specific_metrics[\"eagle\"]\n",
        "                score += 20 if eagle_feats.get(\"has_homography\") else 0\n",
        "                score += 20 if eagle_feats.get(\"has_team_classification\") else 0\n",
        "                score += 20 if eagle_feats.get(\"has_pitch_coordinates\") else 0\n",
        "                score += 20 if eagle_feats.get(\"has_ball_tracking\") else 0\n",
        "            elif system == \"tracklab\":\n",
        "                score += 30  # Good tracking quality\n",
        "            elif system == \"darkmyter\":\n",
        "                score += 25  # Football-specific\n",
        "\n",
        "            use_cases[\"tactical\"][system] = min(score, 100)\n",
        "            print(f\"    {system}: {use_cases['tactical'][system]:.1f}\")\n",
        "\n",
        "        # Use Case 3: Research/Experimentation\n",
        "        print(\"\\n  Research & Development:\")\n",
        "        use_cases[\"research\"] = {}\n",
        "        for system in common_metrics.keys():\n",
        "            score = 0\n",
        "\n",
        "            if system == \"tracklab\" and system in specific_metrics:\n",
        "                score += 40  # Modular architecture\n",
        "                score += 30  # Multiple options\n",
        "                score += 20  # Research framework\n",
        "            elif system == \"eagle\":\n",
        "                score += 30  # Complex features\n",
        "            elif system == \"darkmyter\":\n",
        "                score += 20  # Simple baseline\n",
        "\n",
        "            score += common_metrics[system][\"avg_confidence\"] * 10\n",
        "\n",
        "            use_cases[\"research\"][system] = min(score, 100)\n",
        "            print(f\"    {system}: {use_cases['research'][system]:.1f}\")\n",
        "\n",
        "        return use_cases\n",
        "\n",
        "    def compute_fair_rankings(self, common_metrics, specific_metrics, use_case_scores):\n",
        "        \"\"\"Provide context-aware rankings\"\"\"\n",
        "\n",
        "        print(\"\\nOVERALL RANKINGS BY CONTEXT:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Best for each use case\n",
        "        for use_case, scores in use_case_scores.items():\n",
        "            ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "            print(f\"\\nBest for {use_case.title()}:\")\n",
        "            for i, (system, score) in enumerate(ranked, 1):\n",
        "                print(f\"  {i}. {system}: {score:.1f}/100\")\n",
        "\n",
        "        # Overall balanced score\n",
        "        print(\"\\nBalanced Overall Score:\")\n",
        "        overall_scores = {}\n",
        "        for system in common_metrics.keys():\n",
        "            score = 0\n",
        "            # Common metrics (40% weight)\n",
        "            score += common_metrics[system][\"avg_confidence\"] * 10\n",
        "            score += common_metrics[system][\"track_consistency\"] * 15\n",
        "            score += common_metrics[system][\"coverage\"] * 15\n",
        "\n",
        "            # Average use case performance (60% weight)\n",
        "            use_case_avg = np.mean([scores[system] for scores in use_case_scores.values()])\n",
        "            score += use_case_avg * 0.6\n",
        "\n",
        "            overall_scores[system] = score\n",
        "\n",
        "        ranked_overall = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        for i, (system, score) in enumerate(ranked_overall, 1):\n",
        "            print(f\"  {i}. {system}: {score:.1f}/100\")\n",
        "\n",
        "        # Save comprehensive results\n",
        "        self.results = {\n",
        "            \"common_metrics\": common_metrics,\n",
        "            \"specific_capabilities\": specific_metrics,\n",
        "            \"use_case_scores\": use_case_scores,\n",
        "            \"overall_ranking\": ranked_overall,\n",
        "            \"evaluation_type\": \"comprehensive_fair\"\n",
        "        }\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"FAIR EVALUATION COMPLETE\")\n",
        "        print(\"Each system evaluated on its intended strengths\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    def calculate_track_consistency(self, detections):\n",
        "        \"\"\"Calculate how consistent tracks are\"\"\"\n",
        "        if not detections:\n",
        "            return 0\n",
        "\n",
        "        tracks = defaultdict(list)\n",
        "        for d in detections:\n",
        "            tracks[d.get(\"track_id\", 0)].append(d.get(\"frame_id\", 0))\n",
        "\n",
        "        consistencies = []\n",
        "        for track_id, frames in tracks.items():\n",
        "            if len(frames) > 1:\n",
        "                frames_sorted = sorted(frames)\n",
        "                gaps = [frames_sorted[i+1] - frames_sorted[i] for i in range(len(frames_sorted)-1)]\n",
        "                consistency = 1.0 / (1 + np.std(gaps)) if gaps else 1.0\n",
        "                consistencies.append(consistency)\n",
        "\n",
        "        return np.mean(consistencies) if consistencies else 0\n",
        "\n",
        "    def calculate_coverage(self, detections):\n",
        "        \"\"\"Calculate frame coverage\"\"\"\n",
        "        if not detections:\n",
        "            return 0\n",
        "\n",
        "        frames = set(d.get(\"frame_id\", 0) for d in detections)\n",
        "        if frames:\n",
        "            frame_range = max(frames) - min(frames) + 1\n",
        "            return len(frames) / frame_range if frame_range > 0 else 0\n",
        "        return 0\n",
        "\n",
        "# ================================\n",
        "# Run Comprehensive Fair Evaluation\n",
        "# ================================\n",
        "\n",
        "fair_evaluator = ComprehensiveFairEvaluator()\n",
        "fair_results = fair_evaluator.evaluate_all_systems(all_results)\n",
        "\n",
        "# Save fair evaluation results\n",
        "fair_eval_file = OUTPUT_DIR / \"fair_evaluation_report.json\"\n",
        "with open(fair_eval_file, \"w\") as f:\n",
        "    json.dump(fair_results, f, indent=2)\n",
        "\n",
        "print(f\"\\n✅ Fair evaluation report saved to: {fair_eval_file}\")"
      ],
      "metadata": {
        "id": "CEQ4BiwHwTq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "download",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d1576e9f-a533-4ab6-d1e5-72791f496de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m[INFO] Creating archive...\u001b[0m\n",
            "\u001b[92m[SUCCESS] Downloading...\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4a1bd385-a31f-4663-b666-d09fee16bae7\", \"tracking_results.zip\", 20057428)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m[SUCCESS] Complete!\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Cell 9: Download results\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print_status(\"Creating archive...\", \"INFO\")\n",
        "\n",
        "archive_name = \"tracking_results\"\n",
        "archive_path = BASE_DIR / archive_name\n",
        "\n",
        "shutil.make_archive(str(archive_path), 'zip', OUTPUT_DIR)\n",
        "\n",
        "print_status(\"Downloading...\", \"SUCCESS\")\n",
        "files.download(f\"{archive_path}.zip\")\n",
        "\n",
        "print_status(\"Complete!\", \"SUCCESS\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}